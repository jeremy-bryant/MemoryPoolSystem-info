This is MemoryPoolSystem.info, produced by makeinfo version 7.1.1 from
MemoryPoolSystem.texi.

     Memory Pool System 1.118.0, Feb 11, 2025

     Ravenbrook Limited

     Copyright © 2025, Ravenbrook Limited

INFO-DIR-SECTION Miscellaneous
START-INFO-DIR-ENTRY
* MemoryPoolSystem: (MemoryPoolSystem.info). One line description of project.
END-INFO-DIR-ENTRY


   Generated by Sphinx 8.1.3.


File: MemoryPoolSystem.info,  Node: Internal types,  Prev: External functions,  Up: Interface<20>

5.1.7.3 Internal types
......................

 -- C Type: typedef struct AllocFrameStruct *AllocFrame

*note .type.frame-handle;: c68. Frame handles are defined as an abstract
type.

 -- C Type: typedef struct AllocFrameClassStruct *AllocFrameClass

*note .type.frame-class;: c6a. Frame classes are defined as an abstract
type.

 -- C Type: typedef *note Res: 55f. (*PoolFramePushMethod)(*note
          AllocFrame: b52. *frameReturn, Pool pool, Buffer buf)

*note .fn.push;: c6c. A pool method of this type is called (if needed)
to invoke the 'FramePush' operation.

 -- C Type: typedef *note Res: 55f. (*PoolFramePopMethod)(Pool pool,
          Buffer buf, *note AllocFrame: b52. frame)

*note .fn.pop;: c6e. A pool method of this type is called (if needed) to
invoke the 'FramePop' operation:

 -- C Type: typedef *note Res: 55f. (*PoolFrameSelectMethod)(Pool pool,
          Buffer buf, *note AllocFrame: b52. frame)

*note .fn.select;: c70. A pool method of this type is called to invoke
the 'FrameSelect' operation.

 -- C Type: typedef *note Res: 55f.
          (*PoolFrameSelectFromAddrMethod)(Pool pool, Buffer buf, *note
          Addr: 632. addr)

*note .fn.select-addr;: c72. A pool method of this type is called to
invoke the 'FrameSelectOfAddr' operation.

 -- C Type: typedef *note Res: 55f. (*PoolFrameHasAddrMethod)(*note
          Bool: 3a9. *inframeReturn, Pool pool, *note Seg: b53. seg,
          *note Addr: 632. *addrref, *note AllocFrame: b52. frame)

*note .fn.in-frame;: c74. A pool method of this type is called to invoke
the 'FrameHasAddr' operation.

 -- C Type: typedef *note Res: 55f. (*PoolSetFrameClassMethod)(Pool
          pool, Buffer buf, *note AllocFrameClass: c69. class)

*note .fn.set;: c76. A pool method of this type is called to invoke the
'SetFrameClass' operation.


File: MemoryPoolSystem.info,  Node: Lightweight frames,  Prev: Interface<20>,  Up: Allocation frame protocol

5.1.8 Lightweight frames
------------------------

* Menu:

* Overview: Overview<12>.
* Synchronization::
* Implementation: Implementation<15>.


File: MemoryPoolSystem.info,  Node: Overview<12>,  Next: Synchronization,  Up: Lightweight frames

5.1.8.1 Overview
................

*note .lw-frame.overview;: c79. Allocation points provide direct support
for lightweight frames, and are designed to permit 'FramePush' and
'FramePop' operations without the need for locking and delegation to the
pool method.  The pool method will be called whenever synchronization is
required for other reasons (e.g.  the buffer is tripped).

*note .lw-frame.model;: c7a. Lightweight frames offer direct support for
a particular model of allocation frame use, whereby the 'FramePush'
operation returns the current allocation pointer as a frame handle, and
the 'FramePop' operation causes the allocation pointer to be reset to
the address of the frame handle.  This model should be suitable for
simple stack frames, where more advanced operations like 'FrameSelect'
are not supported.  It may also be suitable for more advanced allocation
frame models when they are being used simply.  The use of a complex
operation always involves synchronization via locking, and the pool may
disable lightweight synchronization temporarily at this time.


File: MemoryPoolSystem.info,  Node: Synchronization,  Next: Implementation<15>,  Prev: Overview<12>,  Up: Lightweight frames

5.1.8.2 Synchronization
.......................

*note .lw-frame.sync;: c7c. The purpose of the design is that mutator
may access the state of an AP without locking with MPS (via the external
functions).  The design assumes the normal MPS restriction that an
operation on an AP may only be performed by a single mutator thread at a
time.  Each of the operations on allocation frames counts as an
operation on an AP.


File: MemoryPoolSystem.info,  Node: Implementation<15>,  Prev: Synchronization,  Up: Lightweight frames

5.1.8.3 Implementation
......................

*note .lw-frame.push;: c7e. The external 'FramePush' operation *note
mps_ap_frame_push(): 16e. performs the following operations:

     IF ap->init != ap->alloc
        FAIL
     ELSE IF ap->init < ap->limit
        *frame_o = ap->init;
     ELSE
       WITH_ARENA_LOCK
         PerformInternalFramePushOperation(...)
       END
     END

*note .lw-frame.push.limit;: c7f. The reason for testing ‘ap->init <
ap->limit’ and not ‘ap->init <= ap->limit’ is that a frame pointer at
the limit of a buffer (and possibly therefore of a segment) would be
ambiguous: is it at the limit of the segment, or at the base of the
segment that’s adjacent in memory?  The internal operation must handle
this case, for example by refilling the buffer and setting the frame at
the beginning.

*note .lw-frame.pop;: c80. The external 'FramePop' operation (*note
mps_ap_frame_pop(): 16d.) performs the following operations:

     IF ap->init != ap->alloc
        FAIL
     ELSE IF BufferBase(ap) <= frame AND frame < ap->init
        ap->init = ap->alloc = frame;
     ELSE
       WITH_ARENA_LOCK
         PerformInternalFramePopOperation(...)
       END
     END

*note .lw-frame.pop.buffer;: c81. The reason for testing that ‘frame’ is
in the buffer is that if it’s not, then we’re popping to an address in
some other segment, and that means that some objects in the other
segment (and all objects in any segments on the stack in between) are
now dead, and the only way for the pool to mark them as being dead is to
do a heavyweight pop.


File: MemoryPoolSystem.info,  Node: Arena,  Next: Virtual Memory Arena,  Prev: Allocation frame protocol,  Up: Old design

5.2 Arena
=========

* Menu:

* Introduction: Introduction<48>.
* Overview: Overview<13>.
* Definitions: Definitions<7>.
* Requirements: Requirements<29>.
* Architecture: Architecture<4>.
* Implementation: Implementation<16>.


File: MemoryPoolSystem.info,  Node: Introduction<48>,  Next: Overview<13>,  Up: Arena

5.2.1 Introduction
------------------

*note .intro;: c87. This is the design of the arena structure.

*note .readership;: c88. MPS developers.


File: MemoryPoolSystem.info,  Node: Overview<13>,  Next: Definitions<7>,  Prev: Introduction<48>,  Up: Arena

5.2.2 Overview
--------------

*note .overview;: c8a. The arena serves two purposes.  It is a structure
that is the top-level state of the MPS, and as such contains a lot of
fields which are considered “global”.  And it provides raw memory to
pools.

An arena belongs to a particular arena class.  The class is selected
when the arena is created.  Classes encapsulate both policy (such as how
pool placement preferences map into actual placement) and mechanism
(such as where the memory originates: operating system virtual memory,
client provided, or via malloc).  Some behaviour (mostly serving the
“top-level datastructure” purpose) is implemented by generic arena code,
and some by arena class code.


File: MemoryPoolSystem.info,  Node: Definitions<7>,  Next: Requirements<29>,  Prev: Overview<13>,  Up: Arena

5.2.3 Definitions
-----------------

*note .def.grain;: c8c. The arena manages memory in units called 'arena
grains', whose size is returned by the macro ‘ArenaGrainSize()’.  Memory
allocated by ‘ArenaAlloc()’ is a contiguous sequence of arena grains,
whose base address and size are multiples of the arena grain size.

*note .def.tract;: c8d. A tract is a data structure containing
information about a region of address space: which pool it belongs to
(if any), which segment contains it, and so on.  Tracts are the hook on
which the segment module is implemented.  Pools which don’t use segments
may use tracts for associating their own data with ranges of address.


File: MemoryPoolSystem.info,  Node: Requirements<29>,  Next: Architecture<4>,  Prev: Definitions<7>,  Up: Arena

5.2.4 Requirements
------------------

     Note: Where do these come from?  Need to identify and document the
     sources of requirements so that they are traceable to client
     requirements.  Most of these come from the architectural design
     (design.mps.architecture) or the fix function design
     (design.mps.fix(1)).  Richard Brooksby, 1995-08-28.

     They were copied from design.mps.arena.vm(2) and edited slightly.
     David Jones, 1999-06-23.

* Menu:

* Block management::
* Address translation::
* Arena partition::
* Constraints: Constraints<2>.

   ---------- Footnotes ----------

   (1) fix.html

   (2) arenavm.html


File: MemoryPoolSystem.info,  Node: Block management,  Next: Address translation,  Up: Requirements<29>

5.2.4.1 Block management
........................

*note .req.fun.block.alloc;: c90. The arena must provide allocation of
contiguous blocks of memory.

*note .req.fun.block.free;: c91. It must also provide freeing of
contiguously allocated blocks owned by a pool, whether or not the block
was allocated via a single request.

*note .req.attr.block.size.min;: c92. The arena must support management
of blocks down to the larger of (i) the grain size of the virtual
mapping interface (if a virtual memory interface is being used); and
(ii) the grain size of the memory protection interface (if protection is
used).

     Note: On all the operating systems we support, these grain sizes
     are the same and are equal to the operating system page size.  But
     we want the MPS to remain flexible enough to be ported to operating
     systems where these are different.

*note .req.attr.block.size.max;: c93. It must also support management of
blocks up to the maximum size allowed by the combination of operating
system and architecture.  This is derived from req.dylan.attr.obj.max
(at least).

*note .req.attr.block.align.min;: c94. The alignment of blocks shall not
be less than *note MPS_PF_ALIGN: 6f. for the architecture.  This is so
that pool classes can conveniently guarantee pool allocated blocks are
aligned to *note MPS_PF_ALIGN: 6f.  (A trivial requirement.)


File: MemoryPoolSystem.info,  Node: Address translation,  Next: Arena partition,  Prev: Block management,  Up: Requirements<29>

5.2.4.2 Address translation
...........................

*note .req.fun.trans;: c96. The arena must provide a translation from
any address to the following information:

*note .req.fun.trans.arena;: c97. Whether the address is managed by the
arena.

*note .req.fun.trans.pool;: c98. Whether the address is managed by a
pool within the arena, and if it is, the pool.

*note .req.fun.trans.arbitrary;: c99. If the address is managed by a
pool, an arbitrary pointer value that the pool can associate with a
group of contiguous addresses at any time.

*note .req.fun.trans.white;: c9a. If the address is managed by an
automatic pool, the set of traces for which the address is white.  This
is required so that the second-stage fix protocol can reject non-white
addresses quickly.  See design.mps.critical-path(1).

*note .req.attr.trans.time;: c9b. The translation shall take no more
than @@@@ [something not very large – drj 1999-06-23]

   ---------- Footnotes ----------

   (1) critical-path.html


File: MemoryPoolSystem.info,  Node: Arena partition,  Next: Constraints<2>,  Prev: Address translation,  Up: Requirements<29>

5.2.4.3 Arena partition
.......................

*note .req.fun.set;: c9d. The arena must provide a method for
approximating sets of addresses.

*note .req.fun.set.time;: c9e. The determination of membership shall
take no more than @@@@ [something very small indeed].  (the non-obvious
solution is refsets)


File: MemoryPoolSystem.info,  Node: Constraints<2>,  Prev: Arena partition,  Up: Requirements<29>

5.2.4.4 Constraints
...................

*note .req.attr.space.overhead;: ca0. req.dylan.attr.space.struct
implies that the arena must limit the space overhead.  The arena is not
the only part that introduces an overhead (pool classes being the next
most obvious), so multiple parts must cooperate in order to meet the
ultimate requirements.

*note .req.attr.time.overhead;: ca1. Time overhead constraint?

     Note: How can there be a time “overhead” on a necessary component?
     David Jones, 1999-06-23.


File: MemoryPoolSystem.info,  Node: Architecture<4>,  Next: Implementation<16>,  Prev: Requirements<29>,  Up: Arena

5.2.5 Architecture
------------------

* Menu:

* Statics::
* Arena classes::
* Chunks::
* Tracts::
* Control pool::
* Polling::
* Commit limit::
* Spare committed (aka “hysteresis”): Spare committed aka “hysteresis”.
* Pause time control::
* Locks::
* Location dependencies::
* Finalization: Finalization<4>.


File: MemoryPoolSystem.info,  Node: Statics,  Next: Arena classes,  Up: Architecture<4>

5.2.5.1 Statics
...............

*note .static;: ca4. There is no higher-level data structure than a
arena, so in order to support several arenas, we have to have some
static data in impl.c.arena.  See impl.c.arena.static.

*note .static.init;: ca5. All the static data items are initialized when
the first arena is created.

*note .static.serial;: ca6. ‘arenaSerial’ is a static *note Serial: b31,
containing the serial number of the next arena to be created.  The
serial of any existing arena is less than this.

*note .static.ring;: ca7. ‘arenaRing’ is the sentinel of the ring of
arenas.

*note .static.ring.init;: ca8. ‘arenaRingInit’ is a *note Bool: 3a9.
showing whether the ring of arenas has been initialized.

*note .static.ring.lock;: ca9. The ring of arenas has to be locked when
traversing the ring, to prevent arenas being added or removed.  This is
achieved by using the (non-recursive) global lock facility, provided by
the lock module.

*note .static.check;: caa. The statics are checked each time any arena
is checked.


File: MemoryPoolSystem.info,  Node: Arena classes,  Next: Chunks,  Prev: Statics,  Up: Architecture<4>

5.2.5.2 Arena classes
.....................

 -- C Type: typedef mps_arena_s *Arena

*note .class;: cac. The *note Arena: 796. data structure is designed to
be subclassable (see design.mps.protocol(1)).  Clients can select what
arena class they’d like when instantiating one with *note
mps_arena_create_k(): 52.  The arguments to *note mps_arena_create_k():
52. are class-dependent.

*note .class.fields;: cad. The ‘grainSize’ (for allocation and freeing)
and ‘zoneShift’ (for computing zone sizes and what zone an address is
in) fields in the arena are the responsibility of the each class, and
are initialized by the ‘init’ method.  The responsibility for
maintaining the ‘commitLimit’, ‘spareCommitted’, and ‘spareCommitLimit’
fields is shared between the (generic) arena and the arena class.
‘commitLimit’ (see *note .commit-limit: cae.) is changed by the generic
arena code, but arena classes are responsible for ensuring the
semantics.  For ‘spareCommitted’ and ‘spareCommitLimit’ see *note
.spare-committed: caf. below.

*note .class.abstract;: cb0. The basic arena class
(‘AbstractArenaClass’) is abstract and must not be instantiated.  It
provides little useful behaviour, and exists primarily as the root of
the tree of arena classes.  Each concrete class must specialize each of
the class method fields, with the exception of the describe method
(which has a trivial implementation) and the ‘extend’, ‘retract’ and
‘spareCommitExceeded’ methods which have non-callable methods for the
benefit of arena classes which don’t implement these features.

   ---------- Footnotes ----------

   (1) protocol.html


File: MemoryPoolSystem.info,  Node: Chunks,  Next: Tracts,  Prev: Arena classes,  Up: Architecture<4>

5.2.5.3 Chunks
..............

*note .chunk;: cb2. Each contiguous region of address space managed by
the MPS is represented by a 'chunk'.

*note .chunk.tracts;: cb3. A chunk contains a table of tracts.  See
*note .tract.table: cb4.

*note .chunk.lookup;: cb5. Looking of the chunk of an address is the
first step in the second-stage fix operation, and so on the critical
path.  See design.mps.critical-path(1).

*note .chunk.tree;: cb6. For efficient lookup, chunks are stored in a
balanced tree; ‘arena->chunkTree’ points to the root of the tree.
Operations on this tree must ensure that the tree remains balanced,
otherwise performance degrades badly with many chunks.

*note .chunk.insert;: cb7. New chunks are inserted into the tree by
calling ‘ArenaChunkInsert()’.  This calls ‘TreeInsert()’, followed by
‘TreeBalance()’ to ensure that the tree is balanced.

*note .chunk.delete;: cb8. There is no corresponding function
‘ArenaChunkDelete()’.  Instead, deletions from the chunk tree are
carried out by calling ‘TreeToVine()’, iterating over the vine (where
deletion is possible, if care is taken) and then calling ‘TreeBalance()’
on the remaining tree.  The function ‘TreeTraverseAndDelete()’
implements this.

*note .chunk.delete.justify;: cb9. This is because we don’t have a
function that deletes an item from a balanced tree efficiently, and
because all functions that delete chunks do so in a loop over the chunks
(so the best we can do is O('n') time in any case).

*note .chunk.delete.tricky;: cba. Deleting chunks from the chunk tree is
tricky in the virtual memory arena because ‘vmChunkDestroy()’ unmaps the
memory containing the chunk, which includes the tree node.  So the next
chunk must be looked up before deleting the current chunk.  The function
‘TreeTraverseAndDelete()’ ensures that this is done.

   ---------- Footnotes ----------

   (1) critical-path.html


File: MemoryPoolSystem.info,  Node: Tracts,  Next: Control pool,  Prev: Chunks,  Up: Architecture<4>

5.2.5.4 Tracts
..............

*note .tract.table;: cb4. The arena maintains tables of tract structures
such that every address managed by the arena belongs to exactly one
tract.

*note .tract.size;: cbc. Each tract covers exactly one arena grain.
This is an implementation detail, not a requirement.

*note .tract.structure;: cbd. The tract structure definition looks like
this:

     typedef struct TractStruct { /* Tract structure */
       Pool pool;      /* MUST BE FIRST <design/arena#.tract.field.pool> */
       Seg seg;                     /* NULL or segment containing tract */
       Addr base;                   /* Base address of the tract */
     } TractStruct;

*note .tract.field.pool;: cbe. The pool field indicates to which pool
the tract has been allocated (*note .req.fun.trans.pool: c98.).  Tracts
are only valid when they are allocated to pools.  When tracts are not
allocated to pools, arena classes are free to reuse tract objects in
undefined ways.  A standard technique is for arena class implementations
to internally describe the objects as a union type of ‘TractStruct’ and
some private representation, and to set the pool field to ‘NULL’ when
the tract is not allocated.  The pool field must come first so that the
private representation can share a common prefix with ‘TractStruct’.
This permits arena classes to determine from their private
representation whether such an object is allocated or not, without
requiring an extra field.

*note .tract.field.seg;: cbf. The seg field is a pointer to the segment
containing the tract, or ‘NULL’ if the tract is not contained in any
segment.

*note .tract.field.base;: cc0. The base field contains the base address
of the memory represented by the tract.

*note .tract.limit;: cc1. The limit of the tract’s memory may be
determined by adding the arena grain size to the base address.

*note .tract.iteration;: cc2. Iteration over tracts is described in
design.mps.arena.tract-iter(0).

 -- C Function: *note Bool: 3a9. TractOfAddr (Tract *tractReturn, Arena
          arena, Addr addr)

*note .tract.if.tractofaddr;: cc3. The function *note TractOfAddr():
4c4. finds the tract corresponding to an address in memory.  (See *note
.req.fun.trans: c96.)  If ‘addr’ is an address which has been allocated
to some pool, then *note TractOfAddr(): 4c4. returns ‘TRUE’, and sets
‘*tractReturn’ to the tract corresponding to that address.  Otherwise,
it returns ‘FALSE’.  This function is similar to ‘TractOfBaseAddr()’
(see design.mps.arena.tract-iter.if.contig-base) but serves a more
general purpose and is less efficient.


File: MemoryPoolSystem.info,  Node: Control pool,  Next: Polling,  Prev: Tracts,  Up: Architecture<4>

5.2.5.5 Control pool
....................

*note .pool;: cc5. Each arena has a “control pool”,
‘arena->controlPoolStruct’, which is used for allocating MPS control
data structures by calling ‘ControlAlloc()’.


File: MemoryPoolSystem.info,  Node: Polling,  Next: Commit limit,  Prev: Control pool,  Up: Architecture<4>

5.2.5.6 Polling
...............

*note .poll;: cc7. ‘ArenaPoll()’ is called “often” by other code (for
instance, on buffer fill or allocation).  It is the entry point for
doing tracing work.  If the polling clock exceeds a set threshold, and
we’re not already doing some tracing work (that is, ‘insidePoll’ is not
set), it calls ‘TracePoll()’ on all busy traces.

*note .poll.size;: cc8. The actual clock is ‘arena->fillMutatorSize’.
This is because internal allocation is only significant when copy
segments are being allocated, and we don’t want to have the pause times
to shrink because of that.  There is no current requirement for the
trace rate to guard against running out of memory.

     Note: Clearly it really ought to: we have a requirement to not run
     out of memory (see req.dylan.prot.fail-alloc,
     req.dylan.prot.consult), and emergency tracing should not be our
     only story.  David Jones, 1999-06-22.

‘BufferEmpty()’ is not taken into account, because the splinter will
rarely be useable for allocation and we are wary of the clock running
backward.

*note .poll.clamp;: cc9. Polling is disabled when the arena is
“clamped”, in which case ‘arena->clamped’ is ‘TRUE’.  Clamping the arena
prevents background tracing work, and further new garbage collections
from starting.  Clamping and releasing are implemented by the
‘ArenaClamp()’ and ‘ArenaRelease()’ methods.

*note .poll.park;: cca. The arena is “parked” by clamping it, then
polling until there are no active traces.  This finishes all the active
collections and prevents further collection.  Parking is implemented by
the ‘ArenaPark()’ method.


File: MemoryPoolSystem.info,  Node: Commit limit,  Next: Spare committed aka “hysteresis”,  Prev: Polling,  Up: Architecture<4>

5.2.5.7 Commit limit
....................

*note .commit-limit;: cae. The arena supports a client configurable
“commit limit” which is a limit on the total amount of committed memory.
The generic arena structure contains a field to hold the value of the
commit limit and the implementation provides two functions for
manipulating it: ‘ArenaCommitLimit()’ to read it, and
‘ArenaSetCommitLimit()’ to set it.  Actually abiding by the contract of
not committing more memory than the commit limit is left up to the
individual arena classes.

*note .commit-limit.err;: ccc. When allocation from the arena would
otherwise succeed but cause the MPS to use more committed memory than
specified by the commit limit ‘ArenaAlloc()’ should refuse the request
and return ‘ResCOMMIT_LIMIT’.

*note .commit-limit.err.multi;: ccd. In the case where an ‘ArenaAlloc()’
request cannot be fulfilled for more than one reason including exceeding
the commit limit then class implementations should strive to return a
result code other than ‘ResCOMMIT_LIMIT’.  That is, ‘ResCOMMIT_LIMIT’
should only be returned if the 'only' reason for failing the
‘ArenaAlloc()’ request is that the commit limit would be exceeded.  The
client documentation allows implementations to be ambiguous with respect
to which result code in returned in such a situation however.


File: MemoryPoolSystem.info,  Node: Spare committed aka “hysteresis”,  Next: Pause time control,  Prev: Commit limit,  Up: Architecture<4>

5.2.5.8 Spare committed (aka “hysteresis”)
..........................................

*note .spare-committed;: caf. See *note mps_arena_spare_committed():
191.  The generic arena structure contains two fields for the spare
committed memory fund: ‘spareCommitted’ records the total number of
spare committed bytes; ‘spareCommitLimit’ records the limit (set by the
user) on the amount of spare committed memory.  ‘spareCommitted’ is
modified by the arena class but its value is used by the generic arena
code.  There are two uses: a getter function for this value is provided
through the MPS interface (*note mps_arena_spare_commit_limit(): 320.),
and by the ‘ArenaSetSpareCommitLimit()’ function to determine whether
the amount of spare committed memory needs to be reduced.
‘spareCommitLimit’ is manipulated by generic arena code, however the
associated semantics are the responsibility of the class.  It is the
class’s responsibility to ensure that it doesn’t use more spare
committed bytes than the value in ‘spareCommitLimit’.

*note .spare-commit-limit;: ccf. The function
‘ArenaSetSpareCommitLimit()’ sets the ‘spareCommitLimit’ field.  If the
limit is set to a value lower than the amount of spare committed memory
(stored in ‘spareCommitted’) then the class specific function
‘spareCommitExceeded’ is called.


File: MemoryPoolSystem.info,  Node: Pause time control,  Next: Locks,  Prev: Spare committed aka “hysteresis”,  Up: Architecture<4>

5.2.5.9 Pause time control
..........................

*note .pause-time;: cd1. The generic arena structure contains the field
‘pauseTime’ for the maximum time any operation in the arena may take
before returning to the mutator.  This value is used by *note
PolicyPollAgain(): cd2. to decide whether to do another unit of tracing
work.  The MPS interface provides getter (*note mps_arena_pause_time():
193.) and setter (*note mps_arena_pause_time_set(): 180.) functions.


File: MemoryPoolSystem.info,  Node: Locks,  Next: Location dependencies,  Prev: Pause time control,  Up: Architecture<4>

5.2.5.10 Locks
..............

*note .lock.ring;: cd4. ‘ArenaAccess()’ is called when we fault on a
barrier.  The first thing it does is claim the non-recursive global lock
to protect the arena ring (see design.mps.lock(0)).

*note .lock.arena;: cd5. After the arena ring lock is claimed,
‘ArenaEnter()’ is called on one or more arenas.  This claims the lock
for that arena.  When the correct arena is identified or we run out of
arenas, the lock on the ring is released.

*note .lock.avoid;: cd6. Deadlocking is avoided as described below:

*note .lock.avoid.mps;: cd7. Firstly we require the MPS not to fault
(that is, when any of these locks are held by a thread, that thread does
not fault).

*note .lock.avoid.thread;: cd8. Secondly, we require that in a
multi-threaded system, memory fault handlers do not suspend threads
(although the faulting thread will, of course, wait for the fault
handler to finish).

*note .lock.avoid.conflict;: cd9. Thirdly, we avoid conflicting deadlock
between the arena and global locks by ensuring we never claim the arena
lock when the recursive global lock is already held, and we never claim
the binary global lock when the arena lock is held.


File: MemoryPoolSystem.info,  Node: Location dependencies,  Next: Finalization<4>,  Prev: Locks,  Up: Architecture<4>

5.2.5.11 Location dependencies
..............................

*note .ld;: cdb. Location dependencies use fields in the arena to
maintain a history of summaries of moved objects, and to keep a notion
of time, so that the staleness of location dependency can be determined.


File: MemoryPoolSystem.info,  Node: Finalization<4>,  Prev: Location dependencies,  Up: Architecture<4>

5.2.5.12 Finalization
.....................

*note .final;: cdd. There is a pool which is optionally (and
dynamically) instantiated to implement finalization.  The fields
‘finalPool’ and ‘isFinalPool’ are used.


File: MemoryPoolSystem.info,  Node: Implementation<16>,  Prev: Architecture<4>,  Up: Arena

5.2.6 Implementation
--------------------

* Menu:

* Tract cache::
* Control pool: Control pool<2>.
* Traces::
* Polling: Polling<2>.
* Location dependencies: Location dependencies<2>.
* Roots: Roots<3>.


File: MemoryPoolSystem.info,  Node: Tract cache,  Next: Control pool<2>,  Up: Implementation<16>

5.2.6.1 Tract cache
...................

*note .impl.tract.cache;: ce0. When tracts are allocated to pools by
‘ArenaAlloc()’, the first tract of the block and it’s base address are
cached in arena fields ‘lastTract’ and ‘lastTractBase’.  The function
‘TractOfBaseAddr()’ (see design.mps.arena.tract-iter.if.block-base(0))
checks against these cached values and only calls the class method on a
cache miss.  This optimizes for the common case where a pool allocates a
block and then iterates over all its tracts (for example, to attach them
to a segment).

*note .impl.tract.uncache;: ce1. When blocks of memory are freed by
pools, ‘ArenaFree()’ checks to see if the cached value for the most
recently allocated tract (see *note .impl.tract.cache: ce0.) is being
freed.  If so, the cache is invalid, and must be reset.  The ‘lastTract’
and ‘lastTractBase’ fields are set to ‘NULL’.


File: MemoryPoolSystem.info,  Node: Control pool<2>,  Next: Traces,  Prev: Tract cache,  Up: Implementation<16>

5.2.6.2 Control pool
....................

*note .impl.pool.init;: ce3. The control pool is initialized by a call
to ‘PoolInit()’ during ‘ArenaCreate()’.

*note .impl.pool.ready;: ce4. All the other fields in the arena are made
checkable before calling ‘PoolInit()’, so ‘PoolInit()’ can call
‘ArenaCheck(arena)’.  The pool itself is, of course, not checkable, so
we have a field ‘arena->poolReady’, which is false until after the
return from ‘PoolInit()’.  ‘ArenaCheck()’ only checks the pool if
‘poolReady’.


File: MemoryPoolSystem.info,  Node: Traces,  Next: Polling<2>,  Prev: Control pool<2>,  Up: Implementation<16>

5.2.6.3 Traces
..............

*note .impl.trace;: ce6. ‘arena->trace[ti]’ is valid if and only if
‘TraceSetIsMember(arena->busyTraces, ti)’.

*note .impl.trace.create;: ce7. Since the arena created by
‘ArenaCreate()’ has ‘arena->busyTraces = TraceSetEMPTY’, none of the
traces are meaningful.

*note .impl.trace.invalid;: ce8. Invalid traces have signature
‘SigInvalid’, which can be checked.


File: MemoryPoolSystem.info,  Node: Polling<2>,  Next: Location dependencies<2>,  Prev: Traces,  Up: Implementation<16>

5.2.6.4 Polling
...............

*note .impl.poll.fields;: cea. There are three fields of a arena used
for polling: ‘pollThreshold’, ‘insidePoll’, and ‘clamped’ (see above).
‘pollThreshold’ is the threshold for the next poll: it is set at the end
of ‘ArenaPoll()’ to the current polling time plus ‘ARENA_POLL_MAX’.


File: MemoryPoolSystem.info,  Node: Location dependencies<2>,  Next: Roots<3>,  Prev: Polling<2>,  Up: Implementation<16>

5.2.6.5 Location dependencies
.............................

*note .impl.ld;: cec. The ‘historyStruct’ contains fields used to
maintain a history of garbage collection and in particular object motion
in order to implement location dependency.

*note .impl.ld.epoch;: ced. The ‘epoch’ is the “current epoch”.  This is
the number of “flips” of traces, in which objects might have moved, in
the arena since it was created.  From the mutator’s point of view,
locations change atomically at flip.

*note .impl.ld.history;: cee. The ‘history’ is a circular buffer of
‘LDHistoryLENGTH’ elements of type *note RefSet: b26.  These are the
summaries of moved objects since the last ‘LDHistoryLENGTH’ epochs.  If
‘e’ is one of these recent epochs, then

     history->history[e % LDHistoryLENGTH]

is a summary of (the original locations of) objects moved since epoch
‘e’.

*note .impl.ld.prehistory;: cef. The ‘prehistory’ is a *note RefSet:
b26. summarizing the original locations of all objects ever moved.  When
considering whether a really old location dependency is stale, it is
compared with this summary.


File: MemoryPoolSystem.info,  Node: Roots<3>,  Prev: Location dependencies<2>,  Up: Implementation<16>

5.2.6.6 Roots
.............

*note .impl.root-ring;: cf1. The arena holds a member of a ring of roots
in the arena.  It holds an incremental serial which is the serial of the
next root.


File: MemoryPoolSystem.info,  Node: Virtual Memory Arena,  Next: Bit tables,  Prev: Arena,  Up: Old design

5.3 Virtual Memory Arena
========================

* Menu:

* Introduction: Introduction<49>.
* Overview: Overview<14>.
* Notes: Notes<3>.
* Requirements: Requirements<30>.
* Architecture: Architecture<5>.
* Solution ideas::
* Data structures: Data structures<2>.
* Notes: Notes<4>.


File: MemoryPoolSystem.info,  Node: Introduction<49>,  Next: Overview<14>,  Up: Virtual Memory Arena

5.3.1 Introduction
------------------

*note .intro;: cf7. This is the design of the Virtual Memory Arena Class
of the Memory Pool System.  The VM Arena Class is just one class
available in the MPS. The generic arena part is described in
design.mps.arena(1).

   ---------- Footnotes ----------

   (1) arena.html


File: MemoryPoolSystem.info,  Node: Overview<14>,  Next: Notes<3>,  Prev: Introduction<49>,  Up: Virtual Memory Arena

5.3.2 Overview
--------------

*note .overview;: cfa. VM arenas provide blocks of memory to all other
parts of the MPS in the form of “tracts” using the virtual mapping
interface (design.mps.vm(1)) to the operating system.  The VM Arena
Class is not expected to be provided on platforms that do not have
virtual memory (for example, Macintosh System 7).

*note .overview.gc;: cfb. The VM Arena Class provides some special
services on these blocks in order to facilitate garbage collection:

*note .overview.gc.zone;: cfc. Allocation of blocks with specific zones.
This means that the generic fix function (design.mps.fix(2)) can use a
fast refset test to eliminate references to addresses that are not in
the condemned set.  This assumes that a pool class that uses this
placement appropriately is being used (such as the generation placement
policy used by AMC: see design.mps.poolamc(3)) and that the pool selects
the condemned sets to coincide with zone stripes.

*note .overview.gc.tract;: cfd. A fast translation from addresses to
tract.  (See design.mps.arena.req.fun.trans(4).)

   ---------- Footnotes ----------

   (1) vm.html

   (2) fix.html

   (3) poolamc.html

   (4) arena.html#design.mps.arena.req.fun.trans


File: MemoryPoolSystem.info,  Node: Notes<3>,  Next: Requirements<30>,  Prev: Overview<14>,  Up: Virtual Memory Arena

5.3.3 Notes
-----------

*note .note.refset;: d00. Some of this document simply assumes that
RefSets (see design.mps.collections.refsets(1)) have been chosen as the
solution for design.mps.arena.req.fun.set(2).  It’s a lot simpler that
way.  Both to write and understand.

   ---------- Footnotes ----------

   (1) collections.html#design.mps.collections.refsets

   (2) arena.html#design.mps.arena.req.fun.set


File: MemoryPoolSystem.info,  Node: Requirements<30>,  Next: Architecture<5>,  Prev: Notes<3>,  Up: Virtual Memory Arena

5.3.4 Requirements
------------------

Most of the requirements are in fact on the generic arena (see
design.mps.arena.req(1)).  However, many of those requirements can only
be met by a suitable arena class design.

Requirements particular to this arena class:

* Menu:

* Placement::
* Arena partition: Arena partition<2>.

   ---------- Footnotes ----------

   (1) arena.html#design.mps.arena.req


File: MemoryPoolSystem.info,  Node: Placement,  Next: Arena partition<2>,  Up: Requirements<30>

5.3.4.1 Placement
.................

*note .req.fun.place;: d04. It must be possible for pools to obtain
tracts at particular addresses.  Such addresses shall be declared by the
pool specifying what refset zones the tracts should lie in and what
refset zones the tracts should not lie in.  It is acceptable for the
arena to not always honour the request in terms of placement if it has
run out of suitable addresses.


File: MemoryPoolSystem.info,  Node: Arena partition<2>,  Prev: Placement,  Up: Requirements<30>

5.3.4.2 Arena partition
.......................

*note .req.fun.set;: d06. See design.mps.arena.req.fun.set(1).  The
approximation to sets of address must cooperate with the placement
mechanism in the way required by *note .req.fun.place: d04. (above).

   ---------- Footnotes ----------

   (1) arena.html#design.mps.arena.req.fun.set


File: MemoryPoolSystem.info,  Node: Architecture<5>,  Next: Solution ideas,  Prev: Requirements<30>,  Up: Virtual Memory Arena

5.3.5 Architecture
------------------

*note .arch.memory;: d08. The underlying memory is obtained from
whatever Virtual Memory interface (see design.mps.vm(1)).  @@@@ Explain
why this is used.

   ---------- Footnotes ----------

   (1) vm.html


File: MemoryPoolSystem.info,  Node: Solution ideas,  Next: Data structures<2>,  Prev: Architecture<5>,  Up: Virtual Memory Arena

5.3.6 Solution ideas
--------------------

*note .idea.grain;: d0a. Set the arena granularity to the grain provided
by the virtual mapping module.

*note .idea.mem;: d0b. Get a single large contiguous address area from
the virtual mapping interface and divide that up.

*note .idea.table;: d0c. Maintain a table with one entry per grain in
order to provide fast mapping (shift and add) between addresses and
table entries.

*note .idea.table.figure;: d0d. [missing figure]

*note .idea.map;: d0e. Store the pointers (design.arena.req.fun.trans)
in the table directly for every grain.

*note .idea.zones;: d0f. Partition the managed address space into zones
(see idea.zones) and provide the set approximation as a reference
signature.

*note .idea.first-fit;: d10. Use a simple first-fit allocation policy
for tracts within each zone (*note .idea.zones: d0f.).  Store the
freelist in the table (*note .idea.table: d0c.).

*note .idea.base;: d11. Store information about each contiguous area
(allocated of free) in the table entry (*note .idea.table: d0c.)
corresponding to the base address of the area.

*note .idea.shadow;: d12. Use the table (*note .idea.table: d0c.) as a
“shadow” of the operating system’s page table.  Keep information such as
last access, protection, etc.  in this table, since we can’t get at this
information otherwise.

*note .idea.barrier;: d13. Use the table (*note .idea.table: d0c.) to
implement the software barrier.  Each segment can have a read and/or
write barrier placed on it by each process.  (*note .idea.barrier.bits;:
d14. Store a bit-pattern which remembers which process protected what.)
This will give a fast translation from a barrier-protected address to
the barrier handler via the process table.

*note .idea.demand-table;: d15. For a 1 GiB managed address space with a
4 KiB page size, the table will have 256K-entries.  At, say, four words
per entry, this is 4 MiB of table.  Although this is only an 0.4%, the
table shouldn’t be preallocated or initially it is an infinite overhead,
and with 1 MiB active, it is a 300% overhead!  The address space for the
table should be reserved, but the pages for it mapped and unmapped on
demand.  By storing the table in a tract, the status of the table’s
pages can be determined by looking at it’s own entries in itself, and
thus the translation lookup (design.arena.req.fun.trans) is slowed to
two lookups rather than one.

*note .idea.pool;: d16. Make the Arena Manager a pool class.  Arena
initialization becomes pool creation.  Tract allocation becomes
‘PoolAlloc()’.  Other operations become class-specific operations on the
“arena pool”.


File: MemoryPoolSystem.info,  Node: Data structures<2>,  Next: Notes<4>,  Prev: Solution ideas,  Up: Virtual Memory Arena

5.3.7 Data structures
---------------------

*note .tables;: d18. There are two table data structures: a page table,
and an alloc table.

*note .table.page.map;: d19. Each page in the VM has a corresponding
page table entry.

*note .table.page.linear;: d1a. The table is a linear array of
PageStruct entries; there is a simple mapping between the index in the
table and the base address in the VM. Namely:

   - index to base address: ‘base-address = arena-base + (index *
     page-size)’

   - base address to index: ‘index = (base-address - arena-base) /
     page-size’

*note .table.page.partial;: d1b. The table is partially mapped on an
“as-needed” basis using the SparseArray abstract type.

*note .table.page.tract;: d1c. Each page table entry contains a tract,
which is only valid if it is allocated to a pool.  If it is not
allocated to a pool, the fields of the tract are used for other
purposes.  (See design.mps.arena.tract.field.pool(1))

*note .table.alloc;: d1d. The alloc table is a simple bit table
(implemented using the BT module, design.mps.bt(2)).

*note .table.alloc.map;: d1e. Each page in the VM has a corresponding
alloc table entry.

*note .table.alloc.semantics;: d1f. The bit in the alloc table is set
iff the corresponding page is allocated (to a pool).

   ---------- Footnotes ----------

   (1) arena.html#design.mps.arena.tract.field.pool

   (2) bt.html


File: MemoryPoolSystem.info,  Node: Notes<4>,  Prev: Data structures<2>,  Up: Virtual Memory Arena

5.3.8 Notes
-----------

*note .fig.page;: d21. How the pages in the arena area are represented
in the tables.

[missing figure]

*note .fig.count;: d22. How a count table can be used to partially map
the page table, as proposed in request.dylan.170049.sol.map(1).

[missing figure]

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170049


File: MemoryPoolSystem.info,  Node: Bit tables,  Next: Allocation buffers and allocation points,  Prev: Virtual Memory Arena,  Up: Old design

5.4 Bit tables
==============

* Menu:

* Introduction: Introduction<50>.
* Definitions: Definitions<8>.
* Requirements: Requirements<31>.
* Non requirements::
* Background: Background<3>.
* Clients::
* Overview: Overview<15>.
* Interface: Interface<21>.
* Detailed design::
* Testing: Testing<6>.
* References: References<21>.


File: MemoryPoolSystem.info,  Node: Introduction<50>,  Next: Definitions<8>,  Up: Bit tables

5.4.1 Introduction
------------------

*note .intro;: d28. This is the design of the Bit Tables module.  A Bit
Table is a linear array of bits.  A Bit Table of length 'n' is indexed
using an integer from 0 up to (but not including) 'n'.  Each bit in a
Bit Table can hold either the value 0 (‘FALSE’) or 1 (‘TRUE’).  A
variety of operations are provided including: get, set, and reset
individual bits; set and reset a contiguous range of bits; search for a
contiguous range of reset bits; making a “negative image” copy of a
range.

*note .readership;: d29. MPS developers.


File: MemoryPoolSystem.info,  Node: Definitions<8>,  Next: Requirements<31>,  Prev: Introduction<50>,  Up: Bit tables

5.4.2 Definitions
-----------------

*note .def.set;: d2b. 'Set'

     Used as a verb meaning to assign the value 1 or ‘TRUE’ to a bit.
     Used descriptively to denote a bit containing the value 1.  Note 1
     and ‘TRUE’ are synonyms in MPS C code (see *note Bool: 3a9.).

*note .def.reset;: d2c. 'Reset'

     Used as a verb meaning to assign the value 0 or ‘FALSE’ to a bit.
     Used descriptively to denote a bit containing the value 0.  Note 0
     and ‘FALSE’ are synonyms in MPS C code (see *note Bool: 3a9.).

     Note: Consider using “fill/empty” or “mark/clear” instead of
     “set/reset”, set/reset is probably a hangover from drj’s z80
     hacking days – drj 1999-04-26

*note .def.bt;: d2d. 'Bit Table'

     A Bit Table is a mapping from [0, 'n') to {0,1} for some 'n',
     represented as a linear array of bits.

     *note .def.bt.justify;: d2e. They are called 'Bit Tables' because a
     single bit is used to encode whether the image of a particular
     integer under the map is 0 or 1.

*note .def.range;: d2f. 'Range'

     A contiguous sequence of bits in a Bit Table.  Ranges are typically
     specified as a 'base'–'limit' pair where the range includes the
     position specified by the base, but excludes that specified by the
     limit.  The mathematical interval notation for half-open intervals,
     ['base', 'limit'), is used.


File: MemoryPoolSystem.info,  Node: Requirements<31>,  Next: Non requirements,  Prev: Definitions<8>,  Up: Bit tables

5.4.3 Requirements
------------------

*note .req.bit;: d31. The storage for a Bit Table of 'n' bits shall take
no more than a small constant addition to the storage required for 'n'
bits.  *note .req.bit.why;: d32. This is so that clients can make some
predictions about how much storage their algorithms use.  A small
constant is allowed over the minimal for two reasons: inevitable
implementation overheads (such as only being able to allocate storage in
multiples of 32 bits), extra storage for robustness or speed (such as
signature and length fields).

*note .req.create;: d33. A means to create Bit Tables.  *note
.req.create.why;: d34. Obvious.

*note .req.destroy;: d35. A means to destroy Bit Tables.  *note
.req.destroy.why;: d36. Obvious.

*note .req.ops;: d37. The following operations shall be supported:

   * *note .req.ops.get;: d38. 'Get'.  Get the value of a bit at a
     specified index.

   * *note .req.ops.set;: d39. 'Set'.  Set a bit at a specified index.

   * *note .req.ops.reset;: d3a. 'Reset'.  Reset a bit at a specified
     index.

*note .req.ops.minimal.why;: d3b. Get, Set, Reset, are the minimal
operations.  All possible mappings can be created and inspected using
these operations.

   * *note .req.ops.set.range;: d3c. 'SetRange'.  Set a range of bits.
     *note .req.ops.set.range.why;: d3d. It’s expected that clients will
     often want to set a range of bits; providing this operation allows
     the implementation of the BT module to make the operation
     efficient.

   * *note .req.ops.reset.range;: d3e. 'ResetRange'.  Reset a range of
     bits.  *note .req.ops.reset.range.why;: d3f. as for SetRange, see
     *note .req.ops.set.range.why: d3d.

   * *note .req.ops.test.range.set;: d40. 'IsSetRange'.  Test whether a
     range of bits are all set.  *note .req.ops.test.range.set.why;:
     d41. Mostly for checking.  For example, often clients will know
     that a range they are about to reset is currently all set, they can
     use this operation to assert that fact.

   * *note .req.ops.test.range.reset;: d42. 'IsResetRange'.  Test
     whether a range of bits are all reset.
     .req.ops.test.range.reset.why As for IsSetRange, see *note
     .req.ops.test.range.set.why: d41.

   * *note .req.ops.find;: d44. Find a range, which we’ll denote ['i',
     'j'), of at least 'L' reset bits that lies in a specified subrange
     of the entire Bit Table.  Various find operations are required
     according to the (additional) properties of the required range:

        * *note .req.ops.find.short.low;: d45. 'FindShortResetRange'.
          Of all candidate ranges, find the range with least 'j' (find
          the leftmost range that has at least 'L' reset bits and return
          just enough of that).  *note .req.ops.find.short.low.why;:
          d46. Required by client and VM arenas to allocate segments.
          The arenas implement definite placement policies (such as
          lowest addressed segment first) so they need the lowest (or
          highest) range that will do.  It’s not currently useful to
          allocate segments larger than the requested size, so finding a
          short range is sufficient.

        * *note .req.ops.find.short.high;: d47.
          'FindShortResetRangeHigh'.  Of all candidate ranges, find the
          range with greatest 'i' (find the rightmost range that has at
          least 'L' reset bits and return just enough of that).  *note
          .req.ops.find.short.high.why;: d48. Required by arenas to
          implement a specific segment placement policy (highest
          addressed segment first).

        * *note .req.ops.find.long.low;: d49. 'FindLongResetRange'.  Of
          all candidate ranges, identify the ranges with least 'i' and
          of those find the one with greatest 'j' (find the leftmost
          range that has at least 'L' reset bits and return all of it).
          .req.ops.find.long.low.why Required by the mark and sweep Pool
          Classes (AMS, AWL, LO) for allocating objects (filling a
          buffer).  It’s more efficient to fill a buffer with as much
          memory as is conveniently possible.  There’s no strong reason
          to find the lowest range but it’s bound to have some
          beneficial (small) cache effect and makes the algorithm more
          predictable.

        * *note .req.ops.find.long.high;: d4b. 'FindLongResetRangeHigh'.
          Provided, but not required, see *note
          .non-req.ops.find.long.high: d4c.

   * *note .req.ops.copy;: d4d. Copy a range of bits from one Bit Table
     to another Bit Table.  Various copy operations are required:

        * *note .req.ops.copy.simple;: d4e. Copy a range of bits from
          one Bit Table to the same position in another Bit Table.
          *note .req.ops.copy.simple.why;: d4f. Required to support
          copying of the tables for the “low” segment during segment
          merging and splitting, for pools using tables (for example,
          ‘PoolClassAMS’).

        * *note .req.ops.copy.offset;: d50. Copy a range of bits from
          one Bit Table to an offset position in another Bit Table.
          *note .req.ops.copy.offset.why;: d51. Required to support
          copying of the tables for the “high” segment during segment
          merging and splitting, for pools which support this (currently
          none, as of 2000-01-17).

        * *note .req.ops.copy.invert;: d52. Copy a range of bits from
          one Bit Table to the same position in another Bit Table
          inverting all the bits in the target copy.  *note
          .req.ops.copy.invert.why;: d53. Required by colour
          manipulation code in ‘PoolClassAMS’ and ‘PoolClassLO’.

*note .req.speed;: d54. Operations shall take no more than a few memory
operations per bit manipulated.  *note .req.speed.why;: d55. Any slower
would be gratuitous.

*note .req.speed.fast;: d56. The following operations shall be very
fast:

   * *note .req.speed.fast.find.short;: d57. FindShortResRange (the
     operation used to meet *note .req.ops.find.short.low: d45.)
     FindShortResRangeHigh (the operation used to meet *note
     .req.ops.find.short.high: d47.).

     *note .req.speed.fast.find.short.why;: d58. These two are used by
     the client arena (design.mps.arena.client) and the VM arena
     (design.mps.arena.vm(1)) for finding segments in page tables.  The
     operation will be used sufficiently often that its speed will
     noticeably affect the overall speed of the MPS. They will be called
     with a length equal to the number of pages in a segment.  Typical
     values of this length depend on the pool classes used and their
     configuration, but we can expect length to be small (1 to 16)
     usually.  We can expect the Bit Table to be populated densely where
     it is populated at all, that is set bits will tend to be clustered
     together in subranges.

   * *note .req.speed.fast.find.long;: d59. FindLongResRange (the
     operation used to meet *note .req.ops.find.long.low: d49.)

     *note .req.speed.fast.find.long.why;: d5a. Used in the allocator
     for ‘PoolClassAWL’ (design.mps.poolawl(2)), ‘PoolClassAMS’
     (design.mps.poolams(3)), ‘PoolClassEPVM’ (design.mps.poolepvm(0)).
     Of these AWL and EPVM have speed requirements.  For AWL the length
     of range to be found will be the length of a Dylan table in words.
     According to mail.tony.1999-05-05.11-36(4), only ‘<entry-vector>’
     objects are allocated in AWL (though not all ‘<entry-vector>’
     objects are allocated in AWL), and the mean length of an
     ‘<entry-vector>’ object is 486 Words.  No data for EPVM alas.

*note .req.speed.fast.other.why;: d5b. We might expect mark and sweep
pools to make use of Bit Tables, the MPS has general requirements to
support efficient mark and sweep pools, so that imposes general speed
requirements on Bit Tables.

   ---------- Footnotes ----------

   (1) arenavm.html

   (2) poolawl.html

   (3) poolams.html

   (4) 
https://info.ravenbrook.com/project/mps/mail/1999/05/05/11-36/0.txt


File: MemoryPoolSystem.info,  Node: Non requirements,  Next: Background<3>,  Prev: Requirements<31>,  Up: Bit tables

5.4.4 Non requirements
----------------------

The following are not requirements but the current design could support
them with little modification or does support them.  Often they used to
be requirements, but are no longer, or were added speculatively or
experimentally but aren’t currently used.

   * *note .non-req.ops.test.range.same;: d5d. 'RangesSame'.  Test
     whether two ranges that occupy the same positions in different Bit
     Tables are the same.  This used to be required by ‘PoolClassAMS’,
     but is no longer.  Currently (1999-05-04) the functionality still
     exists.

   * *note .non-req.ops.find.long.high;: d4c. 'FindLongResetRangeHigh'.
     (see *note .req.ops.find: d44.) Of all candidate ranges, identify
     the ranges with greatest 'j' and of those find the one with least
     'i' (find the rightmost range that has at least 'L' reset bits and
     return all of it).  Provided for symmetry but only currently used
     by the BT tests and ‘cbstest.c’.


File: MemoryPoolSystem.info,  Node: Background<3>,  Next: Clients,  Prev: Non requirements,  Up: Bit tables

5.4.5 Background
----------------

*note .background;: d5f. Originally Bit Tables were used and implemented
by ‘PoolClassLO’ (design.mps.poollo(1)).  It was decided to lift them
out into a separate module when designing the Pool to manage Dylan Weak
Tables which is also a mark and sweep pool and will make use of Bit
Tables (see design.mps.poolawl(2)).

*note .background.analysis;: d60. analysis.mps.bt(0) contains some of
the analysis of the design decisions that were and were not made in this
document.

   ---------- Footnotes ----------

   (1) poollo.html

   (2) poolawl.html


File: MemoryPoolSystem.info,  Node: Clients,  Next: Overview<15>,  Prev: Background<3>,  Up: Bit tables

5.4.6 Clients
-------------

*note .clients;: d62. Bit Tables are used throughout the MPS but the
important uses are in the client and VM arenas
(design.mps.arena.client(0) and design.mps.arena.vm(1)) a bit table is
used to record whether each page is free or not; several pool classes
(‘PoolClassLO’, ‘PoolClassEPVM’, ‘PoolClassAMS’) use bit tables to
record which locations are free and also to store colour.

   ---------- Footnotes ----------

   (1) arenavm.html


File: MemoryPoolSystem.info,  Node: Overview<15>,  Next: Interface<21>,  Prev: Clients,  Up: Bit tables

5.4.7 Overview
--------------

*note .over;: d64. Mostly, the design is as simple as possible.  The
significant complications are iteration (see *note .iteration: d65.
below) and searching (see *note .fun.find-res-range: d66. below) because
both of these are required to be fast.


File: MemoryPoolSystem.info,  Node: Interface<21>,  Next: Detailed design,  Prev: Overview<15>,  Up: Bit tables

5.4.8 Interface
---------------

 -- C Type: typedef *note Word: 653. *BT

*note .if.representation.abstract;: d68. A Bit Table is represented by
the type *note BT: 6b4.

*note .if.declare;: d69. The module declares a type *note BT: 6b4. and a
prototype for each of the functions below.  The type is declared in
impl.h.mpmtypes, the prototypes are declared in impl.h.mpm.  Some of the
functions are in fact implemented as macros in the usual way
(doc.mps.ref-man.if-conv(0).macro.std).

*note .if.general.index;: d6a. Many of the functions specified below
take indexes.  If otherwise unspecified an index must be in the interval
[0, 'n') (note, up to, but not including, 'n') where 'n' is the number
of bits in the relevant Bit Table (as passed to the *note BTCreate():
d6b. function).

*note .if.general.range;: d6c. Where a range is specified by two indexes
('base' and 'limit'), the index 'base', which specifies the beginning of
the range, must be in the interval [0, 'n'), and the index 'limit',
which specifies the end of the range, must be in the interval [1, 'n']
(note can be 'n'), and 'base' must be strictly less than 'limit' (empty
ranges are not allowed).  Sometimes 'i' and 'j' are used instead of
'base' and 'limit'.

 -- C Function: *note Res: 55f. BTCreate (BT *btReturn, Arena arena,
          Count n)

*note .if.create;: d6d. Attempts to create a table of length ‘n’ in the
arena control pool, putting the table in ‘*btReturn’.  Returns ‘ResOK’
if and only if the table is created OK. The initial values of the bits
in the table are undefined (so the client should probably call *note
BTResRange(): d6e. on the entire range before using the *note BT: 6b4.).
Meets *note .req.create: d33.

 -- C Function: void BTDestroy (BT t, Arena arena, Count n)

*note .if.destroy;: d70. Destroys the table ‘t’, which must have been
created with *note BTCreate(): d6b.  The value of argument ‘n’ must be
same as the value of the argument passed to *note BTCreate(): d6b.
Meets *note .req.destroy: d35.

 -- C Function: size_t BTSize (Count n)

*note .if.size;: d72. ‘BTSize(n)’ returns the number of bytes needed for
a Bit Table of ‘n’ bits.  *note BTSize(): d71. is a macro, but
‘(BTSize)(n)’ will assert if ‘n’ exceeds ‘COUNT_MAX - MPS_WORD_WIDTH +
1’.  This is used by clients that allocate storage for the *note BT:
6b4. themselves.  Before *note BTCreate(): d6b. and *note BTDestroy():
d6f. were implemented that was the only way to allocate a Bit Table, but
is now deprecated.

 -- C Function: int BTGet (BT t, Index i)

*note .if.get;: d74. ‘BTGet(t, i)’ returns the ‘i’-th bit of the table
‘t’ (that is, the image of ‘i’ under the mapping).  Meets *note
.req.ops.get: d38.

 -- C Function: void BTSet (BT t, Index i)

*note .if.set;: d76. ‘BTSet(t, i)’ sets the ‘i’-th bit of the table ‘t’
(to 1).  ‘BTGet(t, i)’ will now return 1.  Meets *note .req.ops.set:
d39.

 -- C Function: void BTRes (BT t, Index i)

*note .if.res;: d78. ‘BTRes(t, i)’ resets the ‘i’-th bit of the table
‘t’ (to 0).  ‘BTGet(t, i)’ will now return 0.  Meets *note
.req.ops.reset: d3a.

 -- C Function: void BTSetRange (BT t, Index base, Index limit)

*note .if.set-range;: d7a. ‘BTSetRange(t, base, limit)’ sets the range
of bits [‘base’, ‘limit’) in the table ‘t’.  ‘BTGet(t, x)’ will now
return 1 for ‘base’ ≤ ‘x’ < ‘limit’.  Meets *note
.req.ops.test.range.set: d40.

 -- C Function: void BTResRange (BT t, Index base, Index limit)

*note .if.res-range;: d7b. ‘BTResRange(t, base, limit)’ resets the range
of bits [‘base’, ‘limit’) in the table ‘t’.  ‘BTGet(t, x)’ will now
return 0 for ‘base’ ≤ ‘x’ < ‘limit’.  Meets *note
.req.ops.test.range.reset: d42.

 -- C Function: *note Bool: 3a9. BTIsSetRange (BT bt, Index base, Index
          limit)

*note .if.test.range.set;: d7d. Returns ‘TRUE’ if all the bits in the
range [‘base’, ‘limit’) are set, ‘FALSE’ otherwise.  Meets *note
.req.ops.test.range.set: d40.

 -- C Function: *note Bool: 3a9. BTIsResRange (BT bt, Index base, Index
          limit)

*note .if.test.range.reset;: d7e. Returns ‘TRUE’ if all the bits in the
range [‘base’, ‘limit’) are reset, ‘FALSE’ otherwise.  Meets *note
.req.ops.test.range.reset: d42.

 -- C Function: *note Bool: 3a9. BTRangesSame (BT BTx, BT BTy, Index
          base, Index limit)

*note .if.test.range.same;: d80. returns ‘TRUE’ if ‘BTGet(BTx,i)’ equals
‘BTGet(BTy,i)’ for ‘i’ in [‘base’, ‘limit’), and ‘FALSE’ otherwise.
Meets *note .non-req.ops.test.range.same: d5d.

*note .if.find.general;: d81. There are four functions (below) to find
reset ranges.  All the functions have the same prototype (for symmetry):

     Bool find(Index *baseReturn, Index *limitReturn,
               BT bt,
               Index searchBase, Index searchLimit,
               Count length);

where ‘bt’ is the Bit Table in which to search.  ‘searchBase’ and
‘searchLimit’ specify a subset of the Bit Table to use, the functions
will only find ranges that are subsets of [‘searchBase’, ‘searchLimit’)
(when set, ‘*baseReturn’ will never be less than ‘searchBase’ and
‘*limitReturn’ will never be greater than ‘searchLimit’).  ‘searchBase’
and ‘searchLimit’ specify a range that must conform to the general range
requirements for a range ['i', 'j'), as per *note .if.general.range:
d6c. modified appropriately.  ‘length’ is the number of contiguous reset
bits to find; it must not be bigger than ‘searchLimit - searchBase’
(that would be silly).  If a suitable range cannot be found the function
returns ‘FALSE’ (0) and leaves ‘*baseReturn’ and ‘*limitReturn’
untouched.  If a suitable range is found then the function returns the
range’s base in ‘*baseReturn’ and its limit in ‘*limitReturn’ and
returns ‘TRUE’ (1).

 -- C Function: *note Bool: 3a9. BTFindShortResRange (Index *baseReturn,
          Index *limitReturn, BT bt, Index searchBase, Index
          searchLimit, Count length)

*note .if.find-short-res-range;: d83. Finds a range of reset bits in the
table, starting at ‘searchBase’ and working upwards.  This function is
intended to meet *note .req.ops.find.short.low: d45. so it will find the
leftmost range that will do, and never finds a range longer than the
requested length (the intention is that it will not waste time looking).

 -- C Function: *note Bool: 3a9. BTFindShortResRangeHigh (Index
          *baseReturn, Index *limitReturn, BT bt, Index searchBase,
          Index searchLimit, Count length)

*note .if.find-short-res-range-high;: d85. Finds a range of reset bits
in the table, starting at ‘searchLimit’ and working downwards.  This
function is intended to meet *note .req.ops.find.short.high: d47. so it
will find the rightmost range that will do, and never finds a range
longer than the requested length.

 -- C Function: *note Bool: 3a9. BTFindLongResRange (Index *baseReturn,
          Index *limitReturn, BT bt, Index searchBase, Index
          searchLimit, Count length)

*note .if.find-long-res-range;: d87. Finds a range of reset bits in the
table, starting at ‘searchBase’ and working upwards.  This function is
intended to meet *note .req.ops.find.long.low: d49. so it will find the
leftmost range that will do and returns all of that range (which can be
longer than the requested length).

 -- C Function: *note Bool: 3a9. BTFindLongResRangeHigh (Index
          *baseReturn, Index *limitReturn, BT bt, Index searchBase,
          Index searchLimit, Count length)

*note .if.find-long-res-range-high;: d89. Finds a range of reset bits in
the table, starting at ‘searchLimit’ and working downwards.  This
function is intended to meet *note .req.ops.find.long.high: d4b. so it
will find the rightmost range that will do and returns all that range
(which can be longer than the requested length).

 -- C Function: void BTCopyRange (BT fromBT, BT toBT, Index base, Index
          limit)

*note .if.copy-range;: d8b. Overwrites the ‘i’-th bit of ‘toBT’ with the
‘i’-th bit of ‘fromBT’, for all ‘i’ in [‘base’, ‘limit’).  Meets *note
.req.ops.copy.simple: d4e.

 -- C Function: void BTCopyOffsetRange (BT fromBT, BT toBT, Index
          fromBase, Index fromLimit, Index toBase, Index toLimit)

*note .if.copy-offset-range;: d8d. Overwrites the ‘i’-th bit of ‘toBT’
with the ‘j’-th bit of ‘fromBT’, for all ‘i’ in [‘toBase’, ‘toLimit’)
and corresponding ‘j’ in [‘fromBase’, ‘fromLimit’).  Each of these
ranges must be the same size.  This might be significantly less
efficient than *note BTCopyRange(): d8a.  Meets *note
.req.ops.copy.offset: d50.

 -- C Function: void BTCopyInvertRange (BT fromBT, BT toBT, Index base,
          Index limit)

*note .if.copy-invert-range;: d8f. Overwrites the ‘i’-th bit of ‘toBT’
with the inverse of the ‘i’-th bit of ‘fromBT’, for all ‘i’ in [‘base’,
‘limit’).  Meets *note .req.ops.copy.invert: d52.


File: MemoryPoolSystem.info,  Node: Detailed design,  Next: Testing<6>,  Prev: Interface<21>,  Up: Bit tables

5.4.9 Detailed design
---------------------

* Menu:

* Data structures: Data structures<3>.
* Functions: Functions<7>.


File: MemoryPoolSystem.info,  Node: Data structures<3>,  Next: Functions<7>,  Up: Detailed design

5.4.9.1 Data structures
.......................

*note .datastructure;: d92. Bit Tables will be represented as (a pointer
to) an array of *note Word: 653.  A plain array is used instead of the
more usual design convention of implementing an abstract data type as a
structure with a signature (see guide.impl.c.adt(0)).  *note
.datastructure.words.justify;: d93. The type *note Word: 653. is used as
it will probably map to the object that can be most efficiently accessed
on any particular platform.  *note .datastructure.non-adt.justify;: d94.
The usual abstract data type convention was not followed because (i) The
initial design (drj) was lazy, (ii) Bit Tables are more likely to come
in convenient powers of two with the extra one or two words overhead.
However, the loss of checking is severe.  Perhaps it would be better to
use the usual abstract data type style.


File: MemoryPoolSystem.info,  Node: Functions<7>,  Prev: Data structures<3>,  Up: Detailed design

5.4.9.2 Functions
.................

*note .fun.size;: d96. *note BTSize(): d71.  Since a Bit Table is an
array of *note Word: 653, the size of a Bit Table of 'n' bits is simply
the number of words that it takes to store 'n' bits times the number of
bytes in a word.  This is ‘ceiling(n/MPS_WORD_WIDTH)*sizeof(Word).’
*note .fun.size.justify;: d97. Since there can be at most
‘MPS_WORD_WIDTH - 1’ unused bits in the entire table, this satisfies
*note .req.bit: d31.

*note .index;: d98. The designs for the following functions use a
decomposition of a bit-index, ‘i’, into two parts, ‘iw’, ‘ib’.

   * *note .index.word;: d99. ‘iw’ is the “word-index” which is the
     index into the word array of the word that contains the bit
     referred to by the bit-index.  ‘iw = i / MPS_WORD_WIDTH’.  Since
     *note MPS_WORD_WIDTH: 187. is a power of two, this is the same as
     ‘iw = i >> MPS_WORD_SHIFT’.  The latter expression is used in the
     code.  *note .index.word.justify;: d9a. The compiler is more likely
     to generate good code without the divide.

   * *note .index.sub-word;: d9b. ‘ib’ is the “sub-word-index” which is
     the index of the bit referred to by the bit-index in the above
     word.  ‘ib = i % MPS_WORD_WIDTH’.  Since *note MPS_WORD_WIDTH: 187.
     is a power of two, this is the same as ‘ib = i &
     ~((Word)-1<<MPS_WORD_SHIFT)’.  The latter expression is used in the
     code.  *note .index.sub-word.justify;: d9c. The compiler is more
     likely to generate good code without the modulus.

*note .index.justify.dubious;: d9d. The above justifications are
dubious; gcc 2.7.2 (with -O2) running on a sparc (zaphod) produces
identical code for the following two functions:

     unsigned long f(unsigned long i) {
         return i/32 + i%32;
     }

     unsigned long g(unsigned long i) {
        return (i>>5) + (i&31);
     }

 -- C Macro: ACT_ON_RANGE (base, limit, single_action, bits_action,
          word_action)

 -- C Macro: ACT_ON_RANGE_HIGH (base, limit, single_action, bits_action,
          word_action)

*note .iteration;: d65. Many of the following functions involve
iteration over ranges in a Bit Table.  This is performed on whole words
rather than individual bits, whenever possible (to improve speed).  This
is implemented internally by the macros *note ACT_ON_RANGE(): d9e. and
*note ACT_ON_RANGE_HIGH(): d9f. for iterating over the range forwards
and backwards respectively.  These macros do not form part of the
interface of the module, but are used extensively in the implementation.
The macros are often used even when speed is not an issue because it
simplifies the implementation and makes it more uniform.  The iteration
macros take the parameters ‘base’, ‘limit’, ‘single_action’,
‘bits_action’, and ‘word_action’:

   * ‘base’ and ‘limit’ are of type *note Index: b19. and define the
     range of the iteration.

   * ‘single_action’ is the name of a macro which will be used for
     iterating over bits in the table individually.  This macro must
     take a single *note Index: b19. parameter corresponding to the
     index for the bit.  The expansion of the macro must not contain
     ‘break’ or ‘continue’ because it will be called from within a loop
     from the expansion of *note ACT_ON_RANGE(): d9e.

   * ‘bits_action’ is the name of a macro which will be used for
     iterating over part-words.  This macro must take parameters
     ‘wordIndex’, ‘base’, ‘limit’ where ‘wordIndex’ is the index into
     the array of words, and ‘base’ and ‘limit’ define a range of bits
     within the indexed word.

   * ‘word_action’ is the name of a macro which will be used for
     iterating over whole-words.  This macro must take the single
     parameter ‘wordIndex’ which is the index of the whole-word in the
     array.  The expansion of the macro must not contain ‘break’ or
     ‘continue’ because it will be called from within a loop from the
     expansion of *note ACT_ON_RANGE(): d9e.

*note .iteration.exit;: da0. The expansion of the ‘single_action’,
‘bits_action’, and ‘word_action’ macros is allowed to contain ‘return’
or ‘goto’ to terminate the iteration early.  This is used by the test
(*note .fun.test.range.set: da1.) and find (*note .fun.find: da2.)
operations.

*note .iteration.small;: da3. If the range is sufficiently small only
the ‘single_action’ macro will be used, as this is more efficient in
practice.  The choice of what constitutes a small range is made entirely
on the basis of experimental performance results (and currently,
1999-04-27, a “small range” is 6 bits or fewer.  See
change.mps.epcore.brisling.160181 for some justification).  Otherwise
(for a bigger range) ‘bits_action’ is used on the part words at either
end of the range (or the whole of the range it if it fits in a single
word), and ‘word_action’ is used on the words that comprise the inner
portion of the range.

The implementation of *note ACT_ON_RANGE(): d9e. (and *note
ACT_ON_RANGE_HIGH(): d9f.) is simple enough.  It decides which macros it
should invoke and invokes them.  ‘single_action’ and ‘word_action’ are
invoked inside loops.

*note .fun.get;: da4. *note BTGet(): d73.  The bit-index will be
converted in the usual way, see *note .index: d98.  The relevant *note
Word: 653. will be read out of the Bit Table and shifted right by the
sub-*note Word: 653. index (this brings the relevant bit down to the
least significant bit of the *note Word: 653.), the *note Word: 653.
will then be masked with 1, producing the answer.

*note .fun.set;: da5. *note BTSet(): d75.

*note .fun.res;: da6. *note BTRes(): d77.

In both *note BTSet(): d75. and *note BTRes(): d77. a mask is
constructed by shifting 1 left by the sub-word-index (see *note .index:
d98.).  For *note BTSet(): d75. the mask is or-ed into the relevant word
(thereby setting a single bit).  For *note BTRes(): d77. the mask is
inverted and and-ed into the relevant word (thereby resetting a single
bit).

*note .fun.set-range;: da7. *note BTSetRange(): d79.  *note
ACT_ON_RANGE(): d9e. (see *note .iteration: d65. above) is used with
macros that set a single bit (using *note BTSet(): d75.), set a range of
bits in a word, and set a whole word.

*note .fun.res-range;: da8. *note BTResRange(): d6e. This is implemented
similarly to *note BTSetRange(): d79. (*note .fun.set-range: da7.)
except using *note BTRes(): d77. and reverse bit-masking logic.

*note .fun.test.range.set;: da1. *note BTIsSetRange(): d7c.  *note
ACT_ON_RANGE(): d9e. (see *note .iteration: d65. above) is used with
macros that test whether all the relevant bits are set; if some of the
relevant bits are not set then ‘return FALSE’ is used to terminate the
iteration early and return from the *note BTIsSetRange(): d7c. function.
If the iteration completes then ‘TRUE’ is returned.

*note .fun.test.range.reset;: da9. *note BTIsResRange(): 771.  As for
*note BTIsSetRange(): d7c. (*note .fun.test.range.set: da1. above) but
testing whether the bits are reset.

*note .fun.test.range.same;: daa. *note BTRangesSame(): d7f.  As for
*note BTIsSetRange(): d7c. (*note .fun.test.range.set: da1. above) but
testing whether corresponding ranges in the two Bit Tables are the same.
Note there are no speed requirements, but *note ACT_ON_RANGE(): d9e. is
used for simplicity and uniformity.

*note .fun.find;: da2. The four external find functions (*note
BTFindShortResRange(): d82, *note BTFindShortResRangeHigh(): d84, *note
BTFindLongResRange(): d86, *note BTFindLongResRangeHigh(): d88.) simply
call through to one of the two internal functions: *note
BTFindResRange(): dab. and *note BTFindResRangeHigh(): dac.

 -- C Function: *note Bool: 3a9. BTFindResRange (Index *baseReturn,
          Index *limitReturn, BT bt, Index searchBase, Index
          searchLimit, Count minLength, Count maxLength)

 -- C Function: *note Bool: 3a9. BTFindResRangeHigh (Index *baseReturn,
          Index *limitReturn, BT bt, Index searchBase, Index
          searchLimit, Count minLength, Count maxLength)

There are two length parameters, one specifying the minimum length of
the range to be found, the other the maximum length.  For
‘BTFindShort()’ and ‘BTFindShortHigh()’, ‘maxLength’ is equal to
‘minLength’ when passed; for ‘BTFindLong()’ and ‘BTFindLongHigh()’,
‘maxLength` is equal to the maximum possible range, namely ``searchLimit
- searchBase’.

*note .fun.find-res-range;: d66. *note BTFindResRange(): dab.  Iterate
within the search boundaries, identifying candidate ranges by searching
for a reset bit.  The Boyer–Moore algorithm *note [Boyer_Moore_1977]:
dad. is used (it’s particularly easy to implement when there are only
two symbols, 0 and 1, in the alphabet).  For each candidate range,
iterate backwards over the bits from the end of the range towards the
beginning.  If a set bit is found, this candidate has failed and a new
candidate range is selected.  If when scanning for the set bit a range
of reset bits was found before finding the set bit, then this (small)
range of reset bits is used as the start of the next candidate.
Additionally the end of this small range of reset bits (the end of the
failed candidate range) is remembered so that we don’t have to iterate
over this range again.  But if no reset bits were found in the candidate
range, then iterate again (starting from the end of the failed
candidate) to look for one.  If during the backwards search no set bit
is found, then we have found a sufficiently large range of reset bits;
now extend the valid range as far as possible up to the maximum length
by iterating forwards up to the maximum limit looking for a set bit.
The iterations make use of the *note ACT_ON_RANGE(): d9e. and *note
ACT_ON_RANGE_HIGH(): d9f. macros, which can use ‘goto’ to effect an
early termination of the iteration when a set/reset (as appropriate) bit
is found.  The macro ‘ACTION_FIND_SET_BIT()’ is used in the iterations.
It efficiently finds the first (that is, with lowest index or weight)
set bit in a word or subword.

*note .fun.find-res-range.improve;: dae. Various other performance
improvements have been suggested in the past, including some from
request.epcore.170534(1).  Here is a list of potential improvements
which all sound plausible, but which have not led to performance
improvements in practice:

   * *note .fun.find-res-range.improve.step.partial;: daf. When the top
     index in a candidate range fails, skip partial words as well as
     whole words, using, for example, lookup tables.

   * *note .fun.find-res-range.improve.lookup;: db0. When testing a
     candidate run, examine multiple bits at once (for example, 8),
     using lookup tables for (for example) index of first set bit, index
     of last set bit, number of reset bits, length of maximum run of
     reset bits.

*note .fun.find-res-range-high;: db1. *note BTFindResRangeHigh(): dac.
Exactly the same algorithm as in *note BTFindResRange(): dab. (see *note
.fun.find-res-range: d66. above), but moving over the table in the
opposite direction.

*note .fun.copy-simple-range;: db2. *note BTCopyRange(): d8a.  Uses
*note ACT_ON_RANGE(): d9e. (see *note .iteration: d65. above) with the
obvious implementation.  Should be fast.

*note .fun.copy-offset-range;: db3. *note BTCopyOffsetRange(): d8c.
Uses a simple iteration loop, reading bits with *note BTGet(): d73. and
setting them with *note BTSet(): d75.  Doesn’t use *note ACT_ON_RANGE():
d9e. because the two ranges will not, in general, be similarly
word-aligned.

*note .fun.copy-invert-range;: db4. *note BTCopyInvertRange(): d8e.
Uses *note ACT_ON_RANGE(): d9e. (see *note .iteration: d65. above) with
the obvious implementation.  Should be fast—although there are no speed
requirements.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/epcore/170534


File: MemoryPoolSystem.info,  Node: Testing<6>,  Next: References<21>,  Prev: Detailed design,  Up: Bit tables

5.4.10 Testing
--------------

*note .test;: db6. The following tests are available or have been used
during development.

*note .test.btcv;: db7. ‘btcv.c’.  This is supposed to be a coverage
test, intended to execute all of the module’s code in at least some
minimal way.

*note .test.landtest;: db8. ‘landtest.c’.  This is a test of the *note
Land: 53c. module (design.mps.land(1)) and its concrete implementations.
It compares the functional operation of a *note Land: 53c. with that of
a *note BT: 6b4. so is a good functional test of either module.

*note .test.mmqa.120;: db9. MMQA_test_function!210.c.  This is used
because it has a fair amount of segment allocation and freeing so
exercises the arena code that uses Bit Tables.

*note .test.bttest;: dba. ‘bttest.c’.  This is an interactive test that
can be used to exercise some of the *note BT: 6b4. functionality by
hand.

*note .test.dylan;: dbb. It is possible to modify Dylan so that it uses
Bit Tables more extensively.  See change.mps.epcore.brisling.160181
TEST1 and TEST2.

   ---------- Footnotes ----------

   (1) land.html


File: MemoryPoolSystem.info,  Node: References<21>,  Prev: Testing<6>,  Up: Bit tables

5.4.11 References
-----------------

(Boyer_Moore_1977) Robert S. Boyer and J. Strother Moore.
Communications of the ACM 20(10):762–772.  1977.  “A Fast String
Searching Algorithm(1)”.

   ---------- Footnotes ----------

   (1) http://www.cs.utexas.edu/~moore/publications/fstrpos.pdf


File: MemoryPoolSystem.info,  Node: Allocation buffers and allocation points,  Next: Checking<4>,  Prev: Bit tables,  Up: Old design

5.5 Allocation buffers and allocation points
============================================

* Menu:

* Introduction: Introduction<51>.
* Glossary::
* Source: Source<2>.
* Requirements: Requirements<32>.
* Classes: Classes<3>.
* Logging::
* Measurement::
* Notes from the whiteboard::
* Synchronization: Synchronization<2>.
* Interface: Interface<22>.
* Diagrams::


File: MemoryPoolSystem.info,  Node: Introduction<51>,  Next: Glossary,  Up: Allocation buffers and allocation points

5.5.1 Introduction
------------------

*note .scope;: dc2. This is the design of allocation buffers and
allocation points.

*note .purpose;: dc3. The purpose of this document is to record design
decisions made concerning allocation buffers and allocation points and
justify those decisions in terms of requirements.

*note .readership;: dc4. The document is intended for reading by any MPS
developer.


File: MemoryPoolSystem.info,  Node: Glossary,  Next: Source<2>,  Prev: Introduction<51>,  Up: Allocation buffers and allocation points

5.5.2 Glossary
--------------

trapped

     *note .def.trapped;: dc6. The buffer is in a state such that the
     MPS gets to know about the next use of that buffer.


File: MemoryPoolSystem.info,  Node: Source<2>,  Next: Requirements<32>,  Prev: Glossary,  Up: Allocation buffers and allocation points

5.5.3 Source
------------

*note .source.mail;: dc8. Much of the juicy stuff about buffers is only
floating around in mail discussions.  You might like to try searching
the archives if you can’t find what you want here.

     Note: Mail archives are only accessible to Ravenbrook staff.  RHSK
     2006-06-09.

*note .source.synchronize;: dc9. For a discussion of the synchronization
issues, see mail.richard.1995-05-19.17-10(1),
mail.ptw.1995-05-19.19-15(2), and mail.richard.1995-05-24.10-18(3).

     Note: I believe that the sequence for flip in PTW’s message is
     incorrect.  The operations should be in the other order.  DRJ.

*note .source.interface;: dca. For a description of the buffer interface
in C prototypes, see mail.richard.1997-04-28.09-25(4).

*note .source.qa;: dcb. Discussions with QA were useful in pinning down
the semantics and understanding of some obscure but important boundary
cases.  See the thread with subject “notes on our allocation points
discussion” and messages mail.richard.tucker.1997-05-12.09-45(5),
mail.ptw.1997-05-12.12-46(6), mail.richard.1997-05-12.13-15(7),
mail.richard.1997-05-12.13-28(8), mail.ptw.1997-05-13.15-15(9),
mail.sheep.1997-05-14.11-52(10), mail.rit.1997-05-15.09-19(11),
mail.ptw.1997-05-15.21-22(12), mail.ptw.1997-05-15.21-35(13),
mail.rit.1997-05-16.08-02(14), mail.rit.1997-05-16.08-42(15),
mail.ptw.1997-05-16.12-36(16), mail.ptw.1997-05-16.12-47(17),
mail.richard.1997-05-19.15-46(18), mail.richard.1997-05-19.15-56(19),
and mail.ptw.1997-05-20.20-47(20).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1995/05/19/17-10/0.txt

   (2) 
https://info.ravenbrook.com/project/mps/mail/1995/05/19/19-15/0.txt

   (3) 
https://info.ravenbrook.com/project/mps/mail/1995/05/24/10-18/0.txt

   (4) 
https://info.ravenbrook.com/project/mps/mail/1997/04/28/09-25/0.txt

   (5) 
https://info.ravenbrook.com/project/mps/mail/1997/05/12/09-45/0.txt

   (6) 
https://info.ravenbrook.com/project/mps/mail/1997/05/12/12-46/1.txt

   (7) 
https://info.ravenbrook.com/project/mps/mail/1997/05/12/13-15/0.txt

   (8) 
https://info.ravenbrook.com/project/mps/mail/1997/05/12/13-28/0.txt

   (9) 
https://info.ravenbrook.com/project/mps/mail/1997/05/13/15-15/0.txt

   (10) 
https://info.ravenbrook.com/project/mps/mail/1997/05/14/11-52/0.txt

   (11) 
https://info.ravenbrook.com/project/mps/mail/1997/05/15/09-19/0.txt

   (12) 
https://info.ravenbrook.com/project/mps/mail/1997/05/15/21-22/0.txt

   (13) 
https://info.ravenbrook.com/project/mps/mail/1997/05/15/21-35/0.txt

   (14) 
https://info.ravenbrook.com/project/mps/mail/1997/05/16/08-02/0.txt

   (15) 
https://info.ravenbrook.com/project/mps/mail/1997/05/16/08-42/0.txt

   (16) 
https://info.ravenbrook.com/project/mps/mail/1997/05/16/12-36/0.txt

   (17) 
https://info.ravenbrook.com/project/mps/mail/1997/05/16/12-47/0.txt

   (18) 
https://info.ravenbrook.com/project/mps/mail/1997/05/19/15-46/0.txt

   (19) 
https://info.ravenbrook.com/project/mps/mail/1997/05/19/15-56/0.txt

   (20) 
https://info.ravenbrook.com/project/mps/mail/1997/05/20/20-47/0.txt


File: MemoryPoolSystem.info,  Node: Requirements<32>,  Next: Classes<3>,  Prev: Source<2>,  Up: Allocation buffers and allocation points

5.5.4 Requirements
------------------

*note .req.fast;: dce. Allocation must be very fast.

*note .req.thread-safe;: dcf. Must run safely in a multi-threaded
environment.

*note .req.no-synch;: dd0. Must avoid the use of thread-synchronization.
(*note .req.fast: dce.)

*note .req.manual;: dd1. Support manual memory management.

*note .req.exact;: dd2. Support exact collectors.

*note .req.ambig;: dd3. Support ambiguous collectors.

*note .req.count;: dd4. Must record (approximately) the amount of
allocation (in bytes).

     Note: Actually not a requirement any more, but once was put forward
     as a Dylan requirement.  Bits of the code still reflect this
     requirement.  See request.dylan.170554(1).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170554


File: MemoryPoolSystem.info,  Node: Classes<3>,  Next: Logging,  Prev: Requirements<32>,  Up: Allocation buffers and allocation points

5.5.5 Classes
-------------

*note .class.hierarchy;: dd7. The ‘Buffer’ data structure is designed to
be subclassable (see design.mps.protocol(1)).

*note .class.hierarchy.buffer;: dd8. The basic buffer class
(‘BufferClass’) supports basic allocation-point buffering, and is
appropriate for those manual pools which don’t use segments (*note
.req.manual: dd1.).  The ‘Buffer’ class doesn’t support reference ranks
(that is, the buffers have ‘RankSetEMPTY’).  Clients may use
‘BufferClass’ directly, or create their own subclasses (see *note
.subclassing: dd9.).

*note .class.hierarchy.segbuf;: dda. Class ‘SegBufClass’ is also
provided for the use of pools which additionally need to associate
buffers with segments.  ‘SegBufClass’ is a subclass of ‘BufferClass’.
Manual pools may find it convenient to use ‘SegBufClass’, but it is
primarily intended for automatic pools (*note .req.exact: dd2, *note
.req.ambig: dd3.).  An instance of ‘SegBufClass’ may be attached to a
region of memory that lies within a single segment.  The segment is
associated with the buffer, and may be accessed with the ‘BufferSeg()’
function.  ‘SegBufClass’ also supports references at any rank set.
Hence this class or one of its subclasses should be used by all
automatic pools (with the possible exception of leaf pools).  The rank
sets of buffers and the segments they are attached to must match.
Clients may use ‘SegBufClass’ directly, or create their own subclasses
(see *note .subclassing: dd9.).

*note .class.hierarchy.rankbuf;: ddb. Class ‘RankBufClass’ is also
provided as a subclass of ‘SegBufClass’.  The only way in which this
differs from its superclass is that the rankset of a ‘RankBufClass’ is
set during initialization to the singleton rank passed as an additional
parameter to *note BufferCreate(): ddc.  Instances of ‘RankBufClass’ are
of the same type as instances of ‘SegBufClass’, that is, ‘SegBuf’.
Clients may use ‘RankBufClass’ directly, or create their own subclasses
(see *note .subclassing: dd9.).

*note .class.create;: ddd. The buffer creation functions (*note
BufferCreate(): ddc. and ‘BufferCreateV()’) take a ‘class’ parameter,
which determines the class of buffer to be created.

*note .class.choice;: dde. Pools which support buffered allocation
should specify a default class for buffers.  This class will be used
when a buffer is created in the normal fashion by MPS clients (for
example by a call to *note mps_ap_create(): 339.).  Pools specify the
default class by means of the ‘bufferClass’ field in the pool class
object.  This should be a pointer to a function of type *note
PoolBufferClassMethod: 79e.  The normal class “Ensure” function (for
example ‘EnsureBufferClass()’) has the appropriate type.

*note .subclassing;: dd9. Pools may create their own subclasses of the
standard buffer classes.  This is sometimes useful if the pool needs to
add an extra field to the buffer.  The convenience macro
‘DEFINE_BUFFER_CLASS()’ may be used to define subclasses of buffer
classes.  See design.mps.protocol.int.define-special(2).

*note .replay;: ddf. To work with the allocation replayer (see
design.mps.telemetry.replayer(3)), the buffer class has to emit an event
for each call to an external interface, containing all the parameters
passed by the user.  If a new event type is required to carry this
information, the replayer (impl.c.eventrep) must then be extended to
recreate the call.

*note .replay.pool-buffer;: de0. The replayer must also be updated if
the association of buffer class to pool or the buffer class hierarchy is
changed.

*note .class.method;: de1. Buffer classes provide the following methods
(these should not be confused with the pool class methods related to the
buffer protocol, described in *note .method.create: de2. and following
sections):

 -- C Type: typedef *note Res: 55f. (*BufferInitMethod)(Buffer buffer,
          Pool pool, ArgList args)

*note .class.method.init;: de4. ‘init()’ is a class-specific
initialization method called from ‘BufferInit()’.  It receives the
keyword arguments passed to to ‘BufferInit()’.  Client-defined methods
must call their superclass method (via a next-method call) before
performing any class-specific behaviour.  *note .replay.init;: de5. The
‘init()’ method should emit a ‘BufferInit<foo>’ event (if there aren’t
any extra parameters, ‘<foo> = ""’).

 -- C Type: typedef void (*BufferAttachMethod)(Buffer buffer, *note
          Addr: 632. base, *note Addr: 632. limit, *note Addr: 632.
          init, *note Size: 40e. size)

*note .class.method.attach;: de7. ‘attach()’ is a class-specific method
called whenever a buffer is attached to memory, via *note
BufferAttach(): de8.  Client-defined methods must call their superclass
method (via a next-method call) before performing any class-specific
behaviour.

 -- C Type: typedef void (*BufferDetachMethod)(Buffer buffer)

*note .class.method.detach;: dea. ‘detach()’ is a class-specific method
called whenever a buffer is detached from memory, via *note
BufferDetach(): 7a5.  Client-defined methods must call their superclass
method (via a next-method call) after performing any class-specific
behaviour.

 -- C Type: typedef *note Seg: b53. (*BufferSegMethod)(Buffer buffer)

*note .class.method.seg;: dec. ‘seg()’ is a class-specific accessor
method which returns the segment attached to a buffer (or ‘NULL’ if
there isn’t one).  It is called from ‘BufferSeg()’.  Clients should not
need to define their own methods for this.

 -- C Type: typedef *note RankSet: b21. (*BufferRankSetMethod)(Buffer
          buffer)

*note .class.method.rankSet;: dee. ‘rankSet()’ is a class-specific
accessor method which returns the rank set of a buffer.  It is called
from ‘BufferRankSet()’.  Clients should not need to define their own
methods for this.

 -- C Type: typedef void (*BufferSetRankSetMethod)(Buffer buffer, *note
          RankSet: b21. rankSet)

*note .class.method.setRankSet;: df0. ‘setRankSet()’ is a class-specific
setter method which sets the rank set of a buffer.  It is called from
‘BufferSetRankSet()’.  Clients should not need to define their own
methods for this.

   ---------- Footnotes ----------

   (1) protocol.html

   (2) protocol.html#design.mps.protocol.int.define-special

   (3) telemetry.html#design.mps.telemetry.replayer


File: MemoryPoolSystem.info,  Node: Logging,  Next: Measurement,  Prev: Classes<3>,  Up: Allocation buffers and allocation points

5.5.6 Logging
-------------

*note .logging.control;: df2. Buffers have a separate control for
whether they are logged or not, this is because they are particularly
high volume.  This is a Boolean flag (‘bufferLogging’) in the
‘ArenaStruct’.


File: MemoryPoolSystem.info,  Node: Measurement,  Next: Notes from the whiteboard,  Prev: Logging,  Up: Allocation buffers and allocation points

5.5.7 Measurement
-----------------

*note .count;: df4. Counting the allocation volume is done by
maintaining two fields in the buffer struct:

*note .count.fields;: df5. ‘fillSize’, ‘emptySize’.

*note .count.monotonic;: df6. both of these fields are monotonically
increasing.

*note .count.fillsize;: df7. ‘fillSize’ is an accumulated total of the
size of all the fills (as a result of calling the ‘PoolClass’ *note
BufferFill(): 7a2. method) that happen on the buffer.

*note .count.emptysize;: df8. ‘emptySize’ is an accumulated total of the
size of all the empties than happen on the buffer (which are notified to
the pool using the ‘PoolClass’ ‘BufferEmpty()’ method).

*note .count.generic;: df9. These fields are maintained by the generic
buffer code in *note BufferAttach(): de8. and *note BufferDetach(): 7a5.

*note .count.other;: dfa. Similar count fields are maintained in the
arena.  They are maintained on an internal (buffers used internally by
the MPS) and external (buffers used for mutator allocation points)
basis.  The fields are also updated by the buffer code.  The fields are:

   - in the arena, ‘fillMutatorSize’, ‘fillInternalSize’,
     ‘emptyMutatorSize’, ‘emptyInternalSize’, and ‘allocMutatorSize’ (5
     fields).

*note .count.alloc.how;: dfb. The amount of allocation in the buffer
just after an empty is ‘fillSize - emptySize’.  At other times this
computation will include space that the buffer has the use of (between
base and init) but which may not get allocated in (because the remaining
space may be too large for the next reserve so some or all of it may get
emptied).  The arena field ‘allocMutatorSize’ is incremented by the
allocated size (between base and init) whenever a buffer is detached.
Symmetrically this field is decremented by by the pre-allocated size
(between base and init) whenever a buffer is attached.  The overall
count is asymptotically correct.

*note .count.type;: dfc. All the count fields are type double.

*note .count.type.justify;: dfd. This is because double is the type most
likely to give us enough precision.  Because of the lack of genuine
requirements the type isn’t so important.  It’s nice to have it more
precise than long.  Which double usually is.


File: MemoryPoolSystem.info,  Node: Notes from the whiteboard,  Next: Synchronization<2>,  Prev: Measurement,  Up: Allocation buffers and allocation points

5.5.8 Notes from the whiteboard
-------------------------------

Requirements

   - atomic update of words

   - guarantee order of reads and write to certain memory locations.

Flip

   - limit:=0

   - record init for scanner

Commit

   - init:=alloc

   - if(limit = 0) …

   - L written only by MM

   - A written only by client (except during synchronized MM op)

   - I ditto

   - I read by MM during flip

States

   - busy

   - ready

   - trapped

   - reset

     Note: There are many more states.  DRJ.

Misc

   - During buffer ops all field values can change.  Might trash
     perfectly good (“valid”?)  object if pool isn’t careful.


File: MemoryPoolSystem.info,  Node: Synchronization<2>,  Next: Interface<22>,  Prev: Notes from the whiteboard,  Up: Allocation buffers and allocation points

5.5.9 Synchronization
---------------------

Buffers provide a loose form of synchronization between the mutator and
the collector.

The crucial synchronization issues are between the operation the pool
performs on flip and the mutator’s commit operation.

Commit

   - read init

   - write init

   - Memory Barrier

   - read ‘limit’

Flip

   - write ‘limit’

   - Memory Barrier

   - read init

Commit consists of two parts.  The first is the update to init.  This is
a declaration that the new object just before init is now correctly
formatted and can be scanned.  The second is a check to see if the
buffer has been “tripped”.  The ordering of the two parts is crucial.

Note that the declaration that the object is correctly formatted is
independent of whether the buffer has been tripped or not.  In
particular a pool can scan up to the init pointer (including the newly
declared object) whether or not the pool will cause the commit to fail.
In the case where the pool scans the object, but then causes the commit
to fail (and presumably the allocation to occur somewhere else), the
pool will have scanned a “dead” object, but this is just another example
of conservatism in the general sense.

Not that the read of init in the Flip sequence can in fact be
arbitrarily delayed (as long as it is read before a buffered segment is
scanned).

On processors with Relaxed Memory Order (such as the DEC Alpha), Memory
Barriers will need to be placed at the points indicated.

     * DESIGN
     *
     * An allocation buffer is an interface to a pool which provides
     * very fast allocation, and defers the need for synchronization in
     * a multi-threaded environment.
     *
     * Pools which contain formatted objects must be synchronized so
     * that the pool can know when an object is valid.  Allocation from
     * such pools is done in two stages: reserve and commit.  The client
     * first reserves memory, then initializes it, then commits.
     * Committing the memory declares that it contains a valid formatted
     * object.  Under certain conditions, some pools may cause the
     * commit operation to fail.  (See the documentation for the pool.)
     * Failure to commit indicates that the whole allocation failed and
     * must be restarted.  When using a pool which introduces the
     * possibility of commit failing, the allocation sequence could look
     * something like this:
     *
     * do {
     *   res = BufferReserve(&p, buffer, size);
     *   if(res != ResOK) return res;       // allocation fails, reason res
     *   initialize(p);                     // p now points at valid object
     * } while(!BufferCommit(buffer, p, size));
     *
     * Pools which do not contain formatted objects can use a one-step
     * allocation as usual.  Effectively any random rubbish counts as a
     * "valid object" to such pools.
     *
     * An allocation buffer is an area of memory which is pre-allocated
     * from a pool, plus a buffer descriptor, which contains, inter
     * alia, four pointers: base, init, alloc, and limit.  Base points
     * to the base address of the area, limit to the last address plus
     * one.  Init points to the first uninitialized address in the
     * buffer, and alloc points to the first unallocated address.
     *
     *    L . - - - - - .         ^
     *      |           |     Higher addresses -'
     *      |   junk    |
     *      |           |       the "busy" state, after Reserve
     *    A |-----------|
     *      |  uninit   |
     *    I |-----------|
     *      |   init    |
     *      |           |     Lower addresses  -.
     *    B `-----------'         v
     *
     *    L . - - - - - .         ^
     *      |           |     Higher addresses -'
     *      |   junk    |
     *      |           |       the "ready" state, after Commit
     *  A=I |-----------|
     *      |           |
     *      |           |
     *      |   init    |
     *      |           |     Lower addresses  -.
     *    B `-----------'         v
     *
     * Access to these pointers is restricted in order to allow
     * synchronization between the pool and the client.  The client may
     * only write to init and alloc, but in a restricted and atomic way
     * detailed below.  The pool may read the contents of the buffer
     * descriptor at _any_ time.  During calls to the fill and trip
     * methods, the pool may update any or all of the fields
     * in the buffer descriptor.  The pool may update the limit at _any_
     * time.
     *
     * Access to buffers by these methods is not synchronized.  If a buffer
     * is to be used by more than one thread then it is the client's
     * responsibility to ensure exclusive access.  It is recommended that
     * a buffer be used by only a single thread.
     *
     * [Only one thread may use a buffer at once, unless the client
     * places a mutual exclusion around the buffer access in the usual
     * way.  In such cases it is usually better to create one buffer for
     * each thread.]
     *
     * Here are pseudo-code descriptions of the reserve and commit
     * operations.  These may be implemented in-line by the client.
     * Note that the client is responsible for ensuring that the size
     * (and therefore the alloc and init pointers) are aligned according
     * to the buffer's alignment.
     *
     * Reserve(buf, size)                   ; size must be aligned to pool
     *   if buf->limit - buf->alloc >= size then
     *     buf->alloc +=size                ; must be atomic update
     *     p = buf->init
     *   else
     *     res = BufferFill(&p, buf, size)  ; buf contents may change
     *
     * Commit(buf, p, size)
     *   buf->init = buf->alloc             ; must be atomic update
     *   if buf->limit == 0 then
     *     res = BufferTrip(buf, p, size)   ; buf contents may change
     *   else
     *     res = True
     * (returns True on successful commit)
     *
     * The pool must allocate the buffer descriptor and initialize it by
     * calling BufferInit.  The descriptor this creates will fall
     * through to the fill method on the first allocation.  In general,
     * pools should not assign resources to the buffer until the first
     * allocation, since the buffer may never be used.
     *
     * The pool may update the base, init, alloc, and limit fields when
     * the fallback methods are called.  In addition, the pool may set
     * the limit to zero at any time.  The effect of this is either:
     *
     *   1. cause the _next_ allocation in the buffer to fall through to
     *      the buffer fill method, and allow the buffer to be flushed
     *      and relocated;
     *
     *   2. cause the buffer trip method to be called if the client was
     *      between reserve and commit.
     *
     * A buffer may not be relocated under other circumstances because
     * there is a race between updating the descriptor and the client
     * allocation sequence.


File: MemoryPoolSystem.info,  Node: Interface<22>,  Next: Diagrams,  Prev: Synchronization<2>,  Up: Allocation buffers and allocation points

5.5.10 Interface
----------------

 -- C Function: *note Res: 55f. BufferCreate (Buffer *bufferReturn,
          BufferClass class, Pool pool, Bool isMutator, ArgList args)

*note .method.create;: de2. Create an allocation buffer in a pool.  The
buffer is created in the “ready” state.

A buffer structure is allocated from the space control pool and
partially initialized (in particularly neither the signature nor the
serial field are initialized).  The pool class’s ‘bufferCreate()’ method
is then called.  This method can update (some undefined subset of) the
fields of the structure; it should return with the buffer in the “ready”
state (or fail).  The remainder of the initialization then occurs.

If and only if successful then a valid buffer is returned.

 -- C Function: void BufferDestroy (Buffer buffer)

*note .method.destroy;: e02. Free a buffer descriptor.  The buffer must
be in the “ready” state, that is, not between a Reserve and Commit.
Allocation in the area of memory to which the descriptor refers must
cease after *note BufferDestroy(): e01. is called.

Destroying an allocation buffer does not affect objects which have been
allocated, it just frees resources associated with the buffer itself.

The pool class’s ‘bufferDestroy()’ method is called and then the buffer
structure is uninitialized and freed.

 -- C Function: *note Bool: 3a9. BufferCheck (Buffer buffer)

*note .method.check;: e04. The check method is straightforward, the
non-trivial dependencies checked are:

   - The ordering constraints between base, init, alloc, and limit.

   - The alignment constraints on base, init, alloc, and limit.

   - That the buffer’s rank is identical to the segment’s rank.

 -- C Function: void BufferAttach (Buffer buffer, Addr base, Addr limit,
          Addr init, Size size)

*note .method.attach;: e05. Set the base, init, alloc, and limit fields
so that the buffer is ready to start allocating in area of memory.  The
alloc field is set to ‘init + size’.

*note .method.attach.unbusy;: e06. *note BufferAttach(): de8. must only
be applied to buffers that are not busy.

 -- C Function: void BufferDetach (Buffer buffer, Pool pool)

*note .method.detach;: e07. Set the seg, base, init, alloc, and limit
fields to zero, so that the next reserve request will call the fill
method.

*note .method.detach.unbusy;: e08. *note BufferDetach(): 7a5. must only
be applied to buffers that are not busy.

 -- C Function: *note Bool: 3a9. BufferIsReset (Buffer buffer)

*note .method.isreset;: e0a. Returns ‘TRUE’ if and only if the buffer is
in the reset state, that is, with base, init, alloc, and limit all set
to zero.

 -- C Function: *note Bool: 3a9. BufferIsReady (Buffer buffer)

*note .method.isready;: e0c. Returns ‘TRUE’ if and only if the buffer is
not between a reserve and commit.  The result is only reliable if the
client is not currently using the buffer, since it may update the alloc
and init pointers asynchronously.

 -- C Function: *note mps_ap_t: 1c0. BufferAP (Buffer buffer)

Returns the ‘APStruct’ substructure of a buffer.

 -- C Function: Buffer BufferOfAP (mps_ap_t ap)

*note .method.ofap;: e0f. Return the buffer which owns an ‘APStruct’.

*note .method.ofap.thread-safe;: e10. *note BufferOfAP(): e0e. must be
thread safe (see impl.c.mpsi.thread-safety).  This is achieved simply
because the underlying operation involved is simply a subtraction.

 -- C Function: *note Arena: 796. BufferArena (Buffer buffer)

*note .method.arena;: e12. Returns the arena which owns a buffer.

*note .method.arena.thread-safe;: e13. *note BufferArena(): e11. must be
thread safe (see impl.c.mpsi.thread-safety).  This is achieved simple
because the underlying operation is a read of shared-non-mutable data
(see design.mps.thread-safety(1)).

 -- C Function: Pool BufferPool (Buffer buffer)

Returns the pool to which a buffer is attached.

 -- C Function: *note Res: 55f. BufferReserve (Addr *pReturn, Buffer
          buffer, Size size)

*note .method.reserve;: e16. Reserves memory from an allocation buffer.

This is a provided version of the reserve procedure described above.
The size must be aligned according to the buffer alignment.  If
successful, ‘ResOK’ is returned and ‘*pReturn’ is updated with a pointer
to the reserved memory.  Otherwise ‘*pReturn’ is not touched.  The
reserved memory is not guaranteed to have any particular contents.  The
memory must be initialized with a valid object (according to the pool to
which the buffer belongs) and then passed to the *note BufferCommit():
e17. method (see below).  ‘BufferReserve(0’ may not be applied twice to
a buffer without a *note BufferCommit(): e17. in-between.  In other
words, Reserve/Commit pairs do not nest.

 -- C Function: *note Res: 55f. BufferFill (Addr *pReturn, Buffer
          buffer, Size size)

*note .method.fill;: e18. Refills an empty buffer.  If there is not
enough space in a buffer to allocate in-line, *note BufferFill(): 7a2.
must be called to “refill” the buffer.

 -- C Function: *note Bool: 3a9. BufferCommit (Buffer buffer, Addr p,
          Size size)

*note .method.commit;: e19. Commit memory previously reserved.

*note BufferCommit(): e17. notifies the pool that memory which has been
previously reserved (see above) has been initialized with a valid object
(according to the pool to which the buffer belongs).  The pointer ‘p’
must be the same as that returned by *note BufferReserve(): e15, and the
size must match the size passed to *note BufferReserve(): e15.

*note BufferCommit(): e17. may not be applied twice to a buffer without
a reserve in between.  In other words, objects must be reserved,
initialized, then committed only once.

Commit returns ‘TRUE’ if successful, ‘FALSE’ otherwise.  If commit fails
and returns ‘FALSE’, the client may try to allocate again by going back
to the reserve stage, and may not use the memory at ‘p’ again for any
purpose.

Some classes of pool may cause commit to fail under rare circumstances.

 -- C Function: void BufferTrip (Buffer buffer, Addr p, Size size)

*note .method.trip;: e1b. Act on a tripped buffer.  The pool which owns
a buffer may asynchronously set the buffer limit to zero in order to get
control over the buffer.  If this occurs after a *note BufferReserve():
e15. (but before the corresponding commit), then the *note
BufferCommit(): e17. method calls *note BufferTrip(): e1a. and the *note
BufferCommit(): e17. method returns with the return value of *note
BufferTrip(): e1a.

*note .method.trip.precondition;: e1c. At the time trip is called, from
*note BufferCommit(): e17, the following are true:

   - *note .method.trip.precondition.limit;: e1d. ‘limit == 0’

   - *note .method.trip.precondition.init;: e1e. ‘init == alloc’

   - *note .method.trip.precondition.p;: e1f. ‘p + size == alloc’

   ---------- Footnotes ----------

   (1) thread-safety.html


File: MemoryPoolSystem.info,  Node: Diagrams,  Prev: Interface<22>,  Up: Allocation buffers and allocation points

5.5.11 Diagrams
---------------

Here are a number of diagrams showing how buffers behave.  In general,
the horizontal axis corresponds to mutator action (reserve, commit) and
the vertical axis corresponds to collector action.  I’m not sure which
of the diagrams are the same as each other, and which are best or most
complete when they are different, but they all attempt to show
essentially the same information.  It’s very difficult to get all the
details in.  These diagrams were drawn by Richard Brooksby, Richard
Tucker, Gavin Matthews, and others in April 1997.  In general, the later
diagrams are, I suspect, more correct, complete and useful than the
earlier ones.  I have put them all here for the record.  Richard Tucker,
1998-02-09.

Buffer Diagram: Buffer States

Buffer States (3-column) Buffer States (4-column) Buffer States
(gavinised) Buffer States (interleaved) Buffer States (richardized)

[missing diagrams]


File: MemoryPoolSystem.info,  Node: Checking<4>,  Next: Collection framework,  Prev: Allocation buffers and allocation points,  Up: Old design

5.6 Checking
============

* Menu:

* Introduction: Introduction<52>.
* Implementation: Implementation<17>.
* Common assertions::


File: MemoryPoolSystem.info,  Node: Introduction<52>,  Next: Implementation<17>,  Up: Checking<4>

5.6.1 Introduction
------------------

*note .intro;: e26. This documents the design of structure checking
within the MPS.

*note .readership;: e27. MPS developers.


File: MemoryPoolSystem.info,  Node: Implementation<17>,  Next: Common assertions,  Prev: Introduction<52>,  Up: Checking<4>

5.6.2 Implementation
--------------------

*note .level;: e29. There are three levels of checking:

  1. *note .level.sig;: e2a. The lowest level checks only that the
     structure has a valid ‘Signature’ (see design.mps.sig(1)).

  2. *note .level.shallow;: e2b. Shallow checking checks all local
     fields (including signature) and also checks the signatures of any
     parent or child structures.

  3. *note .level.deep;: e2c. Deep checking checks all local fields
     (including signatures), the signatures of any parent structures,
     and does full recursive checking on any child structures.

*note .level.control;: e2d. Control over the levels of checking is via
the definition of at most one of the macros ‘TARGET_CHECK_SHALLOW’
(which if defined gives *note .level.shallow: e2b.), ‘TARGET_CHECK_DEEP’
(which if defined gives *note .level.deep: e2c.).  If neither macro is
defined then *note .level.sig: e2a. is used.  These macros are not
intended to be manipulated directly by developers, they should use the
interface in impl.h.target.

*note .order;: e2e. Because deep checking (*note .level.deep: e2c.) uses
unchecked recursion, it is important that child relationships are
acyclic (*note .macro.down: e2f.).

*note .fun;: e30. Every abstract data type which is a structure pointer
should have a function ‘<type>Check’ which takes a pointer of type
‘<type>’ and returns a *note Bool: 3a9.  It should check all fields in
order, using one of the macros in *note .macro: e31, or document why
not.

*note .fun.omit;: e32. The only fields which should be omitted from a
check function are those for which there is no meaningful check (for
example, an unlimited unsigned integer with no relation to other
fields).

*note .fun.return;: e33. Although the function returns a *note Bool:
3a9, if the assert handler returns (or there is no assert handler), then
this is taken to mean “ignore and continue”, and the check function
hence returns ‘TRUE’.

*note .macro;: e31. Checking is implemented by invoking four macros in
impl.h.assert:

 -- C Macro: CHECKS (type, val)

*note .macro.sig;: e34. ‘CHECKS(type, val)’ checks the signature only,
and should be called precisely on ‘type’ and the received object
pointer.

 -- C Macro: CHECKL (cond)

*note .macro.local;: e36. ‘CHECKL(cond)’ checks a local field (depending
on level; see *note .level: e29.), and should be called on each local
field that is not an abstract data type structure pointer itself (apart
from the signature), with an appropriate normally-true test condition.

 -- C Macro: CHECKU (type, val)

*note .macro.up;: e38. ‘CHECKU(type, val)’ checks a parent abstract data
type structure pointer, performing at most signature checks (depending
on level; see *note .level: e29.).  It should be called with the parent
type and pointer.

 -- C Macro: CHECKD (type, val)

*note .macro.down;: e2f. ‘CHECKD(type, val)’ checks a child abstract
data type structure pointer, possibly invoking ‘<type>Check’ (depending
on level; see *note .level: e29.).  It should be called with the child
type and pointer.

*note .full-type;: e3a. Use *note CHECKS(): 8ee, *note CHECKD(): e39. or
*note CHECKU(): e37. on all types that satisfy these three requirements:

*note .full-type.pointer;: e3b. The type is a pointer type.

*note .full-type.check;: e3c. The type provides a function ‘Bool
TypeCheck(Type type)’ where ‘Type’ is substituted for the name of the
type (for example, ‘PoolCheck()’).

*note .full-type.sig;: e3d. The expression ‘obj->sig’ is a valid value
of type *note Sig: 8dc. whenever ‘obj’ is a valid value of type ‘Type’.

*note .partial-type;: e3e. Where the type satisfies *note
.full-type.pointer: e3b. and *note .full-type.check: e3c. but not *note
.full-type.sig: e3d. because the type lacks a signature in order to save
space (this applies to small structures that are embedded many times in
other structures, for example *note Ring: 85c.), use ‘CHECKD_NOSIG()’.

*note .hidden-type;: e3f. Where the type satisfies *note
.full-type.pointer: e3b. and *note .full-type.check: e3c. but not *note
.full-type.sig: e3d. because the structure has a signature but the
structure definition is not visible at point of checking (for example
‘Root’), use ‘CHECKD_NOSIG()’ and reference this tag.  The structure
could be considered for addition to ‘mpmst.h’.

   ---------- Footnotes ----------

   (1) sig.html


File: MemoryPoolSystem.info,  Node: Common assertions,  Prev: Implementation<17>,  Up: Checking<4>

5.6.3 Common assertions
-----------------------

*note .common;: e41. Some assertions are commonly triggered by mistakes
in the client program.  These are listed in the section “Common
assertions and their causes” in the MPS Reference, together with an
explanation of their likely cause, and advice for fixing the problem.
To assist with keeping the MPS Reference up to date, these assertions
are marked with a cross-reference to this tag.  When you update the
assertion, you must also update the MPS Reference.


File: MemoryPoolSystem.info,  Node: Collection framework,  Next: Diagnostic feedback,  Prev: Checking<4>,  Up: Old design

5.7 Collection framework
========================

* Menu:

* Introduction: Introduction<53>.
* Overview: Overview<16>.
* Collection abstractions::
* The tracer::
* Barriers::


File: MemoryPoolSystem.info,  Node: Introduction<53>,  Next: Overview<16>,  Up: Collection framework

5.7.1 Introduction
------------------

*note .intro;: e47. This document describes the Collection Framework.
It’s a framework for implementing garbage collection techniques and
integrating them into a system of collectors that all cooperate in
recycling garbage.


File: MemoryPoolSystem.info,  Node: Overview<16>,  Next: Collection abstractions,  Prev: Introduction<53>,  Up: Collection framework

5.7.2 Overview
--------------

*note .framework;: e49. MPS provides a framework that allows the
integration of many different types of GC strategies and provides many
of the basic services that those strategies use.

*note .framework.cover;: e4a. The framework subsumes most major GC
strategies and allows many efficient techniques, like in-line allocation
or software barriers.

*note .framework.overhead;: e4b. The overhead due to cooperation is low.

     Note: But not non-existent.  Can we say something useful about it?

*note .framework.benefits;: e4c. The ability to combine collectors
contributes significantly to the flexibility of the system.  The
reduction in code duplication contributes to reliability and integrity.
The services of the framework make it easier to write new MM strategies
and collectors.

*note .framework.mpm;: e4d. The Collection Framework is merely a part of
the structure of the MPM. See design.mps.architecture and
design.mps.arch for the big picture.

     Note: Those two documents should be combined into one.  Pekka P.
     Pirinen, 1998-01-15.

Other notable components that the MPM manages to integrate into a single
framework are manually-managed memory and finalization services (see
design.mps.finalize(1)).

     Note: A document describing the design of manually-managed memory
     is missing.  Pekka P. Pirinen, 1998-01-15.

*note .see-also;: e4e. This document assumes basic familiarity with the
ideas of pool (see design.mps.arch.pools) and segment (see
design.mps.seg.over).

   ---------- Footnotes ----------

   (1) finalize.html


File: MemoryPoolSystem.info,  Node: Collection abstractions,  Next: The tracer,  Prev: Overview<16>,  Up: Collection framework

5.7.3 Collection abstractions
-----------------------------

* Menu:

* Colours, scanning and fixing: Colours scanning and fixing.
* Reference sets::


File: MemoryPoolSystem.info,  Node: Colours scanning and fixing,  Next: Reference sets,  Up: Collection abstractions

5.7.3.1 Colours, scanning and fixing
....................................

*note .state;: e51. The framework knows about the three colours of the
tri-state abstraction and free blocks.  Recording the state of each
object is the responsibility of the pool, but the framework gets told
about changes in the states and keeps track of colours in each segment.
Specifically, it records whether a segment might contain white, grey and
black objects with respect to each active trace (see *note .tracer:
e52.)

     Note: Black not currently implemented.  Pekka P. Pirinen,
     1998-01-04.

(A segment might contain objects of all colours at once, or none.)  This
information is approximate, because when an object changes colour, or
dies, it usually is too expensive to determine if it was the last object
of its former colour.

*note .state.transitions;: e53. The possible state transitions are as
follows:

     free   ---alloc--> black (or grey) or white or none
     none   --condemn-> white
     none   --refine--> grey
     grey   ---scan---> black
     white  ----fix---> grey (or black)
     black  --revert--> grey
     white  --reclaim-> free
     black  --reclaim-> none

*note .none-is-black;: e54. Outside of a trace, objects don’t really
have colour, but technically, the colour is black.  Objects are only
allocated grey or white during a trace, and by the time the trace has
finished, they are either dead or black, like the other surviving
objects.  We might then reuse the colour field for another trace, so
it’s convenient to set the colour to black when allocating outside a
trace.  This means that refining the foundation
(analysis.tracer.phase.condemn.refine), actually turns black segments
grey, rather than vice versa, but the principle is the same.

*note .scan-fix;: e55. “Scanning” an object means applying the “fix”
function to all references in that object.  Fixing is the generic name
for the operation that takes a reference to a white object and makes it
non-white (usually grey, but black is a possibility, and so is changing
the reference as we do for weak references).  Typical examples of fix
methods are copying the object into to-space or setting its mark bit.

*note .cooperation;: e56. The separation of scanning and fixing is what
allows different GC techniques to cooperate.  The scanning is done by a
method on the pool that the scanned object resides in, and the fixing is
done by a method on the pool that the reference points to.

*note .scan-all;: e57. Pools provide a method to scan all the grey
objects in a segment.


File: MemoryPoolSystem.info,  Node: Reference sets,  Prev: Colours scanning and fixing,  Up: Collection abstractions

5.7.3.2 Reference sets
......................

*note .refsets;: e59. The cost of scanning can be significantly reduced
by storing remembered sets.  We have chosen a very compact and efficient
implementation, called reference sets, or refsets for short (see
idea.remember).

     Note: design.mps.refset is empty!  Perhaps some of this should go
     there.  Pekka P. Pirinen, 1998-02-19.

This makes the cost of maintaining them low, so we maintain them for all
references out of all scannable segments.

*note .refsets.approx;: e5a. You might describe refsets as summaries of
all references out of an area of memory, so they are only approximations
of remembered sets.  When a refset indicates that an interesting
reference might be present in a segment, we still have to scan the
segment to find it.

*note .refsets.scan;: e5b. The refset information is collected during
scanning.  The scan state protocol provides a way for the pool and the
format scan methods to cooperate in this, and to pass this information
to the tracer module which checks it and updates the segment (see
design.mps.scan(1)).

     Note: Actually, there’s very little doc there.  Pekka P. Pirinen,
     1998-02-17.

*note .refsets.maintain;: e5c. The MPS tries to maintain the refset
information when it moves or changes object.

*note .refsets.pollution;: e5d. Ambiguous references and pointers
outside the arena will introduce spurious zones into the refsets.  We
put up with this to keep the scanning costs down.  Consistency checks on
refsets have to take this into account.

*note .refsets.write-barrier;: e5e. A write-barrier are needed to keep
the mutator from invalidating the refsets when writing to a segment.  We
need one on any scannable segment whose refset is not a superset of the
mutator’s (and that the mutator can see).  If we know what the mutator
is writing and whether it’s a reference, we can just add that reference
to the refset (figuring out whether anything can be removed from the
refset is too expensive).  If we don’t know or if we cannot afford to
keep the barrier up, the framework can union the mutator’s refset to the
segment’s refset.

*note .refset.mutator;: e5f. The mutator’s refset could be computed
during root scanning in the usual way, and then kept up to date by using
a read-barrier.  It’s not a problem that the mutator can create new
pointers out of nothing behind the read-barrier, as they won’t be real
references.  However, this is probably not cost-effective, since it
would cause lots of barrier hits.  We’d need a read-barrier on every
scannable segment whose refset is not a subset of the mutator’s (and
that the mutator can see).  So instead we approximate the mutator’s
refset with the universal refset.

   ---------- Footnotes ----------

   (1) scan.html


File: MemoryPoolSystem.info,  Node: The tracer,  Next: Barriers,  Prev: Collection abstractions,  Up: Collection framework

5.7.4 The tracer
----------------

*note .tracer;: e52. The tracer is an engine for implementing multiple
garbage collection processes.  Each process (called a “trace”) proceeds
independently of the others through five phases as described in
analysis.tracer.  The following sections describe how the action of each
phase fits into the framework.  See design.mps.trace(1) for details

     Note: No, there’s not much there, either.  Possibly some of this
     section should go there.  Pekka P. Pirinen, 1998-02-18.

*note .combine;: e61. The tracer can also combine several traces for
some actions, like scanning a segment or a root.  The methods the tracer
calls to do the work get an argument that tells them which traces they
are expected to act for.

     Note: Extend this.

*note .trace.begin;: e62. Traces are started by external request,
usually from a client function or an action (see design.mps.action).

*note .trace.progress;: e63. The tracer gets time slices from the arena
to work on a given trace.

     Note: This is just a provisional arrangement, in lieu of real
     progress control.  Pekka P. Pirinen, 1998-02-18.

In each slice, it selects a small amount of work to do, based on the
state of the trace, and does it, using facilities provided by the pools.

*note .trace.scan;: e64. A typical unit of work is to scan a single
segment.  The tracer can choose to do this for multiple traces at once,
provided the segment is grey for more than one trace.

*note .trace.barrier;: e65. Barrier hits might also cause a need to scan
:mps:a segment (see *note .hw-barriers.hit: e66.).  Again, the tracer
can :mps:choose to combine traces, when it does this.

*note .mutator-colour;: e67. The framework keeps track of the colour of
the mutator separately for each trace.

* Menu:

* The condemn phase::
* The grey mutator phase::
* The flip phase::
* The black mutator phase::
* The reclaim phase::

   ---------- Footnotes ----------

   (1) trace.html


File: MemoryPoolSystem.info,  Node: The condemn phase,  Next: The grey mutator phase,  Up: The tracer

5.7.4.1 The condemn phase
.........................

*note .phase.condemn;: e69. The agent that creates the trace (see *note
.trace.begin: e62.) determines the condemned set and colours it white.
The tracer then examines the refsets on all scannable segments, and if
it can deduce some segment cannot refer to the white set, it’s
immediately coloured black, otherwise the pool is asked to grey any
objects in the segment that might need to be scanned (in copying pools,
this is typically the whole segment).

*note .phase.condemn.zones;: e6a. To get the maximum benefit from the
refsets, we try to arrange that the zones are a minimal superset (for
example, generations uniquely occupy zones) and a maximal subset
(there’s nothing else in the zone) of the condemned set.  This needs to
be arranged at allocation time (or when copying during collection, which
is much like allocation)

     Note: Soon, this will be handled by segment loci, see
     design.mps.locus(1).

*note .phase.condemn.mutator;: e6b. At this point, the mutator might
reference any objects, that is, it is grey.  Allocation can be in any
colour, most commonly white.

     Note: More could be said about this.

   ---------- Footnotes ----------

   (1) locus.html


File: MemoryPoolSystem.info,  Node: The grey mutator phase,  Next: The flip phase,  Prev: The condemn phase,  Up: The tracer

5.7.4.2 The grey mutator phase
..............................

*note .phase.grey-mutator;: e6d. Grey segments are chosen according to
some sort of progress control and scanned by the pool to make them
black.  Eventually, the tracer will decide to flip or it runs out of
grey segments, and proceeds to the next phase.

     Note: Currently, this phase has not been implemented; all traces
     flip immediately after condemn.  Pekka P. Pirinen, 1998-02-18.

*note .phase.grey-mutator.copy;: e6e. At this stage, we don’t want to
copy condemned objects, because we would need an additional barrier to
keep the mutator’s view of the heap consistent (see
analysis.async-gc.copied.pointers-and-new-copy).

*note .phase.grey-mutator.ambig;: e6f. This is a good time to get all
ambiguous scanning out of the way, because we usually can’t do any after
the flip and because it doesn’t cause any copying.

     Note: Write a detailed explanation of this some day.


File: MemoryPoolSystem.info,  Node: The flip phase,  Next: The black mutator phase,  Prev: The grey mutator phase,  Up: The tracer

5.7.4.3 The flip phase
......................

*note .phase.flip;: e71. The roots (see design.mps.root(1)) are scanned.
This has to be an atomic action as far as the mutator is concerned, so
all threads are suspended for the duration.

*note .phase.flip.mutator;: e72. After this, the mutator is black: if we
use a strong barrier (analysis.async-gc.strong), this means it cannot
refer to white objects.  Allocation will be in black (could be grey as
well, but there’s no point to it).

   ---------- Footnotes ----------

   (1) root.html


File: MemoryPoolSystem.info,  Node: The black mutator phase,  Next: The reclaim phase,  Prev: The flip phase,  Up: The tracer

5.7.4.4 The black mutator phase
...............................

*note .phase.black-mutator;: e74. Grey segments are chosen according to
some sort of progress control and scanned by the pool to make them
black.  Eventually, the tracer runs out of segments that are grey for
this trace, and proceeds to the next phase.

*note .phase.black-mutator.copy;: e75. At this stage white objects can
be relocated, because the mutator cannot see them (as long as a strong
barrier is used, as we must do for a copying collection, see
analysis.async-gc.copied.pointers).


File: MemoryPoolSystem.info,  Node: The reclaim phase,  Prev: The black mutator phase,  Up: The tracer

5.7.4.5 The reclaim phase
.........................

*note .phase.reclaim;: e77. The tracer finds the remaining white
segments and asks the pool to reclaim any white objects in them.

*note .phase.reclaim.barrier;: e78. Once a trace has started reclaiming
objects, the others shouldn’t try to scan any objects that are white for
it, because they might have dangling pointers in them.

     Note: Needs cross-reference to document that is yet to be written.

     Currently, we reclaim atomically, but it could be incremental, or
     even overlapped with a new trace on the same condemned set.  Pekka
     P. Pirinen, 1997-12-31.


File: MemoryPoolSystem.info,  Node: Barriers,  Prev: The tracer,  Up: Collection framework

5.7.5 Barriers
--------------

     Note: An introduction and a discussion of general principles should
     go here.  This is a completely undesigned area.

* Menu:

* Hardware barriers::
* Software barriers::


File: MemoryPoolSystem.info,  Node: Hardware barriers,  Next: Software barriers,  Up: Barriers

5.7.5.1 Hardware barriers
.........................

*note .hw-barriers;: e7b. Hardware barrier services cannot, by their
very nature, be independently provided to each trace.  A segment is
either protected or not, and we have to set the protection on a segment
if any trace needs a hardware barrier on it.

*note .hw-barriers.supported;: e7c. The framework currently supports
segment-oriented Appel-Ellis-Li barriers
(analysis.async-gc.barrier.appel-ellis-li), and write-barriers for
keeping the refsets up-to-date.  It would not be hard to add Steele
barriers (analysis.async-gc.barrier.steele.scalable).

*note .hw-barriers.hit;: e66. When a barrier hit happens, the arena
determines which segment it was on.  The segment colour info is used to
determine whether it had trace barriers on it, and if so, the
appropriate barrier action is performed, using the methods of the owning
pool.  If the segment was write-protected, its refset is unioned with
the refset of the mutator.

     Note: In practice this is ‘RefSetUNIV’.

*note .hw-barriers.hit.multiple;: e7d. Fortunately, if we get a barrier
hit on a segment with multiple trace barriers on it, we can scan it for
all the traces that it had a barrier for.

     Note: Needs link to unwritten section under *note .combine: e61.


File: MemoryPoolSystem.info,  Node: Software barriers,  Prev: Hardware barriers,  Up: Barriers

5.7.5.2 Software barriers
.........................

     Note: Write something about software barriers.


File: MemoryPoolSystem.info,  Node: Diagnostic feedback,  Next: The generic fix function,  Prev: Collection framework,  Up: Old design

5.8 Diagnostic feedback
=======================

* Menu:

* Introduction: Introduction<54>.
* Overview: Overview<17>.
* Requirements: Requirements<33>.
* Usage: Usage<2>.
* How to write a diagnostic::
* How the MPS diagnostic system works::
* References: References<22>.


File: MemoryPoolSystem.info,  Node: Introduction<54>,  Next: Overview<17>,  Up: Diagnostic feedback

5.8.1 Introduction
------------------

*note .intro;: e84. This document describes how to use the diagnostic
feedback mechanism in the Memory Pool System.

*note .sources;: e85. Initially abased on *note [RHSK_2007-04-13]: e86.
and *note [RHSK_2007-04-18]: e87.


File: MemoryPoolSystem.info,  Node: Overview<17>,  Next: Requirements<33>,  Prev: Introduction<54>,  Up: Diagnostic feedback

5.8.2 Overview
--------------

Diagnostic feedback is information created by the MPS diagnostic system
for the purpose of helping MPS programmers and client programmers.

Such a piece of information is called “a diagnostic”.  (See also *note
.parts: e89.)

A diagnostic is not intended to be visible to end users, or readable by
them.

A diagnostic is not intended to be stable from one release to the next:
it may be modified or removed at any time.


File: MemoryPoolSystem.info,  Node: Requirements<33>,  Next: Usage<2>,  Prev: Overview<17>,  Up: Diagnostic feedback

5.8.3 Requirements
------------------

MPS diagnostic feedback code must do these things:

   - calculate, store, and propagate data;

   - collate, synthesise, and format it into a human-useful diagnostic;

   - control (for example, filter) output of diagnostics;

   - use a channel to get the diagnostic out.


File: MemoryPoolSystem.info,  Node: Usage<2>,  Next: How to write a diagnostic,  Prev: Requirements<33>,  Up: Diagnostic feedback

5.8.4 Usage
-----------

To get diagnostic output from the MPS, you must use a variety with
diagnostics compiled-in.  Currently, that means variety.cool.  See
‘config.h’.

There are two mechanism for getting diagnostic output:

  1. Automatically via the telemetry system.  See
     design.mps.telemetry(1), and the “Telemetry” chapter in the manual.

  2. Manually via the debugger.  In the debugger, set break points at
     the places where you want to inspect data structures (or wait for
     the debugger to be entered via an ‘abort()’ call or unhandled
     segmentation fault).  Then at the debugger command prompt, run
     ‘Describe()’ commands of your choice.  For example:

          (gdb) run
          Starting program: mv2test
          Reading symbols for shared libraries +............................. done
          cbs.c:94: MPS ASSERTION FAILED: !cbs->inCBS

          Program received signal SIGABRT, Aborted.
          0x00007fff83e42d46 in __kill ()
          (gdb) frame 12
          #12 0x000000010000b1fc in MVTFree (pool=0x103ffe160, base=0x101dfd000, size=5024) at poolmv2.c:711
          711         Res res = CBSInsert(MVTCBS(mvt), base, limit);
          (gdb) p MVTDescribe(mvt, mps_lib_get_stdout(), 0)
          MVT 0000000103FFE160 {
            minSize: 8
            meanSize: 42
            maxSize: 8192
            fragLimit: 30
            reuseSize: 16384
            fillSize: 8192
            availLimit: 90931
            abqOverflow: FALSE
            splinter: TRUE
            splinterBase: 0000000106192FF0
            splinterLimit: 0000000106193000
            size: 303104
            allocated: 262928
            available: 40176
            unavailable: 0
            # ... etc ...
          }

   ---------- Footnotes ----------

   (1) telemetry.html


File: MemoryPoolSystem.info,  Node: How to write a diagnostic,  Next: How the MPS diagnostic system works,  Prev: Usage<2>,  Up: Diagnostic feedback

5.8.5 How to write a diagnostic
-------------------------------

* Menu:

* Compile away in non-diag varieties; no side effects::
* Writing good paragraph text::


File: MemoryPoolSystem.info,  Node: Compile away in non-diag varieties; no side effects,  Next: Writing good paragraph text,  Up: How to write a diagnostic

5.8.5.1 Compile away in non-diag varieties; no side effects
...........................................................

Wrap code with the *note STATISTIC: e8e. and ‘METER’ macros, to make
sure that non-diagnostic varieties do not execute diagnostic-generating
code.

Diagnostic-generating code must have no side effects.


File: MemoryPoolSystem.info,  Node: Writing good paragraph text,  Prev: Compile away in non-diag varieties; no side effects,  Up: How to write a diagnostic

5.8.5.2 Writing good paragraph text
...................................

Make your diagnostics easy to understand!  Other people will read your
diagnostics!  Make them clear and helpful.  Do not make them terse and
cryptic.  If you use symbols, print a key in the diagnostic.


File: MemoryPoolSystem.info,  Node: How the MPS diagnostic system works,  Next: References<22>,  Prev: How to write a diagnostic,  Up: Diagnostic feedback

5.8.6 How the MPS diagnostic system works
-----------------------------------------

* Menu:

* Parts of the MPS diagnostic system::
* Statistics::
* Related systems::


File: MemoryPoolSystem.info,  Node: Parts of the MPS diagnostic system,  Next: Statistics,  Up: How the MPS diagnostic system works

5.8.6.1 Parts of the MPS diagnostic system
..........................................

*note .parts;: e89. The following facilities are considered part of the
MPS diagnostic system:

   - the ‘Describe()’ methods.

   - the *note STATISTIC: e8e. macros (see ‘mpm.h’);

   - the ‘METER’ macros and meter subsystem.


File: MemoryPoolSystem.info,  Node: Statistics,  Next: Related systems,  Prev: Parts of the MPS diagnostic system,  Up: How the MPS diagnostic system works

5.8.6.2 Statistics
..................

*note .stat;: e93. The statistic system collects information about the
behaviour and performance of the MPS that may be useful for MPS
developers and customers, but which is not needed by the MPS itself for
internal decision-making.

*note .stat.remove;: e94. The space needed for these statistics, and the
code for maintaining them, can therefore be removed (compiled out) in
some varieties.

*note .stat.config;: e95. Statistics are compiled in if ‘CONFIG_STATS’
is defined (in the cool variety) and compiled out if ‘CONFIG_STATS_NONE’
is defined (in the hot and rash varieties).

 -- C Macro: STATISTIC_DECL (decl)

*note .stat.decl;: e97. The *note STATISTIC_DECL: e96. macro is used to
wrap the declaration of storage for a statistic.  Note that the
expansion supplies a terminating semi-colon and so it must not be
followed by a semi-colon in use.  This is so that it can be used in
structure declarations.

 -- C Macro: STATISTIC (gather)

*note .stat.gather;: e98. The *note STATISTIC: e8e. macro is used to
gather statistics.  The argument is a statement and the expansion
followed by a semicolon is syntactically a statement.  The macro expends
to ‘NOOP’ in non-statistical varieties.  (Note that it can’t use
‘DISCARD_STAT’ to check the syntax of the statement because it is
expected to use fields that have been compiled away by *note
STATISTIC_DECL: e96, and these will cause compilation errors.)

*note .stat.gather.effect;: e99. The argument to the *note STATISTIC:
e8e. macro is not executed in non-statistical varieties and must have no
side effects, except for updates to fields that are declared in *note
STATISTIC_DECL: e96, and telemetry output containing the values of such
fields.

 -- C Macro: STATISTIC_WRITE (format, arg)

*note .stat.write;: e9b. The *note STATISTIC_WRITE: e9a. macro is used
in *note WriteF(): 446. argument lists to output the values of
statistics.


File: MemoryPoolSystem.info,  Node: Related systems,  Prev: Statistics,  Up: How the MPS diagnostic system works

5.8.6.3 Related systems
.......................

The MPS diagnostic system is separate from the following other MPS
systems:

   - The telemetry-log-events system.  This emits much more data, in a
     less human-readable form, requires MPS-aware external tools, and is
     more stable from release to release).  In non-diagnostic telemetry
     varieties, the telemetry-log-events system emits events that log
     all normal MPS actions.  In diagnostic telemetry varieties, it may
     emit additional events containing diagnostic information.
     Additionally, the telemetry-log-events stream might in future be
     available as a channel for emitting human-readable text
     diagnostics.  See also design.mps.telemetry(1).

   - The MPS message system.  This is present in all varieties, and
     manages asynchronous communication from the MPS to the client
     program).  However, the MPS message system might in future also be
     available as a channel for emitting diagnostics.  See also
     design.mps.message(2).

   ---------- Footnotes ----------

   (1) telemetry.html

   (2) message.html


File: MemoryPoolSystem.info,  Node: References<22>,  Prev: How the MPS diagnostic system works,  Up: Diagnostic feedback

5.8.7 References
----------------

(RHSK_2007-04-13) Richard Kistruck.  2007-04-13.  “diagnostic feedback
from the MPS(1)”.

(RHSK_2007-04-18) Richard Kistruck.  2007-04-18.  “Diverse types of
diagnostic feedback(2)”.

   ---------- Footnotes ----------

   (1) https://info.ravenbrook.com/mail/2007/04/13/13-07-45/0.txt

   (2) https://info.ravenbrook.com/mail/2007/04/18/10-58-49/0.txt


File: MemoryPoolSystem.info,  Node: The generic fix function,  Next: I/O subsystem,  Prev: Diagnostic feedback,  Up: Old design

5.9 The generic fix function
============================

* Menu:

* Introduction: Introduction<55>.
* Was-marked protocol::
* Implementation: Implementation<18>.


File: MemoryPoolSystem.info,  Node: Introduction<55>,  Next: Was-marked protocol,  Up: The generic fix function

5.9.1 Introduction
------------------

*note .intro;: ea3. Fix is the interface through which the existence of
references are communicated from the MPS client to the MPS. The
interface also allows the value of such references to be changed (this
is necessary in order to implement a moving memory manager).


File: MemoryPoolSystem.info,  Node: Was-marked protocol,  Next: Implementation<18>,  Prev: Introduction<55>,  Up: The generic fix function

5.9.2 Was-marked protocol
-------------------------

*note .was-marked;: ea5. The ‘ScanState’ has a ‘Bool wasMarked’ field.
This is used for finalization.

*note .was-marked.not;: ea6. If a segment’s fix method discovers that
the object referred to by the ref (the one that it is supposed to be
fixing) has not previously been marked (that is, this is the first
reference to this object that has been fixed), and that the object was
white (that is, in condemned space), it should (but need not) set the
‘wasMarked’ field to ‘FALSE’ in the passed ‘ScanState’.

*note .was-marked.otherwise;: ea7. Otherwise, the fix method must leave
the ‘wasMarked’ field unchanged.

*note .was-marked.finalizable;: ea8. The MRG pool
(design.mps.poolmrg(1)) uses the value of the ‘wasMarked’ field to
determine whether an object is finalizable.

   ---------- Footnotes ----------

   (1) poolmrg.html


File: MemoryPoolSystem.info,  Node: Implementation<18>,  Prev: Was-marked protocol,  Up: The generic fix function

5.9.3 Implementation
--------------------

*note .fix.nailed;: eab. In a copying collection, a non-ambiguous fix to
a broken heart should be snapped out 'even if' there is a ‘RankAMBIG’
ref to same object (that is, if the broken heart is nailed); the
‘RankAMBIG’ reference must either be stale (no longer in existence) or
bogus.


File: MemoryPoolSystem.info,  Node: I/O subsystem,  Next: Library interface,  Prev: The generic fix function,  Up: Old design

5.10 I/O subsystem
==================

* Menu:

* Introduction: Introduction<56>.
* Background: Background<4>.
* Purpose: Purpose<2>.
* Requirements: Requirements<34>.
* Architecture: Architecture<6>.
* Interface: Interface<23>.
* I/O module implementations::
* Notes: Notes<5>.
* Attachments::


File: MemoryPoolSystem.info,  Node: Introduction<56>,  Next: Background<4>,  Up: I/O subsystem

5.10.1 Introduction
-------------------

*note .intro;: eb1. This document is the design of the MPS I/O
Subsystem, a part of the plinth.

*note .readership;: eb2. This document is intended for MPS developers.


File: MemoryPoolSystem.info,  Node: Background<4>,  Next: Purpose<2>,  Prev: Introduction<56>,  Up: I/O subsystem

5.10.2 Background
-----------------

*note .bg;: eb4. This design is partly based on the design of the
Internet User Datagram Protocol (UDP). Mainly I used this to make sure I
hadn’t left out anything which we might need.


File: MemoryPoolSystem.info,  Node: Purpose<2>,  Next: Requirements<34>,  Prev: Background<4>,  Up: I/O subsystem

5.10.3 Purpose
--------------

*note .purpose;: eb6. The purpose of the MPS I/O Subsystem is to provide
a means to measure, debug, control, and test a memory manager build
using the MPS.

*note .purpose.measure;: eb7. Measurement consists of emitting data
which can be collected and analysed in order to improve the attributes
of application program, quite possibly by adjusting parameters of the
memory manager (see overview.mps.usage).

*note .purpose.control;: eb8. Control means adjusting the behaviour of
the MM dynamically.  For example, one might want to adjust a parameter
in order to observe the effect, then transfer that adjustment to the
client application later.

*note .purpose.test;: eb9. Test output can be used to ensure that the
memory manager is behaving as expected in response to certain inputs.


File: MemoryPoolSystem.info,  Node: Requirements<34>,  Next: Architecture<6>,  Prev: Purpose<2>,  Up: I/O subsystem

5.10.4 Requirements
-------------------

* Menu:

* General::
* Functional::


File: MemoryPoolSystem.info,  Node: General,  Next: Functional,  Up: Requirements<34>

5.10.4.1 General
................

*note .req.fun.non-hosted;: ebc. The MPM must be a host-independent
system.

*note .req.attr.host;: ebd. It should be easy for the client to set up
the MPM for a particular host (such as a washing machine).


File: MemoryPoolSystem.info,  Node: Functional,  Prev: General,  Up: Requirements<34>

5.10.4.2 Functional
...................

*note .req.fun.measure;: ebf. The subsystem must allow the MPS to
transmit quantitative measurement data to an external tool so that the
system can be tuned.

*note .req.fun.debug;: ec0. The subsystem must allow the MPS to transmit
qualitative information about its operation to an external tool so that
the system can be debugged.

*note .req.fun.control;: ec1. The subsystem must allow the MPS to
receive control information from an external tool so that the system can
be adjusted while it is running.

*note .req.dc.env.no-net;: ec2. The subsystem should operate in
environments where there is no networking available.

*note .req.dc.env.no-fs;: ec3. The subsystem should operate in
environments where there is no filesystem available.


File: MemoryPoolSystem.info,  Node: Architecture<6>,  Next: Interface<23>,  Prev: Requirements<34>,  Up: I/O subsystem

5.10.5 Architecture
-------------------

*note .arch.diagram;: ec5. I/O Architecture Diagram

[missing diagram]

*note .arch.int;: ec6. The I/O Interface is a C function call interface
by which the MPM sends and receives “messages” to and from the hosted
I/O module.

*note .arch.module;: ec7. The modules are part of the MPS but not part
of the freestanding core system (see design.mps.exec-env(1)).  The I/O
module is responsible for transmitting those messages to the external
tools, and for receiving messages from external tools and passing them
to the MPM.

*note .arch.module.example;: ec8. For example, the “file implementation”
might just send/write telemetry messages into a file so that they can be
received/read later by an off-line measurement tool.

*note .arch.external;: ec9. The I/O Interface is part of interface to
the freestanding core system (see design.mps.exec-env(2)).  This is so
that the MPS can be deployed in a freestanding environment, with a
special I/O module.  For example, if the MPS is used in a washing
machine the I/O module could communicate by writing output to the
seven-segment display.

* Menu:

* Example configurations::

   ---------- Footnotes ----------

   (1) exec-env.html

   (2) exec-env.html


File: MemoryPoolSystem.info,  Node: Example configurations,  Up: Architecture<6>

5.10.5.1 Example configurations
...............................

*note .example.telnet;: ecb. This shows the I/O subsystem communicating
with a telnet client over a TCP/IP connection.  In this case, the I/O
subsystem is translating the I/O Interface into an interactive text
protocol so that the user of the telnet client can talk to the MM.

[missing diagram]

*note .example.file;: ecc. This shows the I/O subsystem dumping
measurement data into a file which is later read and analysed.  In this
case the I/O subsystem is simply writing out binary in a format which
can be decoded.

[missing diagram]

*note .example.serial;: ecd. This shows the I/O subsystem communicating
with a graphical analysis tool over a serial link.  This could be useful
for a developer who has two machines in close proximity and no
networking support.

*note .example.local;: ece. In this example the application is talking
directly to the I/O subsystem.  This is useful when the application is a
reflective development environment (such as MLWorks) which wants to
observe its own behaviour.

[missing diagram]


File: MemoryPoolSystem.info,  Node: Interface<23>,  Next: I/O module implementations,  Prev: Architecture<6>,  Up: I/O subsystem

5.10.6 Interface
----------------

*note .if.msg;: ed0. The I/O interface is oriented around opaque binary
“messages” which the I/O module must pass between the MPM and external
tools.  The I/O module need not understand or interpret the contents of
those messages.

*note .if.msg.opaque;: ed1. The messages are opaque in order to minimize
the dependency of the I/O module on the message internals.  It should be
possible for clients to implement their own I/O modules for unusual
environments.  We do not want to reveal the internal structure of our
data to the clients.  Nor do we want to burden them with the details of
our protocols.  We’d also like their code to be independent of ours, so
that we can expand or change the protocols without requiring them to
modify their modules.

*note .if.msg.dgram;: ed2. Neither the MPM nor the external tools should
assume that the messages will be delivered in finite time, exactly once,
or in order.  This will allow the I/O modules to be implemented using
unreliable transport layers such as the Internet User Datagram Protocol
(UDP). It will also give the I/O module the freedom to drop information
rather than block on a congested network, or stop the memory manager
when the disk is full, or similar events which really shouldn’t cause
the memory manager to stop working.  The protocols we need to implement
at the high level can be design to be robust against lossage without
much difficulty.

* Menu:

* I/O module state::
* Message types: Message types<2>.
* Limits::
* Interface set-up and tear-down::
* Message send and receive::


File: MemoryPoolSystem.info,  Node: I/O module state,  Next: Message types<2>,  Up: Interface<23>

5.10.6.1 I/O module state
.........................

*note .if.state;: ed4. The I/O module may have some internal state to
preserve.  The I/O Interface defines a type for this state, *note
mps_io_t: 2ce, a pointer to an incomplete structure ‘mps_io_s’.  The I/O
module is at liberty to define this structure.


File: MemoryPoolSystem.info,  Node: Message types<2>,  Next: Limits,  Prev: I/O module state,  Up: Interface<23>

5.10.6.2 Message types
......................

*note .if.type;: ed6. The I/O module must be able to deliver messages of
several different types.  It will probably choose to send them to
different destinations based on their type: telemetry to the measurement
tool, debugging output to the debugger, etc.

     typedef int mps_io_type_t;
     enum {
       MPS_IO_TYPE_TELEMETRY,
       MPS_IO_TYPE_DEBUG
     };


File: MemoryPoolSystem.info,  Node: Limits,  Next: Interface set-up and tear-down,  Prev: Message types<2>,  Up: Interface<23>

5.10.6.3 Limits
...............

*note .if.message-max;: ed8. The interface will define an unsigned
integral constant ‘MPS_IO_MESSAGE_MAX’ which will be the maximum size of
messages that the MPM will pass to *note mps_io_send(): ed9. (*note
.if.send: eda.) and the maximum size it will expect to receive from
*note mps_io_receive(): edb.


File: MemoryPoolSystem.info,  Node: Interface set-up and tear-down,  Next: Message send and receive,  Prev: Limits,  Up: Interface<23>

5.10.6.4 Interface set-up and tear-down
.......................................

*note .if.create;: edd. The MPM will call *note mps_io_create(): 2b9. to
set up the I/O module.  On success, this function should return *note
MPS_RES_OK: 5a.  It may also initialize a “state” value which will be
passed to subsequent calls through the interface.

*note .if.destroy;: ede. The MPM will call *note mps_io_destroy(): 2cf.
to tear down the I/O module, after which it guarantees that the state
value will not be used again.  The ‘state’ parameter is the state
previously returned by *note mps_io_create(): 2b9. (*note .if.create:
edd.).


File: MemoryPoolSystem.info,  Node: Message send and receive,  Prev: Interface set-up and tear-down,  Up: Interface<23>

5.10.6.5 Message send and receive
.................................

 -- C Function: extern *note mps_res_t: 14d. mps_io_send (mps_io_t
          state, mps_io_type_t type, void *message, size_t size)

*note .if.send;: eda. The MPM will call *note mps_io_send(): ed9. when
it wishes to send a message to a destination.  The ‘state’ parameter is
the state previously returned by *note mps_io_create(): 2b9. (*note
.if.create: edd.).  The ‘type’ parameter is the type (*note .if.type:
ed6.) of the message.  The ‘message’ parameter is a pointer to a buffer
containing the message, and ‘size’ is the length of that message, in
bytes.  The I/O module must make an effort to deliver the message to the
destination, but is not expected to guarantee delivery.  The function
should return *note MPS_RES_IO: 150. only if a serious error occurs that
should cause the MPM to return with an error to the client application.
Failure to deliver the message does not count.

     Note: Should there be a timeout parameter?  What are the timing
     constraints?  *note mps_io_send(): ed9. shouldn’t block.

 -- C Function: extern *note mps_res_t: 14d. mps_io_receive (mps_io_t
          state, void **buffer_o, size_t *size_o)

*note .if.receive;: ee0. The MPM will call *note mps_io_receive(): edb.
when it wants to see if a message has been sent to it.  The ‘state’
parameter is the state previously returned by *note mps_io_create():
2b9. (*note .if.create: edd.).  The ‘buffer_o’ parameter is a pointer to
a value which should be updated with a pointer to a buffer containing
the message received.  The ‘size_o’ parameter is a pointer to a value
which should be updated with the length of the message received.  If
there is no message ready for receipt, the length returned should be
zero.

     Note: Should we be able to receive truncated messages?  How can
     this be done neatly?


File: MemoryPoolSystem.info,  Node: I/O module implementations,  Next: Notes<5>,  Prev: Interface<23>,  Up: I/O subsystem

5.10.7 I/O module implementations
---------------------------------

* Menu:

* Routeing::


File: MemoryPoolSystem.info,  Node: Routeing,  Up: I/O module implementations

5.10.7.1 Routeing
.................

The I/O module must decide where to send the various messages.  A
file-based implementation could put them in different files based on
their types.  A network-based implementation must decide how to address
the messages.  In either case, any configuration must either be
statically compiled into the module, or else read from some external
source such as a configuration file.


File: MemoryPoolSystem.info,  Node: Notes<5>,  Next: Attachments,  Prev: I/O module implementations,  Up: I/O subsystem

5.10.8 Notes
------------

The external tools should be able to reconstruct stuff from partial
info.  For example, you come across a fragment of an old log containing
just a few old messages.  What can you do with it?

Here’s some completely untested code which might do the job for UDP.

     #include "mpsio.h"

     #include <sys/types.h>
     #include <sys/socket.h>
     #include <netinet/in.h>
     #include <arpa/inet.h>
     #include <fcntl.h>
     #include <errno.h>

     typedef struct mps_io_s {
       int sock;
       struct sockaddr_in mine;
       struct sockaddr_in telemetry;
       struct sockaddr_in debugging;
     } mps_io_s;

     static mps_bool_t inited = 0;
     static mps_io_s state;


     mps_res_t mps_io_create(mps_io_t *mps_io_o)
     {
       int sock, r;

       if(inited)
         return MPS_RES_LIMIT;

       state.mine = /* setup somehow from config */;
       state.telemetry = /* setup something from config */;
       state.debugging = /* setup something from config */;

       /* Make a socket through which to communicate. */
       sock = socket(AF_INET, SOCK_DGRAM, 0);
       if(sock == -1) return MPS_RES_IO;

       /* Set socket to non-blocking mode. */
       r = fcntl(sock, F_SETFL, O_NDELAY);
       if(r == -1) return MPS_RES_IO;

       /* Bind the socket to some UDP port so that we can receive messages. */
       r = bind(sock, (struct sockaddr *)&state.mine, sizeof(state.mine));
       if(r == -1) return MPS_RES_IO;

       state.sock = sock;

       inited = 1;

       *mps_io_o = &state;
       return MPS_RES_OK;
     }


     void mps_io_destroy(mps_io_t mps_io)
     {
       assert(mps_io == &state);
       assert(inited);

       (void)close(state.sock);

       inited = 0;
     }


     mps_res_t mps_io_send(mps_io_t mps_io, mps_type_t type,
                           void *message, size_t size)
     {
       struct sockaddr *toaddr;

       assert(mps_io == &state);
       assert(inited);

       switch(type) {
         MPS_IO_TYPE_TELEMETRY:
         toaddr = (struct sockaddr *)&state.telemetry;
         break;

         MPS_IO_TYPE_DEBUGGING:
         toaddr = (struct sockaddr *)&state.debugging;
         break;

         default:
         assert(0);
         return MPS_RES_UNIMPL;
       }

       (void)sendto(state.sock, message, size, 0, toaddr, sizeof(*toaddr));

       return MPS_RES_OK;
     }


     mps_res_t mps_io_receive(mps_io_t mps_io,
                              void **message_o, size_t **size_o)
     {
       int r;
       static char buffer[MPS_IO_MESSAGE_MAX];

       assert(mps_io == &state);
       assert(inited);

       r = recvfrom(state.sock, buffer, sizeof(buffer), 0, NULL, NULL);
       if(r == -1)
         switch(errno) {
           /* Ignore interrupted system calls, and failures due to lack */
           /* of resources (they might go away.) */
           case EINTR: case ENOMEM: case ENOSR:
           r = 0;
           break;

           default:
           return MPS_RES_IO;
         }

       *message_o = buffer;
       *size_o = r;
       return MPS_RES_OK;
     }


File: MemoryPoolSystem.info,  Node: Attachments,  Prev: Notes<5>,  Up: I/O subsystem

5.10.9 Attachments
------------------

“O Architecture Diagram” “O Configuration Diagrams”


File: MemoryPoolSystem.info,  Node: Library interface,  Next: Locus manager,  Prev: I/O subsystem,  Up: Old design

5.11 Library interface
======================

* Menu:

* Introduction: Introduction<57>.
* Goals: Goals<2>.
* Description: Description<2>.
* Implementation: Implementation<19>.


File: MemoryPoolSystem.info,  Node: Introduction<57>,  Next: Goals<2>,  Up: Library interface

5.11.1 Introduction
-------------------

*note .intro;: eea. This document is the design of the MPS Library
Interface, a part of the plinth.

*note .readership;: eeb. Any MPS developer.  Any clients that are
prepared to read this in order to get documentation.


File: MemoryPoolSystem.info,  Node: Goals<2>,  Next: Description<2>,  Prev: Introduction<57>,  Up: Library interface

5.11.2 Goals
------------

*note .goal;: eed. The goals of the MPS library interface are:

*note .goal.host;: eee. To control the dependency of the MPS on the
hosted ISO C library so that the core MPS remains freestanding (see
design.mps.exec-env(1)).

*note .goal.free;: eef. To allow the core MPS convenient access to ISO C
functionality that is provided on freestanding platforms (see
design.mps.exec-env(2)).

   ---------- Footnotes ----------

   (1) exec-env.html

   (2) exec-env.html


File: MemoryPoolSystem.info,  Node: Description<2>,  Next: Implementation<19>,  Prev: Goals<2>,  Up: Library interface

5.11.3 Description
------------------

* Menu:

* Overview: Overview<18>.


File: MemoryPoolSystem.info,  Node: Overview<18>,  Up: Description<2>

5.11.3.1 Overview
.................

*note .overview.access;: ef2. The core MPS needs to access functionality
that could be provided by an ISO C hosted environment.

*note .overview.hosted;: ef3. The core MPS must not make direct use of
any facilities in the hosted environment (design.mps.exec-env(1)).
However, it is sensible to make use of them when the MPS is deployed in
a hosted environment.

*note .overview.hosted.indirect;: ef4. The core MPS does not make any
direct use of hosted ISO C library facilities.  Instead, it indirects
through the MPS Library Interface, impl.h.mpslib.

*note .overview.provision.client;: ef5. In a freestanding environment
the client is expected to provide functions meeting this interface to
the MPS.

*note .overview.provision.hosted;: ef6. In a hosted environment,
impl.c.mpsliban may be used.  It just maps impl.h.mpslib directly onto
the ISO C library equivalents.

   ---------- Footnotes ----------

   (1) exec-env.html


File: MemoryPoolSystem.info,  Node: Implementation<19>,  Prev: Description<2>,  Up: Library interface

5.11.4 Implementation
---------------------

*note .impl;: ef8. The MPS Library Interface comprises a header file
impl.h.mpslib and some documentation.

*note .impl.decl;: ef9. The header file defines the interface to
definitions which parallel those parts of the non-freestanding ISO
headers which are used by the MPS.

*note .impl.include;: efa. The header file also includes the
freestanding header ‘<stddef.h>’.


File: MemoryPoolSystem.info,  Node: Locus manager,  Next: GC messages,  Prev: Library interface,  Up: Old design

5.12 Locus manager
==================

* Menu:

* Introduction: Introduction<58>.
* Overview: Overview<19>.
* Definitions: Definitions<9>.
* Requirements: Requirements<35>.
* Analysis: Analysis<4>.
* Interface: Interface<24>.
* Architecture: Architecture<7>.
* Implementation: Implementation<20>.
* Notes: Notes<6>.


File: MemoryPoolSystem.info,  Node: Introduction<58>,  Next: Overview<19>,  Up: Locus manager

5.12.1 Introduction
-------------------

*note .intro;: f00. The locus manager coordinates between the pools and
takes the burden of having to be clever about tract/group placement away
from the pools, preserving trace differentiability and contiguity where
appropriate.

*note .source;: f01. mail.gavinm.1998-02-05.17-52(1),
mail.ptw.1998-02-05.19-53(2), mail.pekka.1998-02-09.13-58(3), and
mail.gavinm.1998-02-09.14-05(4).

*note .readership;: f02. Any MPS developer.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/02/05/17-52/0.txt

   (2) 
https://info.ravenbrook.com/project/mps/mail/1998/02/05/19-53/0.txt

   (3) 
https://info.ravenbrook.com/project/mps/mail/1998/02/09/13-58/0.txt

   (4) 
https://info.ravenbrook.com/project/mps/mail/1998/02/09/14-05/0.txt


File: MemoryPoolSystem.info,  Node: Overview<19>,  Next: Definitions<9>,  Prev: Introduction<58>,  Up: Locus manager

5.12.2 Overview
---------------

The MPS manages three main resources:

  1. storage;

  2. address space;

  3. time.

The locus manager manages address space at the arena level.

     Note: Tucker was right: see mail.ptw.1998-11-02.14-25(1).  Richard
     Kistruck, 2007-04-24.

When a pool wants some address space, it expresses some preferences to
the locus manager.  The locus manager and the arena (working together)
try to honour these preferences, and decide what address space the pool
gets.

Preferences are expressed by the ‘LocusPref’ argument to ‘SegAlloc()’.
Note that, when they call ‘SegAlloc()’, pools are asking for address
space and writeable storage simultaneously, in a single call.  There is
currently no way for pools to reserve address space without requesting
storage.

* Menu:

* Why is it important to manage address space?::
* Discovering the layout::

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/11/02/14-25/0.txt


File: MemoryPoolSystem.info,  Node: Why is it important to manage address space?,  Next: Discovering the layout,  Up: Overview<19>

5.12.2.1 Why is it important to manage address space?
.....................................................

  1. Trace differentiability

     Carefully chosen addresses are used by reference tracing systems
     (ie.  automatic pools), to categorise objects into clumps; and to
     summarise and cheaply find references between clumps.

     Different clumps will become worth collecting at different times
     (the classic example, of course, is generations in a generational
     collector).  For these partial collections to be efficient, it must
     be cheap to keep these clumps differentiable, cheap to condemn
     (Whiten) a particular clump, and cheap to find a good conservative
     approximation to all inward references to a clump (both initially
     to construct the Grey set, and to make scanning the Grey set
     efficient).

     This is what the MPS zone mechanism is all about.

     The locus manager manages the mapping from clumps to zones.

     To specify a clump, pools can pass ‘LocusPrefZONESET’ and a set of
     zones to ‘LocusPrefExpress()’.

  2. Prevent address space fragmentation (within the arena)

     Address space is not infinite.

     In some use cases, the MPS is required to remain efficient when
     using very nearly all available address space and storage.  For
     example, with the client-arena class, where the only address space
     available is that of the storage available.

     Even with the VM arena class, typical storage sizes (as of 2007)
     can make 32-bit address space constrained: the client may need
     several gigabytes, which leaves little spare address space.

     Address space fragmentation incurs failure when there is no way to
     allocate a big block of address space.  The big block may be
     requested via the MPS (by the client), or by something else in the
     same process, such as a third-party graphics library, image
     library, etc.

     Address space fragmentation incurs cost when:

        - desired large-block requests (such as for buffering) are
          denied, causing them to be re-requested as a smaller block, or
          as several smaller blocks;

        - possible operating-system costs in maintaining a fragmented
          mapping?

  3. Prevent storage fragmentation (within tracts and segments)

          Storage is not infinite: it is allocated in multiples of a
          fixed-size tract.  Small lonely objects, each retaining a
          whole tract, cause storage fragmentation.

          Non-moving pools manage this fragmentation with placement
          strategies that use:

             - co-located death (in space and time);

             - segment merging and splitting.

          These pool-level strategies always care about contiguity of
          object storage.  They also often care about the 'ordering' of
          addresses, because pool code uses an address-ordered search
          when choosing where to place a new object.  For these two
          reasons, the address chosen (by the locus manager and arena)
          for new tracts is important.

          Certain specialised pools, and/or some client programs that
          use them, have carefully tuned segment sizes, positioning, and
          search order.  Be careful: seemingly inconsequential changes
          can catastrophically break this tuning.

          Pools can specify a preference for High and Low ends of
          address space, which implies a search-order.  Pools could also
          specify clumping, using ‘LocusPrefZONESET’.


File: MemoryPoolSystem.info,  Node: Discovering the layout,  Prev: Why is it important to manage address space?,  Up: Overview<19>

5.12.2.2 Discovering the layout
...............................

The locus manager is not given advance notice of how much address space
will be required with what preferences.  Instead, the locus manager
starts with an empty layout, and adapts it as more requests come in over
time.  It is attempting to discover a suitable layout by successive
refinement.  This is ambitious.


File: MemoryPoolSystem.info,  Node: Definitions<9>,  Next: Requirements<35>,  Prev: Overview<19>,  Up: Locus manager

5.12.3 Definitions
------------------

*note .note.cohort;: f07. We use the word “cohort” in its usual sense
here, but we’re particularly interested in cohorts that have properties
relevant to tract placement.  It is such cohorts that the pools will try
to organize using the services of the locus manager.  Typical properties
would be trace differentiability or (en masse) death-time
predictability.  Typical cohorts would be instances of a
non-generational pool, or generations of a collection strategy.

*note .def.trace.differentiability;: f08. Objects (and hence tracts)
that are collected, may or may not have “trace differentiability” from
each other, depending on their placement in the different zones.
Objects (or pointers to them) can also have trace differentiability (or
not) from non-pointers in ambiguous references; in practice, we will be
worried about low integers, that may appear to be in zones 0 or -1.


File: MemoryPoolSystem.info,  Node: Requirements<35>,  Next: Analysis<4>,  Prev: Definitions<9>,  Up: Locus manager

5.12.4 Requirements
-------------------

*note .req.cohort;: f0a. Tract allocations must specify the cohort they
allocate in.  These kind of cohorts will be called loci, and they will
have such attributes as are implied by the other requirements.
Critical.

*note .req.counter.objects;: f0b. As a counter-requirement, pools are
expected to manage objects.  Objects the size of a tract allocation
request (segment-sized) are exceptional.  Critical.  *note
.req.counter.objects.just;: f0c. This means the locus manager is not
meant to solve the problems of allocating large objects, and it isn’t
required to know what goes on in pools.

*note .req.contiguity;: f0d. Must support a high level of contiguity
within cohorts when requested.  This means minimizing the number of
times a cohort is made aware of discontiguity.  Essential (as we’ve
effectively renegotiated this in SW, down to a vague hope that certain
critical cohorts are not too badly fragmented).  *note
.req.contiguity.just;: f0e. TSBA.

*note .req.contiguity.specific;: f0f. It should be possible to request
another allocation next to a specific tract on either side (or an
extension in that direction, as the case may be).  Such a request can
fail, if there’s no space there.  Nice.  It would also be nice to have
one for “next to the largest free block”.

*note .req.differentiable;: f10. Must support the trace
differentiability of segments that may be condemned separately.  Due to
the limited number of zones, it must be possible to place several
cohorts into the same zone.  Essential.

*note .req.differentiable.integer;: f11. It must be possible to place
collectable allocations so that they are trace-differentiable from small
integers.  Essential.

*note .req.disjoint;: f12. Must support the disjointness of pages that
have different VM properties (such as mutable/immutable,
read-only/read-write, and different lifetimes).  Optional.

     Note: I expect the implementation will simply work at page or
     larger granularity, so the problem will not arise, but Tucker
     insisted on stating this as a requirement.  Pekka P. Pirinen,
     1998-10-28.

*note .req.low-memory;: f13. The architecture of the locus manager must
not prevent the design of efficient applications that often use all
available memory.  Critical.  *note .req.low-memory.expl;: f14. This
basically says it must be designed to perform well in low-memory
conditions, but that there can be configurations where it doesn’t do as
well, as long as this is documented for the application programmer.
Note that it doesn’t say all applications are efficient, only that if
you manage to design an otherwise efficient application, the locus
manager will not sink it.

*note .req.address;: f15. Must conserve address space in VM arenas to a
reasonable extent.  Critical.

*note .req.inter-pool;: f16. Must support the association of sets of
tracts in different pools into one cohort.  Nice.

*note .req.ep-style;: f17. Must support the existing EP-style of
allocation whereby allocation is from one end of address space either
upwards or downwards (or a close approximation thereto with the same
behavior).  *note .req.ep-style.just;: f18. We cannot risk disrupting a
policy with well-known properties when this technology is introduced.

*note .req.attributes;: f19. There should be a way to inform the locus
manager about various attributes of cohorts that might be useful for
placement: deathtime, expected total size, and so on.  Optional.  It’s a
given that the cohorts must then have these attributes, within the
limits set in the contract of the appropriate interface.  *note
.req.attributes.action;: f1a. The locus manager should use the
attributes to guide its placement decisions.  Nice.

*note .req.blacklisting;: f1b. There should be a way of maintaining at
least one blacklist for pages (or some other small unit), that can
not/should not be allocated to collectable pools.  Optional.

     Note: How to do blacklist breaking for ambiguous refs?

*note .req.hysteresis;: f1c. There should be a way to indicate which
cohorts fluctuate in size and by how much, to guide the arena hysteresis
to hold on to suitable pages.  Optional.


File: MemoryPoolSystem.info,  Node: Analysis<4>,  Next: Interface<24>,  Prev: Requirements<35>,  Up: Locus manager

5.12.5 Analysis
---------------

*note .analysis.sw;: f1e. Almost any placement policy would be an
improvement on the current SW one.

*note .analysis.cause-and-effect;: f1f. The locus manager doesn’t
usually need to know 'why' things need to be differentiable, disjoint,
contiguous, and so on.  Abstracting the reason away from the interface
makes it more generic, more likely to have serendipitous new uses.
Attributes described by a quantity (deathtime, size, etc.)  are an
exception to this, because we can’t devise a common measure.

*note .analysis.stable;: f20. The strategy must be stable: it must avoid
repeated recomputation, especially the kind that switches between
alternatives with a short period (repeated “bites” out the same region
or flip-flopping between two regions).

*note .analysis.fragmentation;: f21. There’s some call to avoid
fragmentation in cohorts that don’t need strict contiguity, but this is
not a separate requirement, since fragmentation is a global condition,
and can only be ameliorated if there’s a global strategy that clumps
allocations together.

*note .analysis.deathtime;: f22. Cohorts with good death-time clumping
of their objects could use some locality of tract allocation, because it
increases the chances of creating large holes in the address space (for
other allocation to use).  OTOH. many cohorts will not do multiple frees
in short succession, or at least cannot reasonably be predicted to do
so.  This locality is not contiguity, nor is it low fragmentation, it’s
just the requirement to place the new tracts next to the tract where the
last object was allocated in the cohort.  Note that the placement of
objects is under the control of the pool, and the locus manager will not
know it, therefore this requirement should be pursued by requesting
allocation next to a particular tract (which we already have a
requirement for).

*note .analysis.asymmetrical;: f23. The strategy has to be asymmetrical
with respect to cohorts growing and shrinking.  The reason of this
asymmetry is that it can choose where to grow, but it cannot choose
where to shrink (except in a small way by growing with good locality).


File: MemoryPoolSystem.info,  Node: Interface<24>,  Next: Architecture<7>,  Prev: Analysis<4>,  Up: Locus manager

5.12.6 Interface
----------------

*note .interface.locus;: f25. A cohort will typically reside on multiple
tracts (and the pools will avoid putting objects of other cohorts on
them), so there should be an interface to describe the properties of the
cohort, and associate each allocation request with the cohort.  We shall
call such an object, created to represent a cohort, a locus (pl.  loci).

*note .interface.locus.pool;: f26. Loci will usually be created by the
pool that uses it.  Some of the locus attributes will be inherited from
client-specified pool attributes [this means there will be additional
pool attributes].

*note .interface.detail;: f27. This describes interface in overview; for
details, see implementation section and code, or user doc.

* Menu:

* Loci::
* Peaks::


File: MemoryPoolSystem.info,  Node: Loci,  Next: Peaks,  Up: Interface<24>

5.12.6.1 Loci
.............

 -- C Function: *note Res: 55f. LocusCreate (Locus *locusReturn,
          LocusAttrs attrs, ZoneGroup zg, LocusAllocDesc adesc)

*note .function.create;: f2a. A function to create a locus: ‘adesc’
contains the information about the allocation sequences in the locus,
‘zg’ is used for zone differentiability, and ‘attrs’ encodes the
following:

   - *note .locus.contiguity;: f2b. A locus can be contiguous.  This
     means performing as required in *note .req.contiguity: f0d,
     non-contiguous allocations can be freely placed anywhere (but
     efficiency dictates that similar allocations are placed close
     together and apart from others).

   - *note .locus.blacklist;: f2c. Allocations in the locus will avoid
     blacklisted pages (for collectable segments).

   - *note .locus.zero;: f2d. Allocations in the locus are zero-filled.

     Note: Other attributes will be added, I’m sure.

*note .interface.zone-group;: f2e. The locus can be made a member of a
zone group.  Passing ‘ZoneGroupNONE’ means it’s not a member of any
group (allocations will be placed without regard to zone, except to keep
them out of stripes likely to be needed for some group).

     Note: I propose no mechanism for managing zone groups at this time,
     since it’s only used internally for one purpose.  Pekka P. Pirinen,
     2000-01-17.

*note .interface.size;: f2f. An allocation descriptor (‘LocusAllocDesc’)
contains various descriptions of how the locus will develop over time
(inconsistent specifications are forbidden, of course):

   - *note .interface.size.typical-alloc;: f30. Size of a typical
     allocation in this locus, in bytes.  This will mainly affect the
     grouping of non-contiguous loci.

   - *note .interface.size.large-alloc;: f31. Typical large allocation
     that the manager should try to allow for (this allows some relief
     from *note .req.counter.objects: f0b.), in bytes.  This will mainly
     affect the size of gaps that will be allotted adjoining this locus.

   - 
     *note .interface.size.direction;: f32. Direction of growth: up/down/none.

          Only useful if the locus is contiguous.

   - *note .interface.size.lifetime;: f33. Some measure of the lifetime
     of tracts (not objects) in the cohort.

          Note: Don’t know the details yet, probably only useful for
          placing similar cohorts next to each other, so the details
          don’t actually matter.  Pekka P. Pirinen, 2000-01-17.

   - *note .interface.size.deathtime;: f34. Some measure of the
     deathtime of tracts (not objects) in the cohort.

          Note: Ditto.  Pekka P. Pirinen, 2000-01-17.

*note .function.init;: f35. ‘LocusInit()’ is like *note LocusCreate():
f29, but without the allocation.  This is the usual interface, since
most loci are embedded in a pool or something.

*note .function.alloc;: f36. ‘ArenaAlloc()’ to take a locus argument.
‘ArenaAllocHere()’ is like it, plus it takes a tract and a specification
to place the new allocation immediately above/below a given tract; if
that is not possible, it returns ‘ResFAIL’ (this will make it useful for
reallocation functionality).

 -- C Function: void ArenaSetTotalLoci (Arena arena, Size nLoci, Size
          nZoneGroups)

*note .function.set-total;: f38. A function to tell the arena the
expected number of (non-miscible client) loci, and of zone groups.


File: MemoryPoolSystem.info,  Node: Peaks,  Prev: Loci,  Up: Interface<24>

5.12.6.2 Peaks
..............

 -- C Function: *note mps_res_t: 14d. mps_peak_create (mps_peak_t*,
          mps_arena_t)

*note .function.peak.create;: f3b. A function to create a peak.  A
newly-created peak is open, and will not be used to guide the strategy
of the locus manager.

 -- C Function: *note mps_res_t: 14d. mps_peak_describe_pool
          (mps_peak_t, mps_pool_t, mps_size_desc_t)

*note .function.peak.add;: f3d. A function to add a description of the
state of one pool into the peak.  Calling this function again for the
same peak and pool instance will replace the earlier description.

*note .function.peak.add.size;: f3e. The size descriptor contains a
total size in bytes or percent of arena size.

     Note: Is this right?  Pekka P. Pirinen, 2000-01-17.

*note .function.peak.add.remove;: f3f. Specifying a ‘NULL’ size will
remove the pool from the peak.  The client is not allowed to destroy a
pool that is mentioned in any peak; it must be first removed from the
peak, or the peak must be destroyed.  This is to ensure that the client
adjusts the peaks in a manner that makes sense to the application; the
locus manager can’t know how to do that.

 -- C Function: *note mps_res_t: 14d. mps_peak_close (mps_peak_t)

*note .function.peak.close;: f41. A function to indicate that all the
significant pools have been added to the peak, and it can now be used to
guide the locus manager.  For any pool not described in the peak, the
locus manager will take its current size at any given moment as the best
prediction of its size at the peak.

*note .function.peak.close.after;: f42. It is legal to add more
descriptions to the peak after closing, but this will reopen the peak,
and it will have to be closed before the locus manager will use it
again.  The locus manager uses the previous closed state of the peak,
while this is going on.

 -- C Function: void mps_peak_destroy (mps_peak_t)

*note .function.peak.destroy;: f44. A function to destroy a peak.

*note .interface.ep-style;: f45. This satisfies *note .req.ep-style:
f17. by allowing SW to specify zero size for most pools (which will
cause them to be place next to other loci with the same growth
direction).

     Note: Not sure this is good enough, but we’ll try it first.  Pekka
     P. Pirinen, 2000-01-17.


File: MemoryPoolSystem.info,  Node: Architecture<7>,  Next: Implementation<20>,  Prev: Interface<24>,  Up: Locus manager

5.12.7 Architecture
-------------------

* Menu:

* Data objects::
* Overview of strategy::
* Allocation: Allocation<3>.
* Deallocation::
* Region placement recomputation::


File: MemoryPoolSystem.info,  Node: Data objects,  Next: Overview of strategy,  Up: Architecture<7>

5.12.7.1 Data objects
.....................

*note .arch.locus;: f48. To represent the cohorts, we have locus
objects.  Usually a locus is embedded in a pool instance, but
generations are separate loci.

*note .arch.locus.attr;: f49. contiguity, blacklist, zg, current region,
@@@@

*note .arch.locus.attr.exceptional;: f4a. The client can define a
typical large allocation for the locus.  Requests substantially larger
than that are deemed exceptional.

*note .arch.zone-group;: f4b. To satisfy *note .req.differentiable: f10,
we offer zone groups.  Each locus can be a member of a zone group, and
the locus manager will attempt to place allocations in this locus in
different zones from all the other zone groups.  A zone-group is
represented as @@@@.

*note .arch.page-table;: f4c. A page table is maintained by the arena,
as usual to track association between tracts, pools and segments, and
mapping status for VM arenas.

*note .arch.region;: f4d. All of the address space is divided into
disjoint regions, represented by region objects.  These objects store
their current limits, and high and low watermarks of currently allocated
tracts (we hope there’s usually a gap of empty space between regions).
The limits are actually quite porous and flexible.

*note .arch.region.assoc;: f4e. Each region is associated with one
contiguous locus or any number of non-contiguous loci (or none).  We
call the first kind of region “contiguous”.  *note .arch.locus.assoc;:
f4f. Each locus remembers all regions where it has tracts currently,
excepting the badly-placed allocations (see below).  It is not our
intention that any locus would have very many, or that loci that share
regions would have any reason to stop doing do.

*note .arch.region.more;: f50. Various quantities used by the placement
computation are also stored in the regions and the loci.  Regions are
created (and destroyed) by the placement recomputation.  Regions are
located in stripes (if it’s a zoned region), but they can extend into
neighboring stripes if an exceptionally large tract allocation is
requested (to allow for large objects).

*note .arch.chunk;: f51. Arenas may allocate more address space in
additional chunks, which may be disjoint from the existing chunks.
Inter-chunk space will be represented by dummy regions.  There are also
sentinel regions at both ends of the address space.  See
design.mps.arena.chunk(1).

   ---------- Footnotes ----------

   (1) arena.html#design.mps.arena.chunk


File: MemoryPoolSystem.info,  Node: Overview of strategy,  Next: Allocation<3>,  Prev: Data objects,  Up: Architecture<7>

5.12.7.2 Overview of strategy
.............................

*note .arch.strategy.delay;: f54. The general strategy is to delay
placement decisions until they have to be made, but no later.

*note .arch.strategy.delay.until;: f55. Hence, the locus manager only
makes placement decisions when an allocation is requested (frees and
other operations might set a flag to cause the next allocation to
redecide).  This also allows the client to change the peak and pool
configuration in complicated ways without causing a lot of
recomputation, by doing all the changes without allocating in the middle
(unless the control pool needs more space because of the changes).

*note .arch.strategy.normal;: f56. While we want the placement to be
sophisticated, we do not believe it is worth the effort to consider all
the data at each allocation.  Hence, allocations are usually just placed
in one of the regions used previously (see *note .arch.alloc: f57.)
without reconsidering the issues.

*note .arch.strategy.normal.limit;: f58. However, the manager sets
precautionary limits on the regions to ensure that the placement
decisions are revisited when an irrevocable placement is about to be
made.

*note .arch.strategy.create;: f59. The manager doesn’t create new
regions until they are needed for allocation (but it might compute where
they could be placed to accommodate a peak).


File: MemoryPoolSystem.info,  Node: Allocation<3>,  Next: Deallocation,  Prev: Overview of strategy,  Up: Architecture<7>

5.12.7.3 Allocation
...................

*note .arch.alloc;: f57. Normally, each allocation to a locus is placed
in its current region.  New regions are only sought when necessary to
fulfill an allocation request or when there is reason to think the
situation has changed significantly (see *note .arch.significant: f5b.).

*note .arch.alloc.same;: f5c. An allocation is first attempted next to
the previous allocation in the same locus, respecting growth direction.
If that is not possible, a good place in the current region is sought.
*note .arch.alloc.same.hole;: f5d. At the moment, for finding a good
place within a region, we just use the current algorithm, limited to the
region.  In future, the placement within regions will be more clever.

*note .arch.alloc.extend;: f5e. If there’s no adequate hole in the
current region and the request is not exceptional, the neighboring
regions are examined to see if the region could be extended at one
border.  (This will basically only be done if the neighbor has shrunk
since the last placement recomputation, because the limit was set on
sophisticated criteria, and should not be changed without
justification.)  *note .arch.alloc.extend.here;: f5f. When an allocation
is requested next to a specific tract (‘ArenaAllocHere()’), we try to
extend a little harder (at least for ‘change_size’, perhaps not for
locality).

*note .arch.alloc.other;: f60. If no way can be found to allocate in the
current region, other regions used for this locus are considered in the
same way, to see if space can be found there.  [Or probably look at
other regions before trying to extend anything?]

*note .arch.alloc.recompute;: f61. When no region of this locus has
enough space for the request, or when otherwise required, region
placement is recomputed to find a new region for the request (which
might be the same region, after extension).

*note .arch.alloc.current;: f62. This region where the allocation was
placed then becomes the current region for this locus, except when the
request was exceptional, or when the region chosen was “bad” (see @@@@).

*note .arch.significant;: f5b. Significant changes to the parameters
affecting placement are deemed to have happened at certain client calls
and when the total allocation has changed substantially since the last
recomputation.  Such conditions set a flag that causes the next
allocation to recompute even if its current region is not full (possibly
second-guess the decision to recompute after some investigation of the
current state?).


File: MemoryPoolSystem.info,  Node: Deallocation,  Next: Region placement recomputation,  Prev: Allocation<3>,  Up: Architecture<7>

5.12.7.4 Deallocation
.....................

*note .arch.free;: f64. Deallocation simply updates the counters in the
region and the locus.  For some loci, it will make the region of the
deallocation the current region.  *note .arch.free.remove;: f65. If a
region becomes entirely empty, it is deleted (and the neighbors limits
might be adjusted).

     Note: This is quite tricky to get right.


File: MemoryPoolSystem.info,  Node: Region placement recomputation,  Prev: Deallocation,  Up: Architecture<7>

5.12.7.5 Region placement recomputation
.......................................

*note .arch.gap;: f67. When doing placement computations, we view the
arena as a sequence of alternating region cores and gaps (which can be
small, even zero-sized).  Initially, we’ll take the core of a region to
be the area between the high and low watermark, but in the future we
might be more flexible about that.

     Note: Edge determination is actually a worthwhile direction to
     explore.

*note .arch.reach;: f68. The gap between two cores could potentially end
up being allocated to either region, if they grow in that direction, or
one or neither, if they don’t.  The set of states that the region
assignment could reach by assigning the gaps to their neighbors is
called the reach of the current configuration.

*note .arch.placement.object;: f69. The object of the recomputation is
to find a configuration of regions that is not too far from the current
configuration and that keeps all the peaks inside its reach; if that is
not possible, keep the nearest ones in the reach and then minimize the
total distance from the rest.

*note .arch.placement.hypothetical;: f6a. The configurations that are
considered will include hypothetical placements for new regions for loci
that cannot fit in their existing regions at the peak.  This is
necessary to avoid choosing a bad alternative.

*note .arch.placement.interesting;: f6b. The computation will only
consider new regions of loci that are deemed interesting, that is, far
from their peak state.  This will reduce the computational burden and
avoid jittering near a peak.

     Note: Details missing.


File: MemoryPoolSystem.info,  Node: Implementation<20>,  Next: Notes<6>,  Prev: Architecture<7>,  Up: Locus manager

5.12.8 Implementation
---------------------

[missing]


File: MemoryPoolSystem.info,  Node: Notes<6>,  Prev: Implementation<20>,  Up: Locus manager

5.12.9 Notes
------------

*note .idea.change;: f6e. Even after the first segment, be prepared to
change your mind, if by the second segment a lot of new loci have been
created.

*note .distance;: f6f. If the current state is far from a peak, there’s
time to reassign regions and for free space to appear (in fact, under
the steady arena assumption, enough free space 'will' appear).

*note .clear-pool;: f70. Need to have a function to deallocate all
objects in a pool, so that ‘PoolDestroy()’ won’t have to be used for
that purpose.


File: MemoryPoolSystem.info,  Node: GC messages,  Next: Debugging features for client objects,  Prev: Locus manager,  Up: Old design

5.13 GC messages
================

* Menu:

* Introduction: Introduction<59>.
* Overview: Overview<20>.
* Introduction: Introduction<60>.
* Purpose: Purpose<3>.
* Names and parts::
* Lifecycle::
* Testing: Testing<7>.


File: MemoryPoolSystem.info,  Node: Introduction<59>,  Next: Overview<20>,  Up: GC messages

5.13.1 Introduction
-------------------

*note .intro;: f76. This document describes the design of the MPS
garbage collection messages.  For a guide to the MPS message system in
general, see design.mps.message.

*note .readership;: f77. Any MPS developer.


File: MemoryPoolSystem.info,  Node: Overview<20>,  Next: Introduction<60>,  Prev: Introduction<59>,  Up: GC messages

5.13.2 Overview
---------------

The MPS provides two types of GC messages:

   - *note mps_message_type_gc_start(): 22a.;

   - *note mps_message_type_gc(): 18d.

They are called “trace start” and “trace end” messages in this document
and in most MPS source code.


File: MemoryPoolSystem.info,  Node: Introduction<60>,  Next: Purpose<3>,  Prev: Overview<20>,  Up: GC messages

5.13.3 Introduction
-------------------

The MPS posts a trace start message (*note mps_message_type_gc_start():
22a.) near the start of every trace (but after calculating the condemned
set, so we can report how large it is).

The MPS posts a trace end message (*note mps_message_type_gc(): 18d.)
near the end of every trace.

These messages are extremely flexible: they can hold arbitrary
additional data simply by writing new accessor functions.  If there is
more data to report at either of these two events, then there is a good
argument for adding it into these existing messages.

     Note: In previous versions of this design document, there was a
     partial unimplemented design for an
     ‘mps_message_type_gc_generation()’ message.  This would not have
     been a good design, because managing and collating multiple
     messages is much more complex for both MPS and client than using a
     single message.  Richard Kistruck, 2008-12-19.


File: MemoryPoolSystem.info,  Node: Purpose<3>,  Next: Names and parts,  Prev: Introduction<60>,  Up: GC messages

5.13.4 Purpose
--------------

*note .purpose;: f7b. The purpose of these messages is to allow the
client program to be aware of GC activity, in order to:

   - adjust its own behaviour programmatically;

   - show or report GC activity in a custom way, such as an in-client
     display, in a log file, etc.

The main message content should be intelligible and helpful to
client-developers (with help from MPS staff if necessary).  There may be
extra content that is only meaningful to MPS staff, to help us diagnose
client problems.

While there is some overlap with the Diagnostic Feedback system
(design.mps.diag(1)) and the Telemetry system (design.mps.telemetry(2)),
the main contrasts are that these GC messages are present in release
builds, are stable from release to release, and are designed to be
parsed by the client program.

   ---------- Footnotes ----------

   (1) diag.html

   (2) telemetry.html


File: MemoryPoolSystem.info,  Node: Names and parts,  Next: Lifecycle,  Prev: Purpose<3>,  Up: GC messages

5.13.5 Names and parts
----------------------

Here’s a helpful list of the names used in the GC message system:

Implementation is mostly in the source file ‘traceanc.c’ (trace
ancillary).

Internal name                     “trace start”                      “trace end”
                                                                     
                                                                     
Internal type                     ‘TraceStartMessage’                ‘TraceMessage’
                                                                     
                                                                     
‘ArenaStruct’ member              ‘tsMessage[]’                      ‘tMessage’
                                                                     
                                                                     
Message type                      ‘MessageTypeGCSTART’               ‘MessageTypeGC’
                                                                     
                                                                     
External name                     ‘mps_message_type_gc_start’        ‘mps_message_type_gc’
                                                                     

     Note: The names of these messages are unconventional; they should
     properly be called “gc (or trace) 'begin'” and “gc (or trace)
     'end'”.  But it’s much too late to change them now.  Richard
     Kistruck, 2008-12-15.

Collectively, the trace-start and trace-end messages are called the
“trace id messages”, and they are managed by the functions
‘TraceIdMessagesCheck()’, ‘TraceIdMessagesCreate()’, and
‘TraceIdMessagesDestroy()’.

The currently supported message-field accessor methods are: *note
mps_message_gc_start_why(): 22d, *note mps_message_gc_live_size(): 22f,
*note mps_message_gc_condemned_size(): 230, and *note
mps_message_gc_not_condemned_size(): 231.  These are documented in the
Reference Manual.


File: MemoryPoolSystem.info,  Node: Lifecycle,  Next: Testing<7>,  Prev: Names and parts,  Up: GC messages

5.13.6 Lifecycle
----------------

*note .lifecycle;: f7f. for each trace id, pre-allocate a pair of
start/end messages by calling ‘ControlAlloc()’.  Then, when a trace runs
using that trace id, fill in and post these messages.  As soon as the
trace has posted both messages, immediately pre-allocate a new pair of
messages, which wait in readiness for the next trace to use that trace
id.

* Menu:

* Requirements: Requirements<36>.
* Storage::
* Creating and Posting::
* Getting and discarding::
* Final clearup::


File: MemoryPoolSystem.info,  Node: Requirements<36>,  Next: Storage,  Up: Lifecycle

5.13.6.1 Requirements
.....................

*note .req.no-start-alloc;: f81. Should avoid attempting to allocate
memory at trace start time.  *note .req.no-start-alloc.why;: f82. There
may be no free memory at trace start time.  Client would still like to
hear about collections in those circumstances.

*note .req.queue;: f83. Must support a client that enables, but does not
promptly retrieve, GC messages.  Messages that have not yet been
retrieved must remain queued, and the client must be able to retrieve
them later without loss.  It is not acceptable to stop issuing GC
messages for subsequent collections merely because messages from
previous collections have not yet been retrieved.  *note
.req.queue.why;: f84. This is because there is simply no reasonable way
for a client to guarantee that it always promptly collects GC messages.

*note .req.match;: f85. Start and end messages should always match up:
never post one of the messages but fail to post the matching one.

*note .req.match.why;: f86. This makes client code much simpler – it
does not have to handle mismatching messages.

*note .req.errors-not-direct;: f87. Errors (such as a ‘ControlAlloc()’
failure) cannot be reported directly to the client, because collections
often happen automatically, without an explicit client call to the MPS
interface.

*note .req.multi-trace;: f88. Up to ‘TraceLIMIT’ traces may be running,
and emitting start/end messages, simultaneously.

*note .req.early;: f89. Nice to tell client as much as possible about
the collection in the start message, if we can.

*note .req.similar;: f8a. Start and end messages are conceptually
similar – it is quite okay, and may be helpful to the client, for the
same datum (for example: the reason why the collection occurred) to be
present in both the start and end message.


File: MemoryPoolSystem.info,  Node: Storage,  Next: Creating and Posting,  Prev: Requirements<36>,  Up: Lifecycle

5.13.6.2 Storage
................

For each trace-id (*note .req.multi-trace: f88.) a pair (*note
.req.match: f85.) of start/end messages is dynamically allocated (*note
.req.queue: f83.) in advance (*note .req.no-start-alloc: f81.).
Messages are allocated in the control pool using ‘ControlAlloc()’.

     Note: Previous implementations of the trace start message used
     static allocation.  This does not satisfy *note .req.queue: f83.
     See also job001570(1).  Richard Kistruck, 2008-12-15.

Pointers to these messages are stored in ‘tsMessage[ti]’ and
‘tMessage[ti]’ arrays in the ‘ArenaStruct’.

     Note: We must not> keep the pre-allocated messages, or pointers to
     them, in ‘TraceStruct’: the memory for these structures is
     statically allocated, but the values in them are re-initialised by
     ‘TraceCreate()’ each time the trace id is used, so the
     ‘TraceStruct()’ is invalid (that is: to be treated as random
     uninitialised memory) when not being used by a trace.  See also
     job001989(2).  Richard Kistruck, 2008-12-15.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001570/

   (2) https://www.ravenbrook.com/project/mps/issue/job001989/


File: MemoryPoolSystem.info,  Node: Creating and Posting,  Next: Getting and discarding,  Prev: Storage,  Up: Lifecycle

5.13.6.3 Creating and Posting
.............................

In ‘ArenaCreate()’ we use ‘TRACE_SET_ITER’ to initialise the
‘tsMessage[ti]’ and ‘tMessage[ti]’ pointers to ‘NULL’, and then (when
the control pool is ready) ‘TRACE_SET_ITER’ calling
‘TraceIdMessagesCreate()’.  This performs the initial pre-allocation of
the trace start/end messages for each trace id.  Allocation failure is
not tolerated here: it makes ‘ArenaCreate()’ fail with an error code,
because the arena is deemed to be unreasonably small.

When a trace is running using trace id ‘ti’, it finds a pre-allocated
message via ‘tsMessage[ti]’ or ‘tMessage[ti]’ in the ‘ArenaStruct()’,
fills in and posts the message, and nulls-out the pointer.  (If the
pointer was null, no message is sent; see below.)  The message is now
reachable only from the arena message queue (but the control pool also
knows about it).

When the trace completes, it calls ‘TraceIdMessagesCreate()’ for its
trace id.  This performs the ongoing pre-allocation of the trace
start/end messages for the next use of this trace id.  The expectation
is that, after a trace has completed, some memory will have been
reclaimed, and the ‘ControlAlloc()’ will succeed.

But allocation failure here is permitted: if it happens, both the start
and the end messages are freed (if present).  This means that, for the
next collection using this trace id, neither a start nor an end message
will be sent (*note .req.match: f85.).  There is no direct way to report
this failure to the client (*note .req.errors-not-direct: f87.), so we
just increment the ‘droppedMessages’ counter in the ‘ArenaStruct’.  This
counter is available via the ‘MessagesDropped’ telemetry event.


File: MemoryPoolSystem.info,  Node: Getting and discarding,  Next: Final clearup,  Prev: Creating and Posting,  Up: Lifecycle

5.13.6.4 Getting and discarding
...............................

If the client has not enabled that message type, the message is
discarded immediately when posted, calling ‘ControlFree()’ and
reclaiming the memory.

If the client has enabled but never gets the message, it remains on the
message queue until ‘ArenaDestroy()’.  Theoretically these messages
could accumulate forever until they exhaust memory.  This is
intentional: the client should not enable a message type and then never
get it!

Otherwise, when the client gets a message, it is dropped from the arena
message queue: now only the client (and the control pool) hold
references to it.  The client must call *note mps_message_discard(): ee.
once it has finished using the message.  This calls ‘ControlFree()’ and
reclaims the memory.

If the client simply drops its reference, the memory will not be
reclaimed until ‘ArenaDestroy()’.  This is intentional: the control pool
is not garbage-collected.


File: MemoryPoolSystem.info,  Node: Final clearup,  Prev: Getting and discarding,  Up: Lifecycle

5.13.6.5 Final clearup
......................

Final clearup is performed at ‘ArenaDestroy()’, as follows:

   - Unused and unsent pre-allocated messages (one per trace id) are
     freed with ‘TRACE_SET_ITER’ calling ‘TraceIdMessagesDestroy()’
     which calls the message Delete functions (and thereby
     ‘ControlFree()’) on anything left in ‘tsMessage[ti]’ and
     ‘tMessage[ti]’.

   - Unretrieved messages are freed by emptying the arena message queue
     with *note MessageEmpty(): 554.

   - Retrieved but undiscarded messages are freed by destroying the
     control pool.


File: MemoryPoolSystem.info,  Node: Testing<7>,  Prev: Lifecycle,  Up: GC messages

5.13.7 Testing
--------------

The main test is “‘zmess.c’”.  See notes there.

Various other tests, including ‘amcss.c’, also collect and report *note
mps_message_type_gc(): 18d. and *note mps_message_type_gc_start(): 22a.

* Menu:

* Coverage::


File: MemoryPoolSystem.info,  Node: Coverage,  Up: Testing<7>

5.13.7.1 Coverage
.................

Current tests do not check:

   - The less common why-codes (reasons why a trace starts).  These
     should be added to ‘zmess.c’.


File: MemoryPoolSystem.info,  Node: Debugging features for client objects,  Next: AMC pool class,  Prev: GC messages,  Up: Old design

5.14 Debugging features for client objects
==========================================

* Menu:

* Introduction: Introduction<61>.
* Overview: Overview<21>.
* Requirements: Requirements<37>.
* Solution ideas: Solution ideas<2>.
* Architecture: Architecture<8>.
* Client interface::
* Examples::
* Implementation: Implementation<21>.


File: MemoryPoolSystem.info,  Node: Introduction<61>,  Next: Overview<21>,  Up: Debugging features for client objects

5.14.1 Introduction
-------------------

*note .intro;: f96. This is the design for all the various debugging
features that MPS clients (and sometimes MPS developers) can use to
discover what is happening to their objects and the memory space.

*note .readership;: f97. MPS developers.


File: MemoryPoolSystem.info,  Node: Overview<21>,  Next: Requirements<37>,  Prev: Introduction<61>,  Up: Debugging features for client objects

5.14.2 Overview
---------------

*note .over.fenceposts;: f99. In its current state, this document mostly
talks about fenceposts, straying a little into tagging where theses
features have an effect on each other.

     Note: There exist other documents that list other required
     features, and propose interfaces and implementations.  These will
     eventually be folded into this one.  Pekka P. Pirinen, 1998-09-10.


File: MemoryPoolSystem.info,  Node: Requirements<37>,  Next: Solution ideas<2>,  Prev: Overview<21>,  Up: Debugging features for client objects

5.14.3 Requirements
-------------------

*note .req.fencepost;: f9b. Try to detect overwrites and underwrites of
allocated blocks by adding fenceposts (source req.product.???  VC++,
req.epcore.fun.debug.support).  [TODO: Locate the relevant product
requirement.  RB 2023-02-23]

*note .req.fencepost.size;: f9c. The fenceposts should be at least 4
bytes on either side or 8 bytes if on one side only, with an adjustable
content (although VC++ only has 4 bytes with pattern 0xFDFDFDFD, having
unwisely combined the implementation with other debug features).

*note .req.fencepost.check;: f9d. There should be a function to check
all the fenceposts (source req.epcore.fun.debug.support).

*note .req.free-block;: f9e. Try to detect attempts to write and read
free blocks.

*note .req.walk;: f9f. There should be a way to map (“walk”) a user
function over all allocated objects (except PS VM objects), possibly
only in a separate debugging variety/mode (source
req.epcore.fun.debug.support).

*note .req.tag;: fa0. There should be a way to store at least a word of
user data (a “tag”, borrowing the SW term) with every object in
debugging mode, to be used in memory dumps (source req.product.???
VC++).  [TODO: Locate the relevant product requirement.  RB 2023-02-23]

*note .req.tag.walk;: fa1. The walking function (as required by *note
.req.walk: f9f.) should have access to this data (source
req.epcore.fun.debug.support).

*note .req.dump.aver;: fa2. It must be possible to perform a memory dump
after an ‘AVER()’ has fired.  Naturally, if the information required for
the dump has been corrupted, it will fail, as softly as possible (source
@@@@).

*note .req.portable;: fa3. Client code that uses these features must be
easily portable to all the supported platforms.  (Source: job003749(1).)

     Note: There are more requirements, especially about memory dumps
     and allocation locations.  Pekka P. Pirinen, 1998-09-10.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job003749/


File: MemoryPoolSystem.info,  Node: Solution ideas<2>,  Next: Architecture<8>,  Prev: Requirements<37>,  Up: Debugging features for client objects

5.14.4 Solution ideas
---------------------

*note .note.assumptions;: fa5. I’ve tried not to assume anything about
the coincidence of manual/automatic, formatted/unformatted, and
ap/mps_alloc.  I think those questions deserve to be decided on their
own merits.  instead of being constrained by a debug feature.

*note .fence.content.repeat;: fa6. The content of a fencepost could be
specified as a byte/word which used repeatedly to fill the fencepost.

*note .fence.content.template;: fa7. The content could be given as a
template which is of the right size and is simply copied onto the
fencepost.

*note .fence.content.template.repeat;: fa8. The content could be given
as a template which is copied repeatedly until the fencepost is full.
(This would avoid the need to specify different templates on different
architectures, and so help meet *note .req.portable: fa3.)

*note .fence.walk;: fa9. *note .req.fencepost.check: f9d. requires the
ability to find all the allocated objects.  In formatted pools, this is
not a problem.  In unformatted pools, we could use the walker.  It’s a
feasible strategy to bet that any pool that might have to support
fenceposting will also have a walking requirement.

*note .fence.tag;: faa. Fenceposting also needs to keep track which
objects have fenceposts.  unless we manage to do them all.  It would be
easiest to put this in the tags.

*note .fence.check.object;: fab. A function to check the fenceposts on a
given object would be nice.

*note .fence.ap;: fac. AP’s could support fenceposting transparently by
having a mode where *note mps_reserve(): b0. always goes out-of-line and
fills in the fenceposts (the pool’s *note BufferFill(): 7a2. method
isn’t involved).  This would leave the MPS with more freedom of
implementation, especially when combined with some of the other ideas.
We think doing a function call for every allocation is not too bad for
debugging.

*note .fence.outside-ap;: fad. We could also let the client insert their
own fenceposts outside the MPS allocation mechanism.  Even if
fenceposting were done like this, we’d still want it to be an MPS
feature, so we’d offer sample C macros for adding the size of the
fencepost and filling in the fencepost pattern.  Possibly something like
this (while we could still store the parameters in the pool or
allocation point, there seems little point in doing so in this case, and
having them as explicit parameters to the macros allows the client to
specify constants to gain efficiency):

     #define mps_add_fencepost(size, fp_size)
     #define mps_fill_fenceposts(obj, size, fp_size, fp_pattern)

The client would need to supply their own fencepost checking function,
obviously, but again we could offer one that matches the sample macros.

*note .fence.tail-only;: fae. In automatic pools, the presence of a
fencepost at the head of the allocated block results in the object
reference being an internal pointer.  This means that the format or the
pool would need to know about fenceposting and convert between
references and pointers.  This would slow down the critical path when
fenceposting is used.  This can be ameliorated by putting a fencepost at
the tail of the block only: this obviates the internal pointer problem
and could provide almost the same degree of checking (provided the size
was twice as large), especially in copying pools, where there are
normally no gaps between allocated blocks.  In addition to the
inescapable effects on allocation and freeing (including copying and
reclaim thereunder), only scanning would have to know about fenceposts.

*note .fence.tail-only.under;: faf. Walking over all the objects in the
pool would be necessary to detect underwrites, as one couldn’t be sure
that there is a fencepost before any given object (or where it’s located
exactly).  If the pool were doing the checking, it could be sure: it
would know about alignments and it could put fenceposts in padding
objects (free blocks will have them because they were once allocated) so
there’d be one on either side of any object (except at the head of a
segment, which is not a major problem, and could be fixed by adding a
padding object at the beginning of every segment).  This requires some
cleverness to avoid splinters smaller than the fencepost size, but it
can be done.

*note .fence.wrapper;: fb0. On formatted pools, fenceposting could be
implemented by “wrapping” the client-supplied format at creation time.
The wrapper can handle the conversion from the fenceposted object and
back.  This will be invisible to the client and gives the added benefit
that the wrapper can validate fenceposts on every format operation,
should it desire.  That is, the pool would see the fenceposts as part of
the client object, but the client would only see its object; the format
wrapper would translate between the two.  Note that hiding the
fenceposts from scan methods, which are required to take a contiguous
range of objects, is a bit complicated.

*note .fence.client-format;: fb1. The MPS would supply such a wrapper,
but clients could also be allowed to write their own fenceposted formats
(provided they coordinate with allocation, see below).  This would make
scanning fenceposted segments more efficient.

*note .fence.wrapper.variable;: fb2. Furthermore, you could create
different classes of fencepost within a pool, because the fencepost
itself could have a variable format.  For instance, you might choose to
have the fencepost be minimal (one to two words) for small objects, and
more detailed/complex for large objects (imagining that large objects
are likely vector-ish and subject to overruns).  You could get really
fancy and have the fencepost class keyed to the object class (for
example, different allocation points create different classes of
fenceposting).

*note .fence.wrapper.alloc;: fb3. Even with a wrapped format, allocation
and freeing would still have know about the fenceposts.  If allocation
points are used, either MPS-side (*note .fence.ap: fac.) or client-side
(*note .fence.outside-ap: fad.) fenceposting could be used, with the
obvious modifications.

*note .fence.wrapper.alloc.format;: fb4. We could add three format
methods, to adjust the pointer and the size for alloc and free, to put
down the fenceposts during alloc, and to check them; to avoid slowing
down all allocation, this would require some MOPping to make the format
class affect the choice of the alloc and free methods (see
mail.pekka.1998-06-11.18-18(1)).

*note .fence.wrapper.alloc.size;: fb5. We could just communicate the
size of the fenceposts between the format and the allocation routines,
but then you couldn’t use variable fenceposts (*note
.fence.wrapper.variable: fb2.).

     Note: All this applies to copying and reclaim in a straight-forward
     manner, I think.

*note .fence.pool.wrapper;: fb6. Pools can be wrapped as well.  This
could be a natural way to represent/implement the fenceposting changes
to the Alloc and Free methods.  [@@@@alignment]

*note .fence.pool.new-class;: fb7. We could simply offer a debugging
version of each pool class (e.g., ‘mps_pool_class_mv_debug()’).  As we
have seen, debugging features have synergies which make it advantageous
to have a coordinated implementation, so splitting them up would not
just complicate the client interface, it would also be an implementation
problem; we can turn features on or off with pool init parameters.

*note .fence.pool.abstract;: fb8. We could simply use pool init
parameters only to control all debugging features (optargs would be
useful here).  While there might be subclasses and wrappers internally,
the client would only see a single pool class; in the internal view,
this would be an abstract class, and the parameters would determine
which concrete class actually gets instantiated.

*note .tag.out-of-line;: fb9. It would be nice if tags were stored
out-of-line, so they can be used to study allocation patterns and
fragmentation behaviours.  Such an implementation of tagging could also
easily be shared among several pools.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/06/11/18-18/0.txt


File: MemoryPoolSystem.info,  Node: Architecture<8>,  Next: Client interface,  Prev: Solution ideas<2>,  Up: Debugging features for client objects

5.14.5 Architecture
-------------------

*note .pool;: fbb. The implementation is at the pool level, because
pools manage allocated objects.  A lot of the code will be generic,
naturally, but the data structures and the control interfaces attach to
pools.  In particular, clients will be able to use tagging and
fenceposting separately on each pool.

*note .fence.size;: fbc. Having fenceposts of adjustable size and
pattern is useful.  Restricting the size to an integral multiple of the
[pool or format?]  alignment would simplify the implementation but
breaks *note .req.portable: fa3.

*note .fence.template;: fbd. We use templates (*note
.fence.content.template: fa7.) to fill in the fenceposts, but we do not
give any guarantees about the location of the fenceposts.  This leaves
us the opportunity to do tail-only fenceposting, if we choose.

*note .fence.slop;: fbe. [see impl.c.dbgpool.FenceAlloc @@@@]

*note .fence.check.free;: fbf. We check the fenceposts when freeing an
object.

*note .unified-walk;: fc0. Combine the walking and tagging requirements
(*note .req.tag.walk: fa1. and @@@@) into a generic facility for walking
and tagging objects with just one interface and one name: tagging.  Also
combine the existing formatted object walker into this metaphor, but
allowing the format and tag parameters of the step function be optional.

     Note: This part has not been implemented yet Pekka P. Pirinen,
     1998-09-10.

*note .init;: fc1. It simplifies the implementation of both tagging and
fenceposting if they are always on, so that we don’t have to keep track
of which objects have been fenceposted and which have not, and don’t
have to have three kinds of tags: for user data, for fenceposting, and
for both.  So we determine this at pool init time (and let fenceposting
turn on tagging, if necessary).

*note .pool-parameters;: fc2. Fencepost templates and tag formats are
passed in as pool parameters.

*note .modularity;: fc3. While a combined generic implementation of tags
and fenceposts is provided, it is structured so that each part of it
could be implemented by a pool-specific mechanism with a minimum of new
protocol.

     Note: This will be improved, when we figure out formatted pools –
     they don’t need tags for fenceposting.

*note .out-of-space;: fc4. If there’s no room for tags, we just fail to
allocate the tag.  We free the block allocated for the object and fail
the allocation, so that the client gets a chance to do whatever
low-memory actions they might want to do.

This breaks the one-to-one relationship between tags and objects, so
some checks cannot be made, but we do count the “lost” tags.

     Note: Need to hash out how to do fenceposting in formatted pools.


File: MemoryPoolSystem.info,  Node: Client interface,  Next: Examples,  Prev: Architecture<8>,  Up: Debugging features for client objects

5.14.6 Client interface
-----------------------

*note .interface.fenceposting.check;: fc6. *note
mps_pool_check_fenceposts(): 280. is a function to check all fenceposts
in a pool (‘AVER()’ if a problem is found)

     Note: From here on, these are tentative and incomplete.

 -- C Function: *note mps_res_t: 14d. mps_fmt_fencepost_wrap (mps_fmt_t
          *format_return, mps_arena_t arena, mps_fmt_t format, ...)

*note .interface.fenceposting.format;: fc8. A function to wrap a format
(class) to provide fenceposting.

 -- C Type: typedef void (*mps_fmt_adjust_fencepost_t)(size_t *size_io)

*note .interface.fenceposting.adjust;: fca. A format method to adjust
size of a block about to be allocated to allow for fenceposts.

 -- C Type: typedef void (*mps_fmt_put_fencepost_t)(*note mps_addr_t:
          11d. *addr_io, size_t size)

*note .interface.fenceposting.add;: fcc. A format method to add a
fencepost around a block about to be allocated.  The ‘NULL’ method adds
a tail fencepost.

 -- C Type: typedef *note mps_bool_t: 129.
          (*mps_fmt_check_fenceposts_t)(*note mps_addr_t: 11d.)

*note .interface.fenceposting.checker;: fce. A format method to check
the fenceposts around an object.  The ‘NULL’ method checks tails.

 -- C Function: *note mps_res_t: 14d. mps_alloc_dbg (mps_addr_t*,
          mps_pool_t, size_t, ...)

 -- C Function: *note mps_res_t: 14d. mps_alloc_dbg_v (mps_addr_t*,
          mps_pool_t, size_t, va_list)

*note .interface.tags.alloc;: fd1. Two functions to extend the existing
*note mps_alloc(): ad. (request.???.???  proposes to remove the varargs)
[TODO: Locate the relevant Harlequin request.  RB 2023-02-23]

 -- C Type: typedef void (*mps_objects_step_t)(*note mps_addr_t: 11d.
          addr, size_t size, *note mps_fmt_t: 141. format, *note
          mps_pool_t: 1b1. pool, void *tag_data, void *p)

*note .interface.tags.walker.type;: fd3. Type of walker function for
*note mps_pool_walk(): 1a6. and ‘mps_arena_walk()’.

*note .interface.tags.walker;: fd4. Functions to walk all the allocated
objects in an arena (only client pools in this case), ‘format’ and
‘tag_data’ can be ‘NULL’ (‘tag_data’ really wants to be ‘void *’, not
*note mps_addr_t: 11d, because it’s stored together with the internal
tag data in an MPS internal pool)


File: MemoryPoolSystem.info,  Node: Examples,  Next: Implementation<21>,  Prev: Client interface,  Up: Debugging features for client objects

5.14.7 Examples
---------------

*note .example.debug-alloc;: fd6.

     #define MPS_ALLOC_DBG(res_io, addr_io, pool, size)
       MPS_BEGIN
         static mps_tag_A_s _ts = { __FILE__, __LINE__ };

         *res_io = mps_alloc(addr_io, pool, size, _ts_)
       MPS_END


File: MemoryPoolSystem.info,  Node: Implementation<21>,  Prev: Examples,  Up: Debugging features for client objects

5.14.8 Implementation
---------------------

*note .new-pool;: fd8. The client interface to control fenceposting
consists of the new classes ‘mps_pool_class_mv_debug()’,
‘mps_pool_class_epdl_debug()’, and ‘mps_pool_class_epdr_debug()’, and
their new init parameter of type *note mps_pool_debug_option_s: 143.

     Note: This is a temporary solution, to get it out without writing
     lots of new interface.  Pekka P. Pirinen, 1998-09-10.

*note .new-pool.impl;: fd9. The debug pools are implemented using the
“class wrapper” ‘EnsureDebugClass()’, which produces a subclass with
modified ‘init’, ‘finish’, ‘alloc’, and ‘free’ methods.  These methods
are implemented in the generic debug class code (‘impl.c.dbgpool’), and
are basically wrappers around the superclass methods (invoked through
the ‘pool->class->super’ field).  To find the data stored in the class
for the debugging features, they use the ‘debugMixin’ method provided by
the subclass.  So to make a debug subclass, three things should be
provided: a structure definition of the instance containing a
‘PoolDebugMixinStruct’, a pool class function that uses
‘EnsureDebugClass()’, and a ‘debugMixin’ method that locates the
‘PoolDebugMixinStruct’ within an instance.

*note .tags.splay;: fda. The tags are stored in a splay tree of tags
allocated from a subsidiary MFS pool.  The client needs to specify the
(maximum) size of the client data in a tag, so that the pool can be
created.

     Note: Lots more should be said, eventually.  Pekka P. Pirinen,
     1998-09-10.


File: MemoryPoolSystem.info,  Node: AMC pool class,  Next: AMS pool class,  Prev: Debugging features for client objects,  Up: Old design

5.15 AMC pool class
===================

* Menu:

* Guide Introduction::
* Guide: Guide<2>.
* Initial design::


File: MemoryPoolSystem.info,  Node: Guide Introduction,  Next: Guide<2>,  Up: AMC pool class

5.15.1 Guide Introduction
-------------------------

*note .guide.intro;: fe0. This document contains a guide (*note .guide:
fe1.) to the MPS AMC pool class, followed by the historical initial
design (*note .initial-design: fe2.).

*note .guide.readership;: fe3. Any MPS developer.


File: MemoryPoolSystem.info,  Node: Guide<2>,  Next: Initial design,  Prev: Guide Introduction,  Up: AMC pool class

5.15.2 Guide
------------

*note .guide;: fe1. The AMC pool class is a general-purpose automatic
(collecting) pool class.  It is intended for most client objects.  AMC
is “Automatic, Mostly Copying”: it preserves objects by copying, except
when an ambiguous reference ‘nails’ the object in place.  It is
generational.  Chain: specify capacity and mortality of generations 0 to
'N' − 1.  Survivors from generation 'N' − 1 get promoted into an
arena-wide “top” generation (often anachronistically called the
“dynamic” generation, which was the term on the Lisp Machine).

* Menu:

* Segment states::
* Pads::
* Placement pads are okay::
* Retained pads could be a problem::
* Small, medium, and large segments: Small medium and large segments.
* The LSP payoff calculation::
* Retained pages::
* Feedback about retained pages::


File: MemoryPoolSystem.info,  Node: Segment states,  Next: Pads,  Up: Guide<2>

5.15.2.1 Segment states
.......................

*note .seg.state;: fe6. AMC segments are in one of three states:
“mobile”, “boarded”, or “stuck”.

*note .seg.state.mobile;: fe7. Segments are normally 'mobile': all
objects on the seg are un-nailed, and thus may be preserved by copying.

*note .seg.state.boarded;: fe8. An ambiguous reference to any address
within an segment makes that segment 'boarded': a nailboard is allocated
to record ambiguous references (“nails”), but un-nailed objects on the
segment are still preserved by copying.

*note .seg.state.stuck;: fe9. Stuck segments only occur in emergency
tracing: a discovery fix to an object in a mobile segment is recorded in
the only non-allocating way available: by making the entire segment
'stuck'.


File: MemoryPoolSystem.info,  Node: Pads,  Next: Placement pads are okay,  Prev: Segment states,  Up: Guide<2>

5.15.2.2 Pads
.............

(See job001809(1) and job001811(2), and mps/branch/2009-03-31/padding.)

*note .pad;: feb. A pad is logically a trivial client object.  Pads are
created by the MPS asking the client’s format code to create them, to
fill up a space in a segment.  Thereafter, the pad appears to the MPS as
a normal client object (that is: the MPS cannot distinguish a pad from a
client object).

*note .pad.reason;: fec. AMC creates pads for three reasons: buffer
empty fragment (BEF), large segment padding (LSP), and non-mobile
reclaim (NMR). (Large segment pads were new with job001811(3).)

*note .pad.reason.bef;: fed. Buffer empty fragment (BEF) pads are made
by *note amcSegBufferEmpty(): fee. whenever it detaches a non-empty
buffer from an AMC segment.  Buffer detachment is most often caused
because the buffer is too small for the current buffer reserve request
(which may be either a client requested or a forwarding allocation).
Detachment may happen for other reasons, such as trace flip.

*note .pad.reason.lsp;: fef. Large segment padding (LSP) pads are made
by *note AMCBufferFill(): 903. when the requested fill size is “large”
(see *note The LSP payoff calculation: ff0. below).  *note
AMCBufferFill(): 903. fills the buffer to exactly the size requested by
the current buffer reserve operation; that is: it does not round up to
the whole segment size.  This prevents subsequent small objects being
placed in the same segment as a single very large object.  If the buffer
fill size is less than the segment size, *note AMCBufferFill(): 903.
fills any remainder with a large segment pad.

*note .pad.reason.nmr;: ff1. Non-mobile reclaim (NMR) pads are made by
‘amcSegReclaimNailed()’, when performing reclaim on a non-mobile (that
is, either boarded or stuck) segment:

The more common NMR scenario is reclaim of a boarded segment after a
non-emergency trace.  Ambiguous references into the segment are recorded
as nails.  Subsequent exact references to a nailed object do nothing
further, but exact refs that do not match a nail cause preserve-by-copy
and leave a forwarding object.  Unreachable objects are not touched
during the scan+fix part of the trace.  On reclaim, only nailed objects
need to be preserved; others (namely forwarding pointers and unreachable
objects) are replaced by an NMR pad.  (Note that a BEF or LSP pad
appears to be an unreachable object, and is therefore overwritten by an
NMR pad).

The less common NMR scenario is after emergency tracing.  Boarded
segments still occur; they may have nailed objects from ambiguous
references, forwarding objects from pre-emergency exact fixes, nailed
objects from mid-emergency exact fixes, and unpreserved objects; reclaim
is as in the non-emergency case.  Stuck segments may have forwarding
objects from pre-emergency exact fixes, objects from mid-emergency
fixes, and unreachable objects – but the latter two are not
distinguishable because there is no nailboard.  On reclaim, all objects
except forwarding pointers are preserved; each forwarding object is
replaced by an NMR pad.

If ‘amcSegReclaimNailed()’ finds no objects to be preserved then it
calls ‘SegFree()’ (new with job001809(4)).

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001809/

   (2) https://www.ravenbrook.com/project/mps/issue/job001811/

   (3) https://www.ravenbrook.com/project/mps/issue/job001811/

   (4) https://www.ravenbrook.com/project/mps/issue/job001809/


File: MemoryPoolSystem.info,  Node: Placement pads are okay,  Next: Retained pads could be a problem,  Prev: Pads,  Up: Guide<2>

5.15.2.3 Placement pads are okay
................................

Placement pads are the BEF and LSP pads created in “to-space” when
placing objects into segments.  This wasted space is an expected
space-cost of AMC’s naive (but time-efficient) approach to placement of
objects into segments.  This is normally not a severe problem.  (The
worst case is a client that always requests ‘amc->extendBy + 1’ byte
objects: this has an overhead of nearly ‘ArenaGrainSize() /
amc->extendBy’).


File: MemoryPoolSystem.info,  Node: Retained pads could be a problem,  Next: Small medium and large segments,  Prev: Placement pads are okay,  Up: Guide<2>

5.15.2.4 Retained pads could be a problem
.........................................

Retained pads are the NMR pads stuck in “from-space”: non-mobile
segments that were condemned but have preserved-in-place objects cannot
be freed by ‘amcSegReclaimNailed()’.  The space around the preserved
objects is filled with NMR pads.

In the worst case, retained pads could waste an enormous amount of
space!  A small (one-byte) object could retain a multi-page segment for
as long as the ambiguous reference persists; that is: indefinitely.
Imagine a 256-page (1 MiB) segment containing a very large object
followed by a handful of small objects.  An ambiguous reference to one
of the small objects will unfortunately cause the entire 256-page
segment to be retained, mostly as an NMR pad; this is a massive overhead
of wasted space.

AMC mitigates this worst-case behaviour, by treating large segments
specially.


File: MemoryPoolSystem.info,  Node: Small medium and large segments,  Next: The LSP payoff calculation,  Prev: Retained pads could be a problem,  Up: Guide<2>

5.15.2.5 Small, medium, and large segments
..........................................

AMC categorises segments as 'small' (up to ‘amc->extendBy’), 'medium'
(larger than small but smaller than large), or 'large' (‘amc->largeSize’
or more):

     size = SegSize(seg);
     if(size < amc->extendBy) {
       /* small */
     } else if(size < amc->largeSize) {
       /* medium */
     } else {
       /* large */
     }</code></pre></blockquote>

‘amc->extendBy’ defaults to 4096 (rounded up to the arena alignment),
and is settable by using ‘MPS_KEY_EXTEND_BY’ keyword argument.
‘amc->largeSize’ is currently 32768 – see *note The LSP payoff
calculation: ff0. below.

AMC might treat “Large” segments specially, in two ways:

   - *note .large.single-reserve;: ff5. A large segment is only used for
     a single (large) buffer reserve request; the remainder of the
     segment (if any) is immediately padded with an LSP pad.

   - *note .large.lsp-no-retain;: ff6. Nails to such an LSP pad do not
     cause ‘amcSegReclaimNailed()’ to retain the segment.

*note .large.single-reserve: ff5. is implemented.  See job001811(1).

*note .large.lsp-no-retain: ff6. is 'not' currently implemented.

The point of *note .large.lsp-no-retain: ff6. would be to avoid
retention of the (large) segment when there is a spurious ambiguous
reference to the LSP pad at the end of the segment.  Such an ambiguous
reference might happen naturally and repeatably if the preceding large
object is an array, the array is accessed by an ambiguous element
pointer (for example, on the stack), and the element pointer ends up
pointing just off the end of the large object (as is normal for
sequential element access in C) and remains with that value for a while.
(Such an ambiguous reference could also occur by chance, for example, by
coincidence with an ‘int’ or ‘float’, or when the stack grows to include
old unerased values).

Implementing *note .large.lsp-no-retain: ff6. is a little tricky.  A pad
is indistinguishable from a client object, so AMC has no direct way to
detect, and safely ignore, the final LSP object in the seg.  If AMC
could 'guarantee' that the single buffer reserve (*note
.large.single-reserve: ff5.) is only used for a single 'object', then
‘amcSegReclaimNailed()’ could honour a nail at the start of a large seg
and ignore all others; this would be extremely simple to implement.  But
AMC cannot guarantee this, because in the MPS Allocation Point Protocol
the client is permitted to make a large buffer reserve and then fill it
with many small objects.  In such a case, AMC must honour all nails (if
the buffer reserve request was an exact multiple of the arena grain
size), or all nails except to the last object (if there was a remainder
filled with an LSP pad).  Because an LSP pad cannot be distinguished
from a client object, and the requested allocation size is not recorded,
AMC cannot distinguish these two conditions at reclaim time.  Therefore
AMC must record whether or not the last object in the seg is a pad, in
order to ignore nails to it.  This could be done by adding a flag to
‘AMCSegStruct’.  (This can be done without increasing the structure
size, by making the ‘Bool new’ field smaller than its current 32 bits.)

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001811/


File: MemoryPoolSystem.info,  Node: The LSP payoff calculation,  Next: Retained pages,  Prev: Small medium and large segments,  Up: Guide<2>

5.15.2.6 The LSP payoff calculation
...................................

The LSP fix for job001811(1) treats large segments differently.  Without
it, after allocating a very large object (in a new very large multi-page
segment), MPS would happily place subsequent small objects in any
remaining space at the end of the segment.  This would risk pathological
fragmentation: if these small objects were systematically preserved by
ambiguous refs, enormous NMR pads would be retained along with them.

The payoff calculation is a bit like deciding whether or not to purchase
insurance.  For single-page and medium-sized segments, we go ahead and
use the remaining space for subsequent small objects.  This is
equivalent to choosing 'not' to purchase insurance.  If the small
objects were to be preserved by ambiguous refs, the retained NMR pads
would be big, but not massive.  We expect such ambiguous refs to be
uncommon, so we choose to live with this slight risk of bad
fragmentation.  The benefit is that the remaining space is used.

For large segments, we decide that the risk of using the remainder is
just too great, and the benefit too small, so we throw it away as an LSP
pad.  This is equivalent to purchasing insurance: we choose to pay a
known small cost every time, to avoid risking an occasional disaster.

To decide what size of segment counts as “large”, we must decide how
much uninsured risk we can tolerate, versus how much insurance cost we
can tolerate.  The likelihood of ambiguous references retaining objects
is entirely dependent on client behaviour.  However, as a sufficient
“one size fits all” policy, I (RHSK 2009-09-14) have judged that
segments smaller than eight pages long do not need to be treated as
large: the insurance cost to “play safe” would be considerable (wasting
up to one page of remainder per seven pages of allocation), and the
fragmentation overhead risk is not that great (at most eight times worse
than the unavoidable minimum).  So ‘AMC_LARGE_SIZE_DEFAULT’ is defined
as 32768 in config.h.  As long as the assumption that most segments are
not ambiguously referenced remains correct, I expect this policy will be
satisfactory.

To verify that this threshold is acceptable for a given client,
poolamc.c calculates metrics; see *note Feedback about retained pages:
ff7. below.  If this one-size-fits-all approach is not satisfactory,
‘amc->largeSize’ is a client-tunable parameter which defaults to
‘AMC_LARGE_SIZE_DEFAULT’.  It can be tuned by passing an
‘MPS_KEY_LARGE_SIZE’ keyword argument to *note mps_pool_create_k(): 166.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001811/


File: MemoryPoolSystem.info,  Node: Retained pages,  Next: Feedback about retained pages,  Prev: The LSP payoff calculation,  Up: Guide<2>

5.15.2.7 Retained pages
.......................

The reasons why a segment and its pages might be retained are:

  1. ambiguous reference to first-obj: unavoidable page retention (only
     the mutator can reduce this, if they so wish, by nulling out ambig
     references);

  2. ambiguous reference to rest-obj: tuning MPS LSP policy could
     mitigate this, reducing the likelihood of rest-objs being
     co-located with large first-objs;

  3. ambiguous reference to final pad: implementing *note
     .large.lsp-no-retain: ff6. could mitigate this;

  4. ambiguous reference to other (NMR) pad: hard to mitigate, as pads
     are indistinguishable from client objects;

  5. emergency trace;

  6. non-object-aligned ambiguous ref: fixed by job001809(1);

  7. other reason (for example, buffered at flip): not expected to be a
     problem.

This list puts the reasons that are more “obvious” to the client
programmer first, and the more obscure reasons last.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001809/


File: MemoryPoolSystem.info,  Node: Feedback about retained pages,  Prev: Retained pages,  Up: Guide<2>

5.15.2.8 Feedback about retained pages
......................................

(New with job001811(1)).  AMC now accumulates counts of pages condemned
and retained during a trace, in categories according to size and reason
for retention, and emits this via the ‘AMCTraceEnd’ telemetry event.
See comments on the ‘PageRetStruct’ in ‘poolamc.c’.  These page-based
metrics are not as precise as actually counting the size of objects, but
they require much less intrusive code to implement, and should be
sufficient to assess whether AMC’s page retention policies and behaviour
are acceptable.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job001811/


File: MemoryPoolSystem.info,  Node: Initial design,  Prev: Guide<2>,  Up: AMC pool class

5.15.3 Initial design
---------------------

*note .initial-design;: fe2. This section contains the original design
for the AMC Pool Class.

* Menu:

* Introduction: Introduction<62>.
* Overview: Overview<22>.
* Definitions: Definitions<10>.
* Segments::
* Fixing and nailing::
* Emergency tracing::
* Buffers::
* Types: Types<9>.
* Generations::
* Ramps::
* Headers: Headers<2>.
* Old and aging notes below here::


File: MemoryPoolSystem.info,  Node: Introduction<62>,  Next: Overview<22>,  Up: Initial design

5.15.3.1 Introduction
.....................

*note .intro;: ffb. This is the design of the AMC Pool Class.  AMC
stands for Automatic Mostly-Copying.  This design is highly fragmentory
and some may even be sufficiently old to be misleading.

*note .readership;: ffc. The intended readership is any MPS developer.


File: MemoryPoolSystem.info,  Node: Overview<22>,  Next: Definitions<10>,  Prev: Introduction<62>,  Up: Initial design

5.15.3.2 Overview
.................

*note .overview;: ffe. This class is intended to be the main pool class
used by Harlequin Dylan.  It provides garbage collection of objects
(hence “automatic”).  It uses generational copying algorithms, but with
some facility for handling small numbers of ambiguous references.
Ambiguous references prevent the pool from copying objects (hence
“mostly copying”).  It provides incremental collection.

     Note: A lot of this design is awesomely old.  David Jones,
     1998-02-04.


File: MemoryPoolSystem.info,  Node: Definitions<10>,  Next: Segments,  Prev: Overview<22>,  Up: Initial design

5.15.3.3 Definitions
....................

*note .def.grain;: 1000. Grain.  An quantity of memory which is both
aligned to the pool’s alignment and equal to the pool’s alignment in
size.  That is, the smallest amount of memory worth talking about.


File: MemoryPoolSystem.info,  Node: Segments,  Next: Fixing and nailing,  Prev: Definitions<10>,  Up: Initial design

5.15.3.4 Segments
.................

*note .seg.class;: 1002. AMC allocates segments of class ‘AMCSegClass’,
which is a subclass of ‘MutatorSegClass’ (see
design.mps.seg.over.hierarchy.mutatorseg(1)).

*note .seg.gen;: 1003. AMC organizes the segments it manages into
generations.

*note .seg.gen.map;: 1004. Every segment is in exactly one generation.

*note .seg.gen.ind;: 1005. The segment’s ‘gen’ field indicates which
generation (that the segment is in) (an ‘AMCGenStruct’ see blah below).

*note .seg.gen.get;: 1006. The map from segment to generation is
implemented by ‘amcSegGen()’ which deals with all this.

   ---------- Footnotes ----------

   (1) seg.html#design.mps.seg.over.hierarchy.mutatorseg


File: MemoryPoolSystem.info,  Node: Fixing and nailing,  Next: Emergency tracing,  Prev: Segments,  Up: Initial design

5.15.3.5 Fixing and nailing
...........................

     Note: This section contains placeholders for design rather than
     design really.  David Jones, 1998-02-04.

*note .nailboard;: 1008. AMC uses a nailboard structure for recording
ambiguous references to segments.  See design.mps.nailboard(1).

*note .nailboard.create;: 1009. A nailboard is allocated dynamically
whenever a segment becomes newly ambiguously referenced.  This table is
used by subsequent scans and reclaims in order to work out which objects
were ambiguously referenced.

*note .nailboard.destroy;: 100a. The nailboatrd is deallocated during
reclaim.

*note .nailboard.emergency;: 100b. During emergency tracing two things
relating to nailboards happen that don’t normally:

  1. *note .nailboard.emergency.nonew;: 100c. Nailboards aren’t
     allocated when we have new ambiguous references to segments.

     *note .nailboard.emergency.nonew.justify;: 100d. We could try and
     allocate a nailboard, but we’re in emergency mode so short of
     memory so it’s unlikely to succeed, and there would be additional
     code for yet another error path which complicates things.

  2. *note .nailboard.emergency.exact;: 100e. nailboards are used to
     record exact references in order to avoid copying the objects.

     *note .nailboard.hyper-conservative;: 100f. Not creating new
     nailboards (*note .nailboard.emergency.nonew: 100c. above) means
     that when we have a new reference to a segment during emergency
     tracing then we nail the entire segment and preserve everything in
     place.

*note .fix.nail.states;: 1010. Partition the segment states into four
sets:

  1. white segment and not nailed (and has no nailboard);

  2. white segment and nailed and has no nailboard;

  3. white segment and nailed and has nailboard;

  4. the rest.

*note .fix.nail.why;: 1011. A segment is recorded as being nailed when
either there is an ambiguous reference to it, or there is an exact
reference to it and the object couldn’t be copied off the segment
(because there wasn’t enough memory to allocate the copy).  In either of
these cases reclaim cannot simply destroy the segment (usually the
segment will not be destroyed because it will have live objects on it,
though see *note .nailboard.limitations.middle: 1012. below).  If the
segment is nailed then we might be using a nailboard to mark objects on
the segment.  However, we cannot guarantee that being nailed implies a
nailboard, because we might not be able to allocate the nailboard.
Hence all these states actually occur in practice.

*note .fix.nail.distinguish;: 1013. The nailed bits in the segment
descriptor (‘SegStruct’) are used to record the set of traces for which
a segment has nailed objects.

*note .nailboard.limitations.single;: 1014. Just having a single
nailboard per segment prevents traces from improving on the findings of
each other: a later trace could find that a nailed object is no longer
nailed or even dead.  Until the nailboard is discarded, that is.

*note .nailboard.limitations.middle;: 1012. An ambiguous reference to a
segment that does not point into any object in that segment will cause
that segment to survive even though there are no surviving objects on
it.

   ---------- Footnotes ----------

   (1) nailboard.html


File: MemoryPoolSystem.info,  Node: Emergency tracing,  Next: Buffers,  Prev: Fixing and nailing,  Up: Initial design

5.15.3.6 Emergency tracing
..........................

*note .emergency.fix;: 1016. ‘amcSegFixEmergency()’ is at the core of
AMC’s emergency tracing policy (unsurprisingly).  ‘amcSegFixEmergency()’
chooses exactly one of three options:

  1. use the existing nailboard structure to record the fix;

  2. preserve and nail the segment in its entirety;

  3. snapout an exact (or high rank) pointer to a broken heart to the
     broken heart’s forwarding pointer.

If the rank of the reference is ‘RankAMBIG’ then it either does (1) or
(2) depending on whether there is an existing nailboard or not.
Otherwise (the rank is exact or higher) if there is a broken heart it is
used to snapout the pointer.  Otherwise it is as for an ‘RankAMBIG’
reference: we either do (1) or (2).

*note .emergency.scan;: 1017. This is basically as before, the only
complication is that when scanning a nailed segment we may need to do
multiple passes, as ‘amcSegFixEmergency()’ may introduce new marks into
the nail board.


File: MemoryPoolSystem.info,  Node: Buffers,  Next: Types<9>,  Prev: Emergency tracing,  Up: Initial design

5.15.3.7 Buffers
................

*note .buffer.class;: 1019. AMC uses buffer of class ‘AMCBufClass’ (a
subclass of SegBufClass).

*note .buffer.gen;: 101a. Each buffer allocates into exactly one
generation.

*note .buffer.field.gen;: 101b. ‘AMCBuf’ buffer contain a gen field
which points to the generation that the buffer allocates into.

*note .buffer.fill.gen;: 101c. *note AMCBufferFill(): 903. uses the
generation (obtained from the ‘gen’ field) to initialise the segment’s
‘segTypeP’ field which is how segments get allocated in that generation.

*note .buffer.condemn;: 101d. We condemn buffered segments, but not the
contents of the buffers themselves, because we can’t reclaim uncommitted
buffers (see design.mps.buffer(1) for details).  If the segment has a
forwarding buffer on it, we detach it.

     Note: Why?  Forwarding buffers are detached because they used to
     cause objects on the same segment to not get condemned, hence
     caused retention of garbage.  Now that we condemn the non-buffered
     portion of buffered segments this is probably unnecessary.  David
     Jones, 1998-06-01.

     But it’s probably more efficient than keeping the buffer on the
     segment, because then the other stuff gets nailed – Pekka P.
     Pirinen, 1998-07-10.

If the segment has a mutator buffer on it, we nail the buffer.  If the
buffer cannot be nailed, we give up condemning, since nailing the whole
segment would make it survive anyway.  The scan methods skip over
buffers and fix methods don’t do anything to things that have already
been nailed, so the buffer is effectively black.

   ---------- Footnotes ----------

   (1) buffer.html


File: MemoryPoolSystem.info,  Node: Types<9>,  Next: Generations,  Prev: Buffers,  Up: Initial design

5.15.3.8 Types
..............

*note .struct;: 101f. ‘AMCStruct’ is the pool class AMC instance
structure.

*note .struct.pool;: 1020. Like other pool class instances, it contains
a ‘PoolStruct’ containing the generic pool fields.

*note .struct.format;: 1021. The ‘format’ field points to a ‘Format’
structure describing the object format of objects allocated in the pool.
The field is initialized by ‘AMCInit()’ from a parameter, and thereafter
it is not changed until the pool is destroyed.

     Note: Actually the format field is in the generic ‘PoolStruct’
     these days.  David Jones, 1998-09-21.

     Note: There are lots more fields here.


File: MemoryPoolSystem.info,  Node: Generations,  Next: Ramps,  Prev: Types<9>,  Up: Initial design

5.15.3.9 Generations
....................

*note .gen;: 1023. Generations partition the segments that a pool
manages (see *note .seg.gen.map: 1004. above).

*note .gen.collect;: 1024. Generations are more or less the units of
condemnation in AMC. And also the granularity for forwarding (when
copying objects during a collection): all the objects which are copied
out of a generation use the same forwarding buffer for allocating the
new copies, and a forwarding buffer results in allocation in exactly one
generation.

*note .gen.rep;: 1025. Generations are represented using an
‘AMCGenStruct’ structure.

*note .gen.create;: 1026. All the generations are created when the pool
is created (during ‘AMCInitComm()’).

*note .gen.manage.ring;: 1027. An AMC’s generations are kept on a ring
attached to the ‘AMCStruct’ (the ‘genRing’ field).

*note .gen.manage.array;: 1028. They are also kept in an array which is
allocated when the pool is created and attached to the ‘AMCStruct’ (the
gens field holds the number of generations, the ‘gen’ field points to an
array of ‘AMCGen’).

     Note: it seems to me that we could probably get rid of the ring.
     David Jones, 1998-09-22.

*note .gen.number;: 1029. There are ‘AMCTopGen + 2’ generations in
total.  “normal” generations numbered from 0 to ‘AMCTopGen’ inclusive
and an extra “ramp” generation (see *note .gen.ramp: 102a. below).

*note .gen.forward;: 102b. Each generation has an associated forwarding
buffer (stored in the ‘forward’ field of ‘AMCGen’).  This is the buffer
that is used to forward objects out of this generation.  When a
generation is created in ‘AMCGenCreate()’, its forwarding buffer has a
null ‘p’ field, indicating that the forwarding buffer has no generation
to allocate in.  The collector will assert out (in *note
AMCBufferFill(): 903. where it checks that ‘buffer->p’ is an ‘AMCGen’)
if you try to forward an object out of such a generation.

*note .gen.forward.setup;: 102c. All the generation’s forwarding
buffer’s are associated with generations when the pool is created (just
after the generations are created in ‘AMCInitComm()’).


File: MemoryPoolSystem.info,  Node: Ramps,  Next: Headers<2>,  Prev: Generations,  Up: Initial design

5.15.3.10 Ramps
...............

*note .ramp;: 102e. Ramps usefully implement the begin/end *note
mps_alloc_pattern_ramp(): 26f. interface.

*note .gen.ramp;: 102a. To implement ramping (request.dylan.170423(1)),
AMC uses a special “ramping mode”, where promotions are redirected.  One
generation is designated the “ramp generation” (‘amc->rampGen’ in the
code).

*note .gen.ramp.ordinary;: 102f. Ordinarily, that is whilst not ramping,
objects are promoted into the ramp generation from younger generations
and are promoted out to older generations.  The generation that the ramp
generation ordinarily promotes into is designated the “after-ramp
generation” (‘amc->afterRampGen’).

*note .gen.ramp.particular;: 1030. the ramp generation is the second
oldest generation and the after-ramp generation is the oldest
generation.

*note .gen.ramp.possible;: 1031. In alternative designs it might be
possible to make the ramp generation a special generation that is only
promoted into during ramping, however, this is not done.

*note .gen.ramp.ramping;: 1032. The ramp generation is promoted into
itself during ramping mode;

*note .gen.ramp.after;: 1033. after this mode ends, the ramp generation
is promoted into the after-ramp generation as usual.

*note .gen.ramp.after.once;: 1034. Care is taken to ensure that there is
at least one collection where stuff is promoted from the ramp generation
to the after-ramp generation even if ramping mode is immediately
re-entered.

*note .ramp.mode;: 1035. This behaviour is controlled in a slightly
convoluted manner by a state machine.  The rampMode field of the pool
forms an important part of the state of the machine.

There are five states: OUTSIDE, BEGIN, RAMPING, FINISH, and COLLECTING.
These appear in the code as ‘RampOUTSIDE’ and so on.

*note .ramp.state.cycle.usual;: 1036. The usual progression of states is
a cycle: OUTSIDE → BEGIN → RAMPING → FINISH → COLLECTING → OUTSIDE.

*note .ramp.count;: 1037. The pool just counts the number of APs that
have begun ramp mode (and not ended).  No state changes occur unless
this count goes from 0 to 1 (starting the first ramp) or from 1 to 0
(leaving the last ramp).  In other words, all nested ramps are ignored
(see code in ‘AMCRampBegin()’ and ‘AMCRampEnd()’).

*note .ramp.state.invariant.count;: 1038. In the OUTSIDE state the count
must be zero.  In the BEGIN and RAMPING states the count must be greater
than zero.  In the FINISH and COLLECTING states the count is not
constrained.

*note .ramp.state.invariant.forward;: 1039. When in OUTSIDE, BEGIN, or
COLLECTING, the ramp generation forwards to the after-ramp generation.
When in RAMPING or FINISH, the ramp generation forwards to itself.

*note .ramp.outside;: 103a. The pool is initially in the OUTSIDE state.
The only transition away from the OUTSIDE state is to the BEGIN state,
when a ramp is entered.

*note .ramp.begin;: 103b. When the count goes up from zero, the state
moves from COLLECTING or OUTSIDE to BEGIN.

*note .ramp.begin.leave;: 103c. We can leave the BEGIN state to either
the OUTSIDE or the RAMPING state.

*note .ramp.begin.leave.outside;: 103d. We go to OUTSIDE if the count
drops to 0 before a collection starts.  This shortcuts the usual cycle
of states for small enough ramps.

*note .ramp.begin.leave.ramping;: 103e. We enter the RAMPING state if a
collection starts that condemns the ramp generation (pedantically when a
new GC begins, and a segment in the ramp generation is condemned, we
leave the BEGIN state, see ‘amcSegWhiten()’).  At this point we switch
the ramp generation to forward to itself (*note .gen.ramp.ramping:
1032.).

*note .ramp.ramping.leave;: 103f. We leave the RAMPING state and go to
the FINISH state when the ramp count goes back to zero.  Thus, the
FINISH state indicates that we have started collecting the ramp
generation while inside a ramp which we have subsequently finished.

*note .ramp.finish.remain;: 1040. We remain in the FINISH state until we
next start to collect the ramp generation (condemn it), regardless of
entering or leaving any ramps.  This ensures that the ramp generation
will be collected to the after-ramp generation at least once.

*note .ramp.finish.leave;: 1041. When we next condemn the ramp
generation, we move to the COLLECTING state.  At this point the
forwarding generations are switched back so that the ramp generation
promotes into the after-ramp generation on this collection.

*note .ramp.collecting.leave;: 1042. We leave the COLLECTING state when
the GC enters reclaim (specifically, when a segment in the ramp
generation is reclaimed), or when we begin another ramp.  Ordinarily we
enter the OUTSIDE state, but if the client has started a ramp then we go
directly to the BEGIN state.

.ramp.collect-all There used to be two flavours of ramps: the normal one
and the collect-all flavour that triggered a full GC after the ramp end.
This was a hack for producing certain Dylan statistics, and no longer
has any effect (the flag is passed to ‘AMCRampBegin()’, but ignored
there).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170423


File: MemoryPoolSystem.info,  Node: Headers<2>,  Next: Old and aging notes below here,  Prev: Ramps,  Up: Initial design

5.15.3.11 Headers
.................

*note .header;: 1045. AMC supports a fixed-size header on objects, with
the client pointers pointing after the header, rather than the base of
the memory block.  See format documentation for details of the
interface.

*note .header.client;: 1046. The code mostly deals in client pointers,
only computing the base and limit of a block when these are needed (such
as when an object is copied).  In several places, the code gets a block
of some sort (a segment or a buffer) and creates a client pointer by
adding the header size (‘pool->format->headerSize’).


File: MemoryPoolSystem.info,  Node: Old and aging notes below here,  Prev: Headers<2>,  Up: Initial design

5.15.3.12 Old and aging notes below here
........................................

 -- C Function: void AMCFinish (Pool pool)

*note .finish.forward;: 1049. If the pool is being destroyed it is OK to
destroy the forwarding buffers, as the condemned set is about to
disappear.

 -- C Function: void amcSegBufferEmpty (Seg seg, Buffer buffer)

*note .flush;: 104a. Free the unused part of the buffer to the segment.

*note .flush.pad;: 104b. The segment is padded out with a dummy object
so that it appears full.

*note .flush.expose;: 104c. The segment needs exposing before writing
the padding object onto it.  If the segment is being used for forwarding
it might already be exposed, in this case the segment attached to it
must be covered when it leaves the buffer.  See *note .fill.expose:
104d.

*note .flush.cover;: 104e. The segment needs covering whether it was
being used for forwarding or not.  See *note .flush.expose: 104c.

 -- C Function: *note Res: 55f. AMCBufferFill (Addr *baseReturn, Addr
          *limitReturn, Pool pool, Buffer buffer, Size size)

*note .fill;: 104f. Reserve was called on an allocation buffer which was
reset, or there wasn’t enough room left in the buffer.  Allocate a group
for the new object and attach it to the buffer.

*note .fill.expose;: 104d. If the buffer is being used for forwarding it
may be exposed, in which case the group attached to it should be
exposed.  See *note .flush.cover: 104e.

 -- C Function: *note Res: 55f. amcSegFix (Seg seg, ScanState ss, Ref
          *refIO)

*note .fix;: 1050. Fix a reference to an AMC segment.

Ambiguous references lock down an entire segment by removing it from
old-space and also marking it grey for future scanning.

Exact, final, and weak references are merged because the action for an
already forwarded object is the same in each case.  After that situation
is checked for, the code diverges.

Weak references are either snapped out or replaced with ‘ss->weakSplat’
as appropriate.

Exact and final references cause the referenced object to be copied to
new-space and the old copy to be forwarded (broken-heart installed) so
that future references are fixed up to point at the new copy.

*note .fix.exact.expose;: 1051. In order to allocate the new copy the
forwarding buffer must be exposed.  This might be done more efficiently
outside the entire scan, since it’s likely to happen a lot.

*note .fix.exact.grey;: 1052. The new copy must be at least as grey as
the old as it may have been grey for some other collection.

 -- C Function: *note Res: 55f. amcSegScan (Bool *totalReturn, Seg seg,
          ScanState ss1)

*note .scan;: 1053. Searches for a group which is grey for the trace and
scans it.  If there aren’t any, it sets the finished flag to true.

 -- C Function: void amcSegReclaim (Seg seg, Trace trace)

*note .reclaim;: 1055. After a trace, destroy any groups which are still
condemned for the trace, because they must be dead.

*note .reclaim.grey;: 1056. Note that this might delete things which are
grey for other collections.  This is OK, because we have conclusively
proved that they are dead – the other collection must have assumed they
were alive.  There might be a problem with the accounting of grey
groups, however.

*note .reclaim.buf;: 1057. If a condemned group still has a buffer
attached, we can’t destroy it, even though we know that there are no
live objects there.  Even the object the mutator is allocating is dead,
because the buffer is tripped.


File: MemoryPoolSystem.info,  Node: AMS pool class,  Next: AWL pool class,  Prev: AMC pool class,  Up: Old design

5.16 AMS pool class
===================

* Menu:

* Introduction: Introduction<63>.
* Overview: Overview<23>.
* Requirements: Requirements<38>.
* Architecture: Architecture<9>.
* Implementation: Implementation<22>.
* Testing: Testing<8>.
* Notes: Notes<7>.


File: MemoryPoolSystem.info,  Node: Introduction<63>,  Next: Overview<23>,  Up: AMS pool class

5.16.1 Introduction
-------------------

*note .intro;: 105d. This is the design of the AMS pool class.

*note .readership;: 105e. MM developers.

*note .source;: 105f. design.mps.buffer(1), design.mps.trace(2),
design.mps.scan(3), design.mps.action and design.mps.pool(4) [none of
these were actually used – pekka 1998-04-21].  No requirements doc [we
need a req.mps that captures the commonalities between the products –
pekka 1998-01-27].

   ---------- Footnotes ----------

   (1) buffer.html

   (2) trace.html

   (3) scan.html

   (4) pool.html


File: MemoryPoolSystem.info,  Node: Overview<23>,  Next: Requirements<38>,  Prev: Introduction<63>,  Up: AMS pool class

5.16.2 Overview
---------------

*note .overview;: 1062. This is the design of the AMS (Automatic
Mark-and-Sweep) pool class.  The AMS pool is a proof-of-concept design
for a mark-sweep pool in the MPS. It’s not meant to be efficient, but it
could serve as a model for an implementation of a more advanced pool
(such as EPVM).


File: MemoryPoolSystem.info,  Node: Requirements<38>,  Next: Architecture<9>,  Prev: Overview<23>,  Up: AMS pool class

5.16.3 Requirements
-------------------

*note .req.mark-sweep;: 1064. The pool must use a mark-and-sweep GC
algorithm.

*note .req.colour;: 1065. The colour representation should be as
efficient as possible.

*note .req.incremental;: 1066. The pool must support incremental GC.

*note .req.ambiguous;: 1067. The pool must support ambiguous references
to objects in it (but ambiguous references into the middle of an object
do not preserve the object).

*note .req.format;: 1068. The pool must be formatted, for generality.

*note .req.correct;: 1069. The design and the implementation should be
simple enough to be seen to be correct.

*note .req.simple;: 106a. Features not related to mark-and-sweep GC
should initially be implemented as simply as possible, in order to save
development effort.

*note .not-req.grey;: 106b. We haven’t figured out how buffers ought to
work with a grey mutator, so we use *note .req.correct: 1069. to allow
us to design a pool that doesn’t work in that phase.  This is acceptable
as long as we haven’t actually implemented grey mutator collection.


File: MemoryPoolSystem.info,  Node: Architecture<9>,  Next: Implementation<22>,  Prev: Requirements<38>,  Up: AMS pool class

5.16.4 Architecture
-------------------

* Menu:

* Subclassing::
* Allocation: Allocation<4>.
* Colours::
* Scanning: Scanning<2>.


File: MemoryPoolSystem.info,  Node: Subclassing,  Next: Allocation<4>,  Up: Architecture<9>

5.16.4.1 Subclassing
....................

*note .subclass;: 106e. Since we expect to have many mark-and-sweep
pools, we build in some protocol for subclasses to modify various
aspects of the behaviour.  Notably there’s a subclassable segment class,
and a protocol for performing iteration.


File: MemoryPoolSystem.info,  Node: Allocation<4>,  Next: Colours,  Prev: Subclassing,  Up: Architecture<9>

5.16.4.2 Allocation
...................

*note .align;: 1070. We divide the segments in grains, each the size of
the format alignment.  *note .alloc-bit-table;: 1071. We keep track of
allocated grains using a bit table.  This allows a simple implementation
of allocation and freeing using the bit table operators, satisfying
*note .req.simple: 106a, and can simplify the GC routines.  Eventually,
this should use some sophisticated allocation technique suitable for
non-moving automatic pools.

*note .buffer;: 1072. We use buffered allocation, satisfying *note
.req.incremental: 1066.  The AMC buffer technique is reused, although it
is not suitable for non-moving pools, but req.simple allows us to do
that for now.

*note .extend;: 1073. If there’s no space in any existing segment, a new
segment is allocated.  The actual class is allowed to decide the size of
the new segment.

*note .no-alloc;: 1074. Do not support ‘PoolAlloc()’, because we can’t
support one-phase allocation for a scannable pool (unless we disallow
incremental collection).  For exact details, see design.mps.buffer(1).

*note .no-free;: 1075. Do not support ‘PoolFree()’, because automatic
pools don’t need explicit free and having it encourages clients to use
it (and therefore to have dangling pointers, double frees, and other
memory management errors.)

   ---------- Footnotes ----------

   (1) buffer.html


File: MemoryPoolSystem.info,  Node: Colours,  Next: Scanning<2>,  Prev: Allocation<4>,  Up: Architecture<9>

5.16.4.3 Colours
................

*note .colour;: 1077. Objects in a segment which is 'not' condemned (for
some trace) take their colour (for this trace) from the segment.

*note .colour.object;: 1078. Since we need to implement a non-copying
GC, we keep track of the colour of each object in a condemned segment
separately.  For this, we use bit tables with a bit for each grain.
This format is fast to access, has better locality than mark bits in the
objects themselves, and allows cheap interoperation with the allocation
bit table.

*note .colour.encoding;: 1079. As to the details, we follow
analysis.non-moving-colour(3), implementing both the alloc-white sharing
option described in
analysis.non-moving-colour.constraint.reclaim.white-free-bit and the
vanilla three-table option, because the former cannot work with interior
pointers.  However, the colour encoding in both is the same.

*note .ambiguous.middle;: 107a. We will allow ambiguous references into
the middle of an object (as required by *note .req.ambiguous: 1067.),
using the trick in analysis.non-moving-colour.interior.ambiguous-only to
speed up scanning.

*note .interior-pointer;: 107b. Note that non-ambiguous interior
pointers are outlawed.

*note .colour.alloc;: 107c. Objects are allocated black.  This is the
most efficient alternative for traces in the black mutator phase, and
*note .not-req.grey: 106b. means that’s sufficient.

     Note: Some day, we need to think about allocating grey or white
     during the grey mutator phase.


File: MemoryPoolSystem.info,  Node: Scanning<2>,  Prev: Colours,  Up: Architecture<9>

5.16.4.4 Scanning
.................

*note .scan.segment;: 107e. The tracer protocol requires (for segment
barrier hits) that there is a method for scanning a segment and turning
all grey objects on it black.  This cannot be achieved with a single
sequential sweep over the segment, since objects that the sweep has
already passed may become grey as later objects are scanned.

*note .scan.graph;: 107f. For a non-moving GC, it is more efficient to
trace along the reference graph than segment by segment.  It also allows
passing type information from fix to scan.  Currently, the tracer
doesn’t offer this option when it’s polling for work.

*note .scan.stack;: 1080. Tracing along the reference graph cannot be
done by recursive descent, because we can’t guarantee that the stack
won’t overflow.  We can, however, maintain an explicit stack of things
to trace, and fall back on iterative methods (*note .scan.iter: 1081.)
when it overflows and can’t be extended.

*note .scan.iter;: 1081. As discussed in *note .scan.segment: 107e, when
scanning a segment, we need to ensure that there are no grey objects in
the segment when the scan method returns.  We can do this by iterating a
sequential scan over the segment until nothing is grey (see *note
.marked.scan: 1082. for details).

*note .scan.iter.only;: 1083. Some iterative method is needed as a
fallback for the more advanced methods, and as this is the simplest way
of implementing the current tracer protocol, we will start by
implementing it as the only scanning method.

*note .scan.buffer;: 1084. We do not scan between ScanLimit and Limit of
a buffer (see *note .iteration.buffer: 1085.), as usual.

     Note: design.mps.buffer(1) should explain why this works, but
     doesn’t.  Pekka P. Pirinen, 1998-02-11.

*note .fix.to-black;: 1086. When fixing a reference to a white object,
if the segment does not refer to the white set, the object cannot refer
to the white set, and can therefore be marked as black immediately
(rather than grey).

   ---------- Footnotes ----------

   (1) buffer.html


File: MemoryPoolSystem.info,  Node: Implementation<22>,  Next: Testing<8>,  Prev: Architecture<9>,  Up: AMS pool class

5.16.5 Implementation
---------------------

* Menu:

* Colour::
* Iteration: Iteration<2>.
* Scanning Algorithm::
* Allocation: Allocation<5>.
* Initialization::
* Condemnation::
* Reclaim::
* Segment merging and splitting::


File: MemoryPoolSystem.info,  Node: Colour,  Next: Iteration<2>,  Up: Implementation<22>

5.16.5.1 Colour
...............

*note .colour.determine;: 1089. Following the plan in *note .colour:
1077, if ‘SegWhite(seg)’ includes the trace, the colour of an object is
given by the bit tables.  Otherwise if ‘SegGrey(seg)’ includes the
trace, all the objects are grey.  Otherwise all the objects are black.

*note .colour.bits;: 108a. As we only have searches for runs of zero
bits, we use two bit tables, the non-grey and non-white tables, but this
is hidden beneath a layer of macros talking about grey and white in
positive terms.

*note .colour.single;: 108b. We have only implemented a single set of
mark and scan tables, so we can only condemn a segment for one trace at
a time.  This is checked for in condemnation.  If we want to do
overlapping white sets, each trace needs its own set of tables.

*note .colour.check;: 108c. The grey-and-non-white state is illegal, and
free objects must be white as explained in
analysis.non-moving-colour.constraint.reclaim.


File: MemoryPoolSystem.info,  Node: Iteration<2>,  Next: Scanning Algorithm,  Prev: Colour,  Up: Implementation<22>

5.16.5.2 Iteration
..................

*note .iteration;: 108e. Scan, reclaim and other operations need to
iterate over all objects in a segment.  We abstract this into a single
iteration function, even though we no longer use it for reclaiming and
rarely for scanning.

*note .iteration.buffer;: 1085. Iteration skips directly from ScanLimit
to Limit of a buffer.  This is because this area may contain
partially-initialized and uninitialized data, which cannot be processed.
Since the iteration skips the buffer, callers need to take the
appropriate action, if any, on it.

     Note: ScanLimit is used for reasons which are not documented in
     design.mps.buffer(1).

   ---------- Footnotes ----------

   (1) buffer.html


File: MemoryPoolSystem.info,  Node: Scanning Algorithm,  Next: Allocation<5>,  Prev: Iteration<2>,  Up: Implementation<22>

5.16.5.3 Scanning Algorithm
...........................

*note .marked;: 1090. Each segment has a ‘marksChanged’ flag, indicating
whether anything in it has been made grey since the last scan iteration
(*note .scan.iter: 1081.) started.  This flag only concerns the colour
of objects with respect to the trace for which the segment is condemned,
as this is the only trace for which objects in the segment are being
made grey by fixing.  Note that this flag doesn’t imply that there are
grey objects in the segment, because the grey objects might have been
subsequently scanned and blackened.

*note .marked.fix;: 1091. The ‘marksChanged’ flag is set ‘TRUE’ by
‘amsSegFix()’ when an object is made grey.

*note .marked.scan;: 1082. ‘amsSegScan()’ must blacken all grey objects
on the segment, so it must iterate over the segment until all grey
objects have been seen.  Scanning an object in the segment might grey
another one (*note .marked.fix: 1091.), so the scanner iterates until
this flag is ‘FALSE’, setting it to ‘FALSE’ before each scan.  It is
safe to scan the segment even if it contains nothing grey.

*note .marked.scan.fail;: 1092. If the format scanner returns failure
(see protocol.mps.scanning), we abort the scan in the middle of a
segment.  So in this case the marksChanged flag is set back to TRUE,
because we may not have blackened all grey objects.

     Note: Is that the best reference for the format scanner?

*note .marked.unused;: 1093. The ‘marksChanged’ flag is meaningless
unless the segment is condemned.  We make it ‘FALSE’ in these
circumstances.

*note .marked.condemn;: 1094. Condemnation makes all objects in a
segment either black or white, leaving nothing grey, so it doesn’t need
to set the ‘marksChanged’ flag which must already be ‘FALSE’.

*note .marked.reclaim;: 1095. When a segment is reclaimed, it can
contain nothing marked as grey, so the ‘marksChanged’ flag must already
be ‘FALSE’.

*note .marked.blacken;: 1096. When the tracer decides not to scan, but
to call ‘SegBlacken()’, we know that any greyness can be removed.
‘amsSegBlacken()’ does this and resets the ‘marksChanged’ flag, if it
finds that the segment has been condemned.

*note .marked.clever;: 1097. AMS could be clever about not setting the
‘marksChanged’ flag, if the fixed object is ahead of the current scan
pointer.  It could also keep low- and high-water marks of grey objects,
but we don’t need to implement these improvements at first.


File: MemoryPoolSystem.info,  Node: Allocation<5>,  Next: Initialization,  Prev: Scanning Algorithm,  Up: Implementation<22>

5.16.5.4 Allocation
...................

*note .buffer-init;: 1099. We take one init arg to set the Rank on the
buffer, just to see how it’s done.

*note .no-bit;: 109a. As an optimization, we won’t use the alloc bit
table until the first reclaim on the segment.  Before that, we just keep
a high-water mark.

*note .fill;: 109b. ‘AMSBufferFill()’ takes the simplest approach: it
iterates over the segments in the pool, looking for one which can be
used to refill the buffer.

*note .fill.colour;: 109c. The objects allocated from the new buffer
must be black for all traces (*note .colour.alloc: 107c.), so putting it
on a black segment (meaning one where neither ‘SegWhite(seg)’ nor
‘SegGrey(seg)’ include the trace, see *note .colour.determine: 1089.) is
obviously OK. White segments (where ‘SegWhite(seg)’ includes the trace)
are also fine, as we can use the colour tables to make it black.  At
first glance, it seems we can’t put it on a segment that is grey but not
white for some trace (one where ‘SegWhite(seg)’ doesn’t include the
trace, but ‘SegGrey(seg)’ does), because the new objects would become
grey as the buffer’s ScanLimit advanced.  However, in many
configurations, the mutator would hit a barrier as soon as it started
initializing the object, which would flip the buffer.  In fact, the
current (2002-01) implementation of buffers assumes buffers are black,
so they’d better.

*note .fill.colour.reclaim;: 109d. In fact, putting a buffer on a
condemned segment will screw up the accounting in ‘amsSegReclaim()’, so
it’s disallowed.

*note .fill.slow;: 109e. ‘AMSBufferFill()’ gets progressively slower as
more segments fill up, as it laboriously checks whether the buffer can
be refilled from each segment, by inspecting the allocation bit map.
This is helped a bit by keeping count of free grains in each segment,
but it still spends a lot of time iterating over all the full segments
checking the free size.  Obviously, this can be much improved (we could
keep track of the largest free block in the segment and in the pool, or
we could keep the segments in some more efficient structure, or we could
have a real free list structure).

*note .fill.extend;: 109f. If there’s no space in any existing segment,
the ‘segSize’ method is called to decide the size of the new segment to
allocate.  If that fails, the code tries to allocate a segment that’s
just large enough to satisfy the request.

*note .empty;: 10a0. ‘amsSegBufferEmpty()’ makes the unused space free,
since there’s no reason not to.  We have to adjust the colour tables as
well, since these grains were black and now they need to be white (or at
least encoded -G and W).

*note .reclaim.empty.buffer;: 10a1. Segments which after reclaim only
contain a buffer could be destroyed by trapping the buffer, but there’s
no point to this.


File: MemoryPoolSystem.info,  Node: Initialization,  Next: Condemnation,  Prev: Allocation<5>,  Up: Implementation<22>

5.16.5.5 Initialization
.......................

*note .init;: 10a3. The initialization method ‘AMSInit()’ takes three
additional arguments: the format of objects allocated in the pool, the
chain that controls GC timing, and a flag for supporting ambiguous
references.

*note .init.share;: 10a4. If support for ambiguity is required, the
‘shareAllocTable’ flag is reset to indicate the pool uses three separate
bit tables, otherwise it is set and the pool shares a table for
non-white and alloc (see *note .colour.encoding: 1079.).

*note .init.align;: 10a5. The pool alignment is set equal to the format
alignment (see design.mps.align).

*note .init.internal;: 10a6. Subclasses call ‘AMSInitInternal()’ to
avoid the problems of sharing ‘va_list’ and emitting a superfluous
‘PoolInitAMS’ event.


File: MemoryPoolSystem.info,  Node: Condemnation,  Next: Reclaim,  Prev: Initialization,  Up: Implementation<22>

5.16.5.6 Condemnation
.....................

*note .condemn.buffer;: 10a8. Buffers are not condemned, instead they
are coloured black, to make sure that the objects allocated will be
black, following *note .colour.alloc: 107c. (or, if you wish, because
buffers are ignored like free space, so need the same encoding).


File: MemoryPoolSystem.info,  Node: Reclaim,  Next: Segment merging and splitting,  Prev: Condemnation,  Up: Implementation<22>

5.16.5.7 Reclaim
................

*note .reclaim;: 10aa. Reclaim uses either of
analysis.non-moving-colour.constraint.reclaim.white-free-bit (just reuse
the non-white table as the alloc table) or
analysis.non-moving-colour.constraint.reclaim.free-bit (copy it),
depending on the ‘shareAllocTable’ flag (as set by *note .init.share:
10a4.).  However, bit table still has to be iterated over to count the
free grains.  Also, in a debug pool, each white block has to be
splatted.


File: MemoryPoolSystem.info,  Node: Segment merging and splitting,  Prev: Reclaim,  Up: Implementation<22>

5.16.5.8 Segment merging and splitting
......................................

*note .split-merge;: 10ac. We provide methods for splitting and merging
AMS segments.  The pool implementation doesn’t cause segments to be
split or merged – but a subclass might want to do this (see *note
.stress.split-merge: 10ad.).  The methods serve as an example of how to
implement this facility.

*note .split-merge.constrain;: 10ae. There are some additional
constraints on what segments may be split or merged:

   - *note .split-merge.constrain.align;: 10af. Segments may only be
     split or merged at an address which is aligned to the pool
     alignment as well as to the arena grain size.

     *note .split-merge.constrain.align.justify;: 10b0. This constraint
     is implied by the design of allocation and colour tables, which
     cannot represent segments starting at unaligned addresses.  The
     constraint only arises if the pool alignment is larger than the
     arena alignment.  There’s no requirement to split segments at
     unaligned addresses.

   - *note .split-merge.constrain.empty;: 10b1. The higher segment must
     be empty.  That is, the higher segment passed to *note SegMerge():
     10b2. must be empty, and the higher segment returned by *note
     SegSplit(): 10b3. must be empty.

     *note .split-merge.constrain.empty.justify;: 10b4. This constraint
     makes the code significantly simpler.  There’s no requirement for a
     more complex solution at the moment (as the purpose is primarily
     pedagogic).

*note .split-merge.fail;: 10b5. The split and merge methods are not
proper anti-methods for each other (see
design.mps.seg.split-merge.fail.anti.no(1)).  Methods will not reverse
the side-effects of their counterparts if the allocation of the colour
and allocation bit tables should fail.  Client methods which over-ride
split and merge should not be written in such a way that they might
detect failure after calling the next method, unless they have reason to
know that the bit table allocations will not fail.

   ---------- Footnotes ----------

   (1) seg.html#design.mps.seg.split-merge.fail.anti.no


File: MemoryPoolSystem.info,  Node: Testing<8>,  Next: Notes<7>,  Prev: Implementation<22>,  Up: AMS pool class

5.16.6 Testing
--------------

*note .stress;: 10b8. There’s a stress test, MMsrc!amsss.c, that does
800 kB of allocation, enough for about three GCs.  It uses a modified
Dylan format, and checks for corruption by the GC. Both ambiguous and
exact roots are tested.

*note .stress.split-merge;: 10ad. There’s also a stress test for segment
splitting and merging, MMsrc!segsmss.c.  This is similar to amsss.c –
but it defines a subclass of AMS, and causes segments to be split and
merged.  Both buffered and non-buffered segments are split / merged.


File: MemoryPoolSystem.info,  Node: Notes<7>,  Prev: Testing<8>,  Up: AMS pool class

5.16.7 Notes
------------

*note .addr-index.slow;: 10ba. Translating from an address to and from a
grain index in a segment uses macros such as ‘AMS_INDEX’ and
‘AMS_INDEX_ADDR’.  These are slow because they call ‘SegBase()’ on every
translation – we could cache that.

*note .grey-mutator;: 10bb. To enforce the restriction set in *note
.not-req.grey: 106b. we check that all the traces are flipped in
‘amsSegScan()’.  It would be good to check in ‘amsSegFix()’ as well, but
we can’t do that, because it’s called during the flip, and we can’t tell
the difference between the flip and the grey mutator phases with the
current tracer interface.


File: MemoryPoolSystem.info,  Node: AWL pool class,  Next: LO pool class,  Prev: AMS pool class,  Up: Old design

5.17 AWL pool class
===================

* Menu:

* Introduction: Introduction<64>.
* Requirements: Requirements<39>.
* Definitions: Definitions<11>.
* Overview: Overview<24>.
* Interface: Interface<25>.
* Data structures: Data structures<4>.
* Functions: Functions<8>.
* Test::


File: MemoryPoolSystem.info,  Node: Introduction<64>,  Next: Requirements<39>,  Up: AWL pool class

5.17.1 Introduction
-------------------

*note .readership;: 10c1. Any MPS developer.

*note .intro;: 10c2. The AWL (Automatic Weak Linked) pool is used to
manage Dylan Weak Tables (see req.dylan.fun.weak).  Currently the design
is specialised for Dylan Weak Tables, but it could be generalised in the
future.


File: MemoryPoolSystem.info,  Node: Requirements<39>,  Next: Definitions<11>,  Prev: Introduction<64>,  Up: AWL pool class

5.17.2 Requirements
-------------------

See req.dylan.fun.weak.

See meeting.dylan.1997-02-27(0) where many of the requirements for this
pool were first sorted out.

Must satisfy request.dylan.170123(1).

*note .req.obj-format;: 10c4. Only objects of a certain format need be
supported.  This format is a subset of the Dylan Object Format.  The
pool uses the first slot in the fixed part of an object to store an
association.  See mail.drj.1997-03-11.12-05(2).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170123

   (2) 
https://info.ravenbrook.com/project/mps/mail/1997/03/11/12-05/0.txt


File: MemoryPoolSystem.info,  Node: Definitions<11>,  Next: Overview<24>,  Prev: Requirements<39>,  Up: AWL pool class

5.17.3 Definitions
------------------

*note .def.grain;: 10c7. alignment grain, grain.  A grain is a range of
addresses where both the base and the limit of the range are aligned and
the size of range is equal to the (same) alignment.  In this context the
alignment is the pool’s alignment (‘pool->alignment’).  The grain is the
unit of allocation, marking, scanning, etc.


File: MemoryPoolSystem.info,  Node: Overview<24>,  Next: Interface<25>,  Prev: Definitions<11>,  Up: AWL pool class

5.17.4 Overview
---------------

*note .overview;: 10c9.

*note .overview.ms;: 10ca. The pool is mark and sweep.  *note
.overview.ms.justify;: 10cb. Mark-sweep pools are slightly easier to
write (than moving pools), and there are no requirements (yet) that this
pool be high performance or moving or anything like that.

*note .overview.alloc;: 10cc. It is possible to allocate weak or exact
objects using the normal reserve/commit AP protocol.  *note
.overview.alloc.justify;: 10cd. Allocation of both weak and exact
objects is required to implement Dylan Weak Tables.  Objects are
formatted; the pool uses format A.

*note .overview.scan;: 10ce. The pool handles the scanning of weak
objects specially so that when a weak reference is deleted the
corresponding reference in an associated object is deleted.  The
associated object is determined by using information stored in the
object itself (see *note .req.obj-format: 10c4.).


File: MemoryPoolSystem.info,  Node: Interface<25>,  Next: Data structures<4>,  Prev: Overview<24>,  Up: AWL pool class

5.17.5 Interface
----------------

*note .if.init;: 10d0. The init method takes one extra parameter in the
vararg list.  This parameter should have type ‘Format’ and be a format
object that describes the format of the objects to be allocated in this
pool.  The format should support scan and skip methods.  There is an
additional restriction on the layout of objects, see *note
.req.obj-format: 10c4.

*note .if.buffer;: 10d1. The ‘BufferInit()’ method takes one extra
parameter in the vararg list.  This parameter should be either
‘RankEXACT’ or ‘RankWEAK’.  It determines the rank of the objects
allocated using that buffer.


File: MemoryPoolSystem.info,  Node: Data structures<4>,  Next: Functions<8>,  Prev: Interface<25>,  Up: AWL pool class

5.17.6 Data structures
----------------------

*note .sig;: 10d3. This signature for this pool will be 0x519bla3l
(SIGPooLAWL).

*note .poolstruct;: 10d4. The class specific pool structure is:

     struct AWLStruct {
       PoolStruct poolStruct;
       PoolGenStruct pgenStruct; /* pool generation */
       PoolGen pgen;             /* NULL or pointer to pgenStruct */
       Count succAccesses;       /* number of successive single accesses */
       FindDependentFunction findDependent; /*  to find a dependent object */
       awlStatTotalStruct stats;
       Sig sig;                  /* <code/misc.h#sig> */
     }

*note .awlseg;: 10d5. The pool defines a segment class ‘AWLSegClass’,
which is a subclass of ‘MutatorSegClass’ (see
design.mps.seg.over.hierarchy.mutatorseg(1)).  All segments allocated by
the pool are instances of this class, and are of type ‘AWLSeg’, for
which the structure is:

     struct AWLSegStruct {
       GCSegStruct gcSegStruct;  /* superclass fields must come first */
       BT mark;
       BT scanned;
       BT alloc;
       Count grains;
       Count freeGrains;         /* free grains */
       Count bufferedGrains;     /* grains in buffers */
       Count newGrains;          /* grains allocated since last collection */
       Count oldGrains;          /* grains allocated prior to last collection */
       Count singleAccesses;     /* number of accesses processed singly */
       awlStatSegStruct stats;
       Sig sig;                  /* <code/misc.h#sig> */
     }

*note .awlseg.bt;: 10d6. The ‘mark’, ‘alloc’, and ‘scanned’ fields are
bit-tables (see design.mps.bt(2)).  Each bit in the table corresponds to
a single alignment grain in the pool.

*note .awlseg.mark;: 10d7. The ‘mark’ bit table is used to record mark
bits during a trace.  *note awlSegWhiten(): 10d8. (see *note
.fun.whiten: 10d9. below) sets all the bits of this table to zero.  Fix
will read and set bits in this table.  Currently there is only one mark
bit table.  This means that the pool can only be condemned for one
trace.

*note .awlseg.mark.justify;: 10da. This is simple, and can be improved
later when we want to run more than one trace.

*note .awlseg.scanned;: 10db. The ‘scanned’ bit-table is used to note
which objects have been scanned.  Scanning (see *note .fun.scan: 10dc.
below) a segment will find objects that are marked but not scanned, scan
each object found and set the corresponding bits in the scanned table.

*note .awlseg.alloc;: 10dd. The ‘alloc’ bit table is used to record
which portions of a segment have been allocated.  Ranges of bits in this
table are set in *note awlSegBufferFill(): 10de. when a buffer is
attached to the segment.  When a buffer is flushed (that is, *note
awlSegBufferEmpty(): 10df. is called) from the segment, the bits
corresponding to the unused portion at the end of the buffer are reset.

*note .awlseg.alloc.invariant;: 10e0. A bit is set in the alloc table if
and only if the corresponding address is currently being buffered, or
the corresponding address lies within the range of an allocated object.

*note .awlseg.grains;: 10e1. The ‘grains’ field is the number of grains
that fit in the segment.  Strictly speaking this is not necessary as it
can be computed from ‘SegSize’ and AWL’s alignment, however,
precalculating it and storing it in the segment makes the code simpler
by avoiding lots of repeated calculations.

*note .awlseg.freeGrains;: 10e2. A conservative estimate of the number
of free grains in the segment.  It is always guaranteed to be greater
than or equal to the number of free grains in the segment, hence can be
used during allocation to quickly pass over a segment.

     Note: Maintained by blah and blah.  Unfinished obviously.

   ---------- Footnotes ----------

   (1) seg.html#design.mps.seg.over.hierarchy.mutatorseg

   (2) bt.html


File: MemoryPoolSystem.info,  Node: Functions<8>,  Next: Test,  Prev: Data structures<4>,  Up: AWL pool class

5.17.7 Functions
----------------

     Note: How will pool collect?  It needs an action structure.

* Menu:

* External::
* Internal::


File: MemoryPoolSystem.info,  Node: External,  Next: Internal,  Up: Functions<8>

5.17.7.1 External
.................

 -- C Function: *note Res: 55f. AWLInit (Pool pool, va_list arg)

*note .fun.init;: 10e6. ‘AWLStruct’ has four fields, each one needs
initializing.

*note .fun.init.poolstruct;: 10e7. The ‘poolStruct’ field has already
been initialized by generic code (impl.c.pool).

*note .fun.init.sig;: 10e8. The ‘sig’ field will be initialized with the
signature for this pool.

 -- C Function: *note Res: 55f. AWLFinish (Pool pool)

*note .fun.finish;: 10ea. Iterates over all segments in the pool and
destroys each segment (by calling ‘SegFree()’).  Overwrites the sig
field in the ‘AWLStruct’.  Finishing the generic pool structure is done
by the generic pool code (impl.c.pool).

*note .fun.alloc;: 10eb. ‘PoolNoAlloc()’ will be used, as this class
does not implement alloc.

*note .fun.free;: 10ec. ‘PoolNoFree()’ will be used, as this class does
not implement free.

 -- C Function: *note Res: 55f. AWLBufferFill (Seg *segReturn, Addr
          *baseReturn, Pool pool, Buffer buffer, Size size)

*note .fun.fill;: 10ee. This zips round all the segments applying
‘SegBufferFill()’ to each segment.  *note awlSegBufferFill(): 10de.
attempts to find a large-enough free range; if it finds one then it may
be bigger than the actual request, in which case the remainder can be
used to “fill” the rest of the buffer.  If no free range can be found in
an existing segment then a new segment will be created (which is at
least large enough).  The range of buffered addresses is marked as
allocated in the segment’s alloc table.

 -- C Function: *note Res: 55f. AWLDescribe (Pool pool, mps_lib_FILE
          *stream, Count depth)

*note .fun.describe;: 10f0.


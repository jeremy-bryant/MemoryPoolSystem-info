This is MemoryPoolSystem.info, produced by makeinfo version 7.1.1 from
MemoryPoolSystem.texi.

     Memory Pool System 1.118.0, Feb 11, 2025

     Ravenbrook Limited

     Copyright © 2025, Ravenbrook Limited

INFO-DIR-SECTION Miscellaneous
START-INFO-DIR-ENTRY
* MemoryPoolSystem: (MemoryPoolSystem.info). One line description of project.
END-INFO-DIR-ENTRY


   Generated by Sphinx 8.1.3.


File: MemoryPoolSystem.info,  Node: Internal,  Prev: External,  Up: Functions<8>

5.17.7.2 Internal
.................

 -- C Function: *note Res: 55f. AWLSegCreate (AWLSeg *awlsegReturn, Size
          size)

*note .fun.awlsegcreate;: 10f3. Creates a segment of class ‘AWLSegClass’
of size at least ‘size’.

*note .fun.awlsegcreate.size.round;: 10f4. ‘size’ is rounded up to the
arena grain size before requesting the segment.

*note .fun.awlsegcreate.size.round.justify;: 10f5. The arena requires
that all segment sizes are rounded up to the arena grain size.

*note .fun.awlsegcreate.where;: 10f6. The segment is allocated using a
generation preference, using the generation number stored in the
‘AWLStruct’ (the ‘gen’ field), see *note .poolstruct: 10d4. above.

 -- C Function: *note Res: 55f. awlSegInit (Seg seg, Pool pool, Addr
          base, Size size, ArgList args)

*note .fun.awlseginit;: 10f8. Init method for ‘AWLSegClass’, called for
‘SegAlloc()’ whenever an ‘AWLSeg’ is created (see *note
.fun.awlsegcreate: 10f3. above).

*note .fun.awlseginit.tables;: 10f9. The segment’s mark scanned and
alloc tables (see *note .awlseg.bt: 10d6. above) are allocated and
initialised.  The segment’s grains field is computed and stored.

 -- C Function: void awlSegFinish (Seg seg)

*note .fun.awlsegfinish;: 10fb. Finish method for ‘AWLSegClass’, called
from ‘SegFree()’.  Will free the segment’s tables (see *note .awlseg.bt:
10d6.).

 -- C Function: *note Bool: 3a9. awlSegBufferFill (Addr *baseReturn,
          Addr *limitReturn, Seg seg, Size size, RankSet rankSet)

*note .fun.seg.buffer-fill;: 10fc. Searches for a free block in the
segment that is at least ‘size’ bytes long.  The base address of the
block is returned in ‘*baseReturn’, the limit of the entire free block
(which must be at least as large as ‘size’ and may be bigger) is
returned in ‘*limitReturn’.  The requested size is converted to a number
of grains, *note BTFindResRange(): dab. is called to find a run of this
length in the alloc bit-table (*note .awlseg.alloc: 10dd.).  The results
(if it is successful) from *note BTFindResRange(): dab. are in terms of
grains, they are converted back to addresses before returning the
relevant values from this function.

 -- C Function: void awlSegBufferEmpty (Seg seg, Buffer buffer)

*note .fun.seg.buffer-empty;: 10fd. Locates the free portion of the
buffer, that is the memory between the init and the limit of the buffer
and records these locations as being free in the alloc table.

 -- C Function: *note Res: 55f. awlSegWhiten (Seg seg, Trace trace)

*note .fun.whiten;: 10d9. The current design only permits each segment
to be condemned for one trace (see *note .awlseg.mark: 10d7.).  This
function checks that the segment is not white for any trace (‘seg->white
== TraceSetEMPTY’).  The segment’s mark bit-table is reset, and the
whiteness of the seg (‘seg->white’) has the current trace added to it.

 -- C Function: void awlSegGreyen (Seg seg, Trace trace)

*note .fun.grey;: 10ff. If the segment is not white for this trace, the
segment’s mark table is set to all 1s and the segment is recorded as
being grey.

 -- C Function: *note Res: 55f. awlSegScan (Bool *totalReturn, Seg seg,
          ScanState ss)

*note .fun.scan;: 10dc.

*note .fun.scan.overview;: 1101. The scanner performs a number of passes
over the segment, scanning each marked and unscanned (grey) object that
is finds.

*note .fun.scan.overview.finish;: 1102. It keeps perform a pass over the
segment until it is finished.

*note .fun.scan.overview.finish.condition;: 1103. A condition for
finishing is that no new marks got placed on objects in this segment
during the pass.

*note .fun.scan.overview.finish.approximation;: 1104. We use an even
stronger condition for finishing that assumes that scanning any object
may introduce marks onto this segment.  It is finished when a pass
results in scanning no objects (that is, all objects were either
unmarked or both marked and scanned).

*note .fun.scan.overview.finished-flag;: 1105. There is a flag called
‘finished’ which keeps track of whether we should finish or not.  We
only ever finish at the end of a pass.  At the beginning of a pass the
flag is set.  During a pass if any objects are scanned then the
‘finished’ flag is reset.  At the end of a pass if the ‘finished’ flag
is still set then we are finished.  No more passes take place and the
function returns.

*note .fun.scan.pass;: 1106. A pass consists of a setup phase and a
repeated phase.

*note .fun.scan.pass.buffer;: 1107. The following assumes that in the
general case the segment is buffered; if the segment is not buffered
then the actions that mention buffers are not taken (they are
unimportant if the segment is not buffered).

*note .fun.scan.pass.p;: 1108. The pass uses a cursor called ‘p’ to
progress over the segment.  During a pass ‘p’ will increase from the
base address of the segment to the limit address of the segment.  When
‘p’ reaches the limit address of the segment, the pass in complete.

*note .fun.scan.pass.setup;: 1109. ‘p’ initially points to the base
address of the segment.

*note .fun.scan.pass.repeat;: 110a. The following comprises the repeated
phase.  The repeated phase is repeated until the pass completion
condition is true (that is, ‘p’ has reached the limit of the segment,
see *note .fun.scan.pass.p: 1108. above and *note
.fun.scan.pass.repeat.complete: 110b. below).

*note .fun.scan.pass.repeat.complete;: 110b. If ‘p’ is equal to the
segment’s limit then we are done.  We proceed to check whether any
further passes need to be performed (see *note .fun.scan.pass.more:
110c. below).

*note .fun.scan.pass.repeat.free;: 110d. If ‘!alloc(p)’ (the grain is
free) then increment ‘p’ and return to the beginning of the loop.

*note .fun.scan.pass.repeat.buffer;: 110e. If ‘p’ is equal to the
buffer’s ScanLimit, as returned by ‘BufferScanLimit()’, then set ‘p’
equal to the buffer’s Limit, as returned by ‘BufferLimit()’ and return
to the beginning of the loop.

*note .fun.scan.pass.repeat.object-end;: 110f. The end of the object is
located using the ‘format->skip’ method.

*note .fun.scan.pass.repeat.object;: 1110. if ‘mark(p) && !scanned(p)’
then the object pointed at is marked but not scanned, which means we
must scan it, otherwise we must skip it.

*note .fun.scan.pass.repeat.object.dependent;: 1111. To scan the object
the object we first have to determine if the object has a dependent
object (see *note .req.obj-format: 10c4.).

*note .fun.scan.pass.repeat.object.dependent.expose;: 1112. If it has a
dependent object then we must expose the segment that the dependent
object is on (only if the dependent object actually points to MPS
managed memory) prior to scanning and cover the segment subsequent to
scanning.

*note .fun.scan.pass.repeat.object.dependent.summary;: 1113. The summary
of the dependent segment must be set to ‘RefSetUNIV’ to reflect the fact
that we are allowing it to be written to (and we don’t know what gets
written to the segment).

*note .fun.scan.pass.repeat.object.scan;: 1114. The object is then
scanned by calling the format’s scan method with base and limit set to
the beginning and end of the object (*note
.fun.scan.scan.improve.single;: 1115. A scan1 format method would make
it slightly simpler here).  Then the finished flag is cleared and the
bit in the segment’s scanned table is set.

*note .fun.scan.pass.repeat.advance;: 1116. ‘p’ is advanced past the
object and we return to the beginning of the loop.

*note .fun.scan.pass.more;: 110c. At the end of a pass the finished flag
is examined.

*note .fun.scan.pass.more.not;: 1117. If the finished flag is set then
we are done (see *note .fun.scan.overview.finished-flag: 1105. above),
*note awlSegScan(): 1100. returns.

*note .fun.scan.pass.more.so;: 1118. Otherwise (the finished flag is
reset) we perform another pass (see *note .fun.scan.pass: 1106. above).

 -- C Function: *note Res: 55f. awlSegFix (Seg seg, ScanState ss, Ref
          *refIO)

*note .fun.fix;: 111a. If the rank (‘ss->rank’) is ‘RankAMBIG’ then fix
returns immediately unless the reference is in the segment bounds,
aligned to the pool alignment, and allocated.

The bit in the marked table corresponding to the referenced grain will
be read.  If it is already marked then fix returns.  Otherwise (the
grain is unmarked), ‘ss->wasMarked’ is set to ‘FALSE’ (see
design.mps.fix.was-marked.not(1)), the remaining actions depend on
whether the rank (‘ss->rank’) is ‘RankWEAK’ or not.  If the rank is weak
then the reference is adjusted to 0 (see design.mps.weakness) and fix
returns.  If the rank is something else then the mark bit corresponding
to the referenced grain is set, and the segment is greyed using
‘SegSetGrey()’.

 -- C Function: void awlSegReclaim (Seg seg, Trace trace)

*note .fun.reclaim;: 111c. This iterates over all allocated objects in
the segment and frees objects that are not marked.  When this iteration
is complete the marked array is completely reset.

‘p’ points to base of segment.  Then:

     while(p < SegLimit(seg) {
       if(!alloc(p)) { ++p;continue; }
       q = skip(p) /* q points to just past the object pointed at by p */
       if !marked(p) free(p, q); /* reset the bits in the alloc table from p to q-1 inclusive. */
       p = q
     }

Finally, reset the entire marked array using *note BTResRange(): d6e.

*note .fun.reclaim.improve.pad;: 111d. Consider filling free ranges with
padding objects.  Now reclaim doesn’t need to check that the objects are
allocated before skipping them.  There may be a corresponding change for
scan as well.

 -- C Function: *note Bool: 3a9. AWLDependentObject (Addr *objReturn,
          Addr parent)

*note .fun.dependent-object;: 111f. This function abstracts the
association between an object and its linked dependent (see *note
.req.obj-format: 10c4.).  It currently assumes that objects are Dylan
Object formatted according to design.dylan.container (see
analysis.mps.poolawl.dependent.abstract for suggested improvements).  An
object has a dependent object iff the second word of the object, that
is, ‘((Word *)parent)[1]’, is non-‘NULL’.  The dependent object is the
object referenced by the second word and must be a valid object.

This function assumes objects are in Dylan Object Format (see
design.dylan.container).  It will check that the first word looks like a
Dylan wrapper pointer.  It will check that the wrapper indicates that
the wrapper has a reasonable format (namely at least one fixed field).
If the second word is ‘NULL’ it will return ‘FALSE’.  If the second word
is non-‘NULL’ then the contents of it will be assigned to ‘*objReturn’,
and it will return ‘TRUE’.

   ---------- Footnotes ----------

   (1) fix.html#design.mps.fix.was-marked.not


File: MemoryPoolSystem.info,  Node: Test,  Prev: Functions<8>,  Up: AWL pool class

5.17.8 Test
-----------

   - must create Dylan objects.

   - must create Dylan vectors with at least one fixed field.

   - must allocate weak thingies.

   - must allocate exact tables.

   - must link tables together.

   - must populate tables with junk.

   - some junk must die.

Use an LO pool and an AWL pool.  Three buffers.  One buffer for the LO
pool, one exact buffer for the AWL pool, one weak buffer for the AWL
pool.

Initial test will allocate one object from each buffer and then destroy
all buffers and pools and exit


File: MemoryPoolSystem.info,  Node: LO pool class,  Next: MFS pool class,  Prev: AWL pool class,  Up: Old design

5.18 LO pool class
==================

* Menu:

* Introduction: Introduction<65>.
* Definitions: Definitions<12>.
* Requirements: Requirements<40>.
* Overview: Overview<25>.
* Interface: Interface<26>.
* Data structures: Data structures<5>.
* Functions: Functions<9>.
* Attachment::


File: MemoryPoolSystem.info,  Node: Introduction<65>,  Next: Definitions<12>,  Up: LO pool class

5.18.1 Introduction
-------------------

*note .readership;: 1126. Any MPS developer.

*note .intro;: 1127. The LO (Leaf Object) pool class is a pool class
developed for DylanWorks.  It is designed to manage objects that have no
references (leaf objects) such as strings, bit tables, etc.  It is a
garbage collected pool (in that objects allocated in the pool are
automatically reclaimed when they are discovered to be unreachable.

     Note: Need to sort out issue of alignment.  Currently lo grabs
     alignment from format, almost certainly “ought” to use the greater
     of the format alignment and the ‘MPS_ALIGN’ value.  David Jones,
     1997-07-02.


File: MemoryPoolSystem.info,  Node: Definitions<12>,  Next: Requirements<40>,  Prev: Introduction<65>,  Up: LO pool class

5.18.2 Definitions
------------------

*note .def.leaf;: 1129. A “leaf” object is an object that contains no
references, or an object all of whose references refer to roots.  That
is, any references that the object has must refer to a priori alive
objects that are guaranteed not to move, hence the references do not
need fixing.

*note .def.grain;: 112a. A grain (of some alignment) is a contiguous
aligned area of memory of the smallest size possible (which is the same
size as the alignment).


File: MemoryPoolSystem.info,  Node: Requirements<40>,  Next: Overview<25>,  Prev: Definitions<12>,  Up: LO pool class

5.18.3 Requirements
-------------------

*note .req.source;: 112c. See req.dylan.fun.obj.alloc and
req.dylan.prot.ffi.access.

*note .req.leaf;: 112d. The pool must manage formatted leaf objects (see
*note .def.leaf: 1129. above for a definition).  This is intended to
encompass Dylan and C leaf objects.  Dylan leaf objects have a reference
to their wrapper, but are still leaf objects (in the sense of *note
.def.leaf: 1129.) because the wrapper will be a root.

*note .req.nofault;: 112e. The memory containing objects managed by the
pool must not be protected.  The client must be allowed to access these
objects without hitting an MPS barrier.


File: MemoryPoolSystem.info,  Node: Overview<25>,  Next: Interface<26>,  Prev: Requirements<40>,  Up: LO pool class

5.18.4 Overview
---------------

*note .overview;: 1130.

*note .overview.ms;: 1131. The LO Pool is a non-moving mark-and-sweep
collector.

*note .overview.ms.justify;: 1132. Mark-and-sweep pools are simpler than
moving pools.

*note .overview.alloc;: 1133. Objects are allocated in the pool using
the reserve/commit protocol on allocation points.

*note .overview.format;: 1134. The pool is formatted.  The format of the
objects in the pool is specified at instantiation time, using a format
object derived from a variant A format (using variant A is overkill, see
*note .if.init: 1135. below) (see design.mps.format for excuse about
calling the variant ‘A’).


File: MemoryPoolSystem.info,  Node: Interface<26>,  Next: Data structures<5>,  Prev: Overview<25>,  Up: LO pool class

5.18.5 Interface
----------------

*note .if.init;: 1135.

*note .if.init.args;: 1137. The init method for this class takes one
extra parameter in the vararg parameter list.

*note .if.init.format;: 1138. The extra parameter should be an object of
type Format and should describe the format of the objects that are to be
allocated in the pool.

*note .if.init.format.use;: 1139. The pool uses the skip and alignment
slots of the format.  The skip method is used to determine the length of
objects (during reclaim).  The alignment field is used to determine the
granularity at which memory should be managed.

*note .if.init.format.a;: 113a. Currently only format variant A is
supported though clearly that is overkill as only skip and alignment are
used.


File: MemoryPoolSystem.info,  Node: Data structures<5>,  Next: Functions<9>,  Prev: Interface<26>,  Up: LO pool class

5.18.6 Data structures
----------------------

*note .sig;: 113c. The signature for the LO Pool Class is 0x51970b07
(SIGLOPOoL).

*note .poolstruct;: 113d. The class specific pool structure is:

     typedef struct LOStruct {
       PoolStruct poolStruct;        /* generic pool structure */
       PoolGenStruct pgenStruct;     /* pool generation */
       PoolGen pgen;                 /* NULL or pointer to pgenStruct */
       Sig sig;                      /* <code/misc.h#sig> */
     } LOStruct;

*note .loseg;: 113e. Every segment is an instance of segment class
‘LOSegClass’, a subclass of ‘MutatorSegClass’ (see
design.mps.seg.over.hierarchy.mutatorseg(1)), and is an object of type
‘LOSegStruct’.

*note .loseg.purpose;: 113f. The purpose of the ‘LOSeg’ structure is to
associate the bit tables used for recording allocation and mark
information with the segment.

*note .loseg.decl;: 1140. The declaration of the structure is as
follows:

     typedef struct LOSegStruct {
       GCSegStruct gcSegStruct;  /* superclass fields must come first */
       BT mark;                  /* mark bit table */
       BT alloc;                 /* alloc bit table */
       Count freeGrains;         /* free grains */
       Count bufferedGrains;     /* grains in buffers */
       Count newGrains;          /* grains allocated since last collection */
       Count oldGrains;          /* grains allocated prior to last collection */
       Sig sig;                  /* <code/misc.h#sig> */
     } LOSegStruct;

*note .loseg.sig;: 1141. The signature for a loseg is 0x519705E9
(SIGLOSEG).

*note .loseg.lo;: 1142. The lo field points to the LO structure that
owns this segment.

*note .loseg.bit;: 1143. Bit Tables (see design.mps.bt(2)) are used to
record allocation and mark information.  This is relatively
straightforward, but might be inefficient in terms of space in some
circumstances.

*note .loseg.mark;: 1144. This is a Bit Table that is used to mark
objects during a trace.  Each grain in the segment is associated with 1
bit in this table.  When *note loSegFix(): 4c0. (see *note .fun.fix:
1145. below) is called the address is converted to a grain within the
segment and the corresponding bit in this table is set.

*note .loseg.alloc;: 1146. This is a Bit Table that is used to record
which addresses are allocated.  Addresses that are allocated and are not
buffered have their corresponding bit in this table set.  If a bit in
this table is reset then either the address is free or is being
buffered.

*note .loseg.diagram;: 1147. The following diagram is now obsolete.
It’s also not very interesting - but I’ve left the sources in case
anyone ever gets around to updating it.  tony 1999-12-16

[missing diagram]

   ---------- Footnotes ----------

   (1) seg.html#design.mps.seg.over.hierarchy.mutatorseg

   (2) bt.html


File: MemoryPoolSystem.info,  Node: Functions<9>,  Next: Attachment,  Prev: Data structures<5>,  Up: LO pool class

5.18.7 Functions
----------------

* Menu:

* External: External<2>.
* Internal: Internal<2>.


File: MemoryPoolSystem.info,  Node: External<2>,  Next: Internal<2>,  Up: Functions<9>

5.18.7.1 External
.................

*note .fun.init;: 114a.

*note .fun.destroy;: 114b.

*note .fun.buffer-fill;: 114c.

     Note: Explain way in which buffers interact with the alloc table
     and how it could be improved.

*note .fun.buffer-empty;: 114d.

*note .fun.condemn;: 114e.


File: MemoryPoolSystem.info,  Node: Internal<2>,  Prev: External<2>,  Up: Functions<9>

5.18.7.2 Internal
.................

 -- C Function: *note Res: 55f. loSegFix (Seg seg, ScanState ss, Ref
          *refIO)

*note .fun.fix;: 1145. Fix treats references of most ranks much the
same.  There is one mark table that records all marks.  A reference of
rank ‘RankAMBIG’ is first checked to see if it is aligned to the pool
alignment and discarded if not.  The reference is converted to a grain
number within the segment (by subtracting the segments’ base from the
reference and then dividing by the grain size).  The bit (the one
corresponding to the grain number) is set in the mark table.  Exception,
for a weak reference (rank is ‘RankWEAK’) the mark table is checked and
the reference is fixed to 0 if this address has not been marked
otherwise nothing happens.  Note that there is no check that the
reference refers to a valid object boundary (which wouldn’t be a valid
check in the case of ambiguous references anyway).

 -- C Function: void loSegReclaim (Seg seg, Trace trace)

*note .fun.segreclaim;: 1150. For all the contiguous allocated regions
in the segment it locates the boundaries of all the objects in that
region by repeatedly skipping (by calling ‘format->skip’) from the
beginning of the region (the beginning of the region is guaranteed to
coincide with the beginning of an object).  For each object it examines
the bit in the mark bit table that corresponds to the beginning of the
object.  If that bit is set then the object has been marked as a result
of a previous call to *note loSegFix(): 4c0, the object is preserved by
doing nothing.  If that bit is not set then the object has not been
marked and should be reclaimed; the object is reclaimed by resetting the
appropriate range of bits in the segment’s free bit table.

     Note: Special things happen for buffered segments.

     Explain how the marked variable is used to free segments.


File: MemoryPoolSystem.info,  Node: Attachment,  Prev: Functions<9>,  Up: LO pool class

5.18.8 Attachment
-----------------

[missing attachment “LOGROUP.CWK”]


File: MemoryPoolSystem.info,  Node: MFS pool class,  Next: MRG pool class,  Prev: LO pool class,  Up: Old design

5.19 MFS pool class
===================

* Menu:

* Overview: Overview<26>.
* Implementation: Implementation<23>.


File: MemoryPoolSystem.info,  Node: Overview<26>,  Next: Implementation<23>,  Up: MFS pool class

5.19.1 Overview
---------------

MFS stands for “Manual Fixed Small”.  The MFS pool class manages objects
that are of a fixed size.  It is intended to only manage small objects
efficiently.  Storage is recycled manually by the client programmer.

A particular instance of an MFS Pool can manage objects only of a single
size, but different instances can manage objects of different sizes.
The size of object that an instance can manage is declared when the
instance is created.


File: MemoryPoolSystem.info,  Node: Implementation<23>,  Prev: Overview<26>,  Up: MFS pool class

5.19.2 Implementation
---------------------

*note .impl.extents;: 1158. MFS operates in a very simple manner: each
extent allocated from the arena is divided into units.

*note .impl.free-units;: 1159. Free units are kept on a linked list
using a header stored in the unit itself.  The linked list is not
ordered; allocation and deallocation simply pop and push from the head
of the list.  This is fast, but successive allocations might have poor
locality if previous successive frees did.

*note .impl.extent-ring;: 115a. The list of extents belonging to the
pool is maintained as a ring with a node at the start of each extent.

*note .impl.extent-ring.justify;: 115b. Storing the linked list of free
nodes and the extent ring node in the managed memory is against the
general principle of the MPS design, which keeps its management
structures away from client memory.  However, the MFS pool is used
during the bootstrapping process (see
design.mps.bootstrap.land.sol.pool(1)) and so has no other memory pools
available for storage.

   ---------- Footnotes ----------

   (1) bootstrap.html#design.mps.bootstrap.land.sol.pool


File: MemoryPoolSystem.info,  Node: MRG pool class,  Next: Manual Variable Temporal MVT pool design,  Prev: MFS pool class,  Up: Old design

5.20 MRG pool class
===================

* Menu:

* Introduction: Introduction<66>.
* Goals: Goals<3>.
* Requirements: Requirements<41>.
* Terminology: Terminology<2>.
* Overview: Overview<27>.
* Protocols::
* Data structures: Data structures<6>.
* Functions: Functions<10>.
* Transgressions::
* Future: Future<3>.
* Tests: Tests<2>.
* Notes: Notes<8>.


File: MemoryPoolSystem.info,  Node: Introduction<66>,  Next: Goals<3>,  Up: MRG pool class

5.20.1 Introduction
-------------------

*note .readership;: 1162. Any MPS developer.

*note .intro;: 1163. This is the design of the MRG (Manual Rank
Guardian) pool class.  The MRG pool class is part of the MPS. The MRG
pool class is internal to the MPS (has no client interface) and is used
to implement finalization.

*note .source;: 1164. Some of the techniques in paper.dbe93 (“Guardians
in a Generation-Based Garbage Collector”) were used in this design.
Some analysis of this design (including various improvements and some
more in-depth justification) is in analysis.mps.poolmrg.  That document
should be understood before changing this document.  It is also helpful
to look at design.mps.finalize(1) and design.mps.message(2).

   ---------- Footnotes ----------

   (1) finalize.html

   (2) message.html


File: MemoryPoolSystem.info,  Node: Goals<3>,  Next: Requirements<41>,  Prev: Introduction<66>,  Up: MRG pool class

5.20.2 Goals
------------

*note .goal.final;: 1167. The MRG pool class should support all
requirements pertaining to finalization.


File: MemoryPoolSystem.info,  Node: Requirements<41>,  Next: Terminology<2>,  Prev: Goals<3>,  Up: MRG pool class

5.20.3 Requirements
-------------------

*note .req;: 1169. We have only one requirement pertaining to
finalization:

*note .req.dylan.fun.finalization;: 116a. Support the Dylan
language-level implementation of finalized objects: objects are
registered, and are finalized in random order when they would otherwise
have died.  Cycles are broken at random places.  There is no guarantee
of promptness.

*note .req.general;: 116b. However, finalization is a very common piece
of functionality that is provided by (sophisticated) memory managers, so
we can expect other clients to request this sort of functionality.

*note .anti-req;: 116c. Is it required that the MRG pool class return
unused segments to the arena?  MFS, for example, does not do this.  MRG
will not do this in its initial implementation.


File: MemoryPoolSystem.info,  Node: Terminology<2>,  Next: Overview<27>,  Prev: Requirements<41>,  Up: MRG pool class

5.20.4 Terminology
------------------

*note .def.mrg;: 116e. 'MRG': The MRG pool class’s identifier will be
MRG. This stands for “Manual Rank Guardian”.  The pool is manually
managed and implements guardians for references of a particular rank
(currently just final).

*note .def.final.ref;: 116f. 'final reference': A reference of rank
final (see design.mps.type.rank).

*note .def.final.object;: 1170. 'finalizable object': An object is
finalizable with respect to a final reference if, since the creation of
that reference, there was a point in time when no references to the
object of lower (that is, stronger) rank were reachable from a root.

*note .def.final.object.note;: 1171. Note that this means an object can
be finalizable even if it is now reachable from the root via exact
references.

*note .def.finalize;: 1172. 'finalize': To finalize an object is to
notify the client that the object is finalizable.  The client is
presumed to be interested in this information (typically it will apply
some method to the object).

*note .def.guardian;: 1173. 'guardian': An object allocated in the MRG
Pool.  A guardian contains exactly one final reference, and some fields
for the pool’s internal use.  Guardians are used to implement a
finalization mechanism.


File: MemoryPoolSystem.info,  Node: Overview<27>,  Next: Protocols,  Prev: Terminology<2>,  Up: MRG pool class

5.20.5 Overview
---------------

*note .over;: 1175. The MRG pool class is a pool class in the MPS. It is
intended to provide the functionality of “finalization”.

*note .over.internal;: 1176. The MRG pool class is internal to the MPM:
it is not intended to have a client interface.  Clients are expected to
access the functionality provided by this pool (finalization) using a
separate MPS finalization interface (design.mps.finalize(1)).

*note .over.one-size;: 1177. The MRG pool class manages objects of a
single size, each object containing a single reference of rank final.

*note .over.one-size.justify;: 1178. This is all that is necessary to
meet our requirements for finalization.  Whenever an object is
registered for finalization, it is sufficient to create a single
reference of rank final to it.

*note .over.queue;: 1179. A pool maintains a list of live guardian
objects, called (for historical reasons) the “entry” list.

*note .over.queue.free;: 117a. The pool also maintains a list of free
guardian objects called the “free” list.

*note .over.queue.exit.not;: 117b. There used to be an “exit” list, but
this is now historical and there shouldn’t be any current references to
it.

*note .over.alloc;: 117c. When guardians are allocated, they are placed
on the entry list.  Guardians on the entry list refer to objects that
have not yet been shown to be finalizable (either the object has
references of lower rank than final to it, or the MPS has not yet got
round to determining that the object is finalizable).

*note .over.message.create;: 117d. When a guardian is discovered to
refer to a finalizable object it is removed from the entry list and
becomes a message on the arena’s messages queue.

*note .over.message.deliver;: 117e. When the MPS client receives the
message the message system arranges for the message to be destroyed and
the pool reclaims the storage associated with the guardian/message.

*note .over.scan;: 117f. When the pool is scanned at rank final each
reference will be fixed.  If the reference is to an unmarked object
(before the fix), then the object must now be finalizable.  In this case
the containing guardian will be removed from the entry list and posted
as a message.

*note .over.scan.justify;: 1180. The scanning process is a crucial step
necessary for implementing finalization.  It is the means by which the
MPS detects that objects are finalizable.

*note .over.message;: 1181. ‘PoolClassMRG’ implements a *note
MessageClass: 715. (see design.mps.message(2)).  All the messages are of
one *note MessageType: 708.  This type is ‘MessageTypeFINALIZATION’.
Messages are created when objects are discovered to be finalizable and
destroyed when the MPS client has received the message.

*note .over.message.justify;: 1182. Messages provide a means for the MPS
to communicate with its client.  Notification of finalization is just
such a communication.  Messages allow the MPS to inform the client of
finalization events when it is convenient for the MPS to do so (i.e.
not in PageFault context).

*note .over.manual;: 1183. Objects in the MRG pool are manually managed.

*note .over.manual.alloc;: 1184. They are allocated by *note
ArenaFinalize(): 55e. when objects are registered for finalization.

*note .over.manual.free;: 1185. They are freed when the associated
message is destroyed.

*note .over.manual.justify;: 1186. The lifetime of a guardian object is
very easy to determine so manual memory management is appropriate.

   ---------- Footnotes ----------

   (1) finalize.html

   (2) message.html


File: MemoryPoolSystem.info,  Node: Protocols,  Next: Data structures<6>,  Prev: Overview<27>,  Up: MRG pool class

5.20.6 Protocols
----------------

* Menu:

* Object Registration::
* Finalizer execution::
* Setup / destroy::


File: MemoryPoolSystem.info,  Node: Object Registration,  Next: Finalizer execution,  Up: Protocols

5.20.6.1 Object Registration
............................

*note .protocol.register;: 1189. There is a protocol by which objects
can be registered for finalization.  This protocol is handled by the
arena module on behalf of finalization.  see
design.mps.finalize.int.finalize(1).

   ---------- Footnotes ----------

   (1) finalize.html#design.mps.finalize.int.finalize


File: MemoryPoolSystem.info,  Node: Finalizer execution,  Next: Setup / destroy,  Prev: Object Registration,  Up: Protocols

5.20.6.2 Finalizer execution
............................

*note .protocol.finalizer;: 118c. If an object is proven to be
finalizable then a message to this effect will eventually be posted.  A
client can receive the message, determine what to do about it, and do
it.  Typically this would involve calling the finalization method for
the object, and deleting the message.  Once the message is deleted, the
object may become recyclable.


File: MemoryPoolSystem.info,  Node: Setup / destroy,  Prev: Finalizer execution,  Up: Protocols

5.20.6.3 Setup / destroy
........................

*note .protocol.life;: 118e. An instance of PoolClassMRG is needed in
order to support finalization, it is called the “final” pool and is
attached to the arena (see design.mps.finalize.int.arena.struct(1)).

*note .protocol.life.birth;: 118f. The final pool is created lazily by
*note ArenaFinalize(): 55e.

*note .protocol.life.death;: 1190. The final pool is destroyed during
‘ArenaDestroy()’.

   ---------- Footnotes ----------

   (1) finalize.html#design.mps.finalize.int.arena.struct


File: MemoryPoolSystem.info,  Node: Data structures<6>,  Next: Functions<10>,  Prev: Protocols,  Up: MRG pool class

5.20.7 Data structures
----------------------

*note .guardian;: 1192. The guardian

*note .guardian.over;: 1193. A guardian is an object used to manage the
references and other data structures that are used by the pool in order
to keep track of which objects are registered for finalization, which
ones have been finalized, and so on.

*note .guardian.state;: 1194. A guardian can be in one of four states:

*note .guardian.state.enum;: 1195. The states are Free, Prefinal, Final,
PostFinal (referred to as MRGGuardianFree, etc.  in the implementation).

  1. *note .guardian.state.free;: 1196. The guardian is free, meaning
     that it is on the free list for the pool and available for
     allocation.

  2. *note .guardian.state.prefinal;: 1197. The guardian is allocated,
     and refers to an object that has not yet been discovered to be
     finalizable.  It is on the entry list for the pool.

  3. *note .guardian.state.final;: 1198. The guardian is allocated, and
     refers to an object that has been shown to be finalizable; this
     state corresponds to the existence of a message.

  4. *note .guardian.state.postfinal;: 1199. This state is only used
     briefly and is entirely internal to the pool; the guardian enters
     this state just after the associated message has been destroyed
     (which happens when the client receives the message) and will be
     freed immediately (whereupon it will enter the Free state).  This
     state is used for checking only (so that MRGFree can check that
     only guardians in this state are being freed).

*note .guardian.life-cycle;: 119a. Guardians go through the following
state life-cycle: Free ⟶ Prefinal ⟶ Final ⟶ Postfinal ⟶ Free.

*note .guardian.two-part;: 119b. A guardian is a structure consisting
abstractly of a link part and a reference part.  Concretely, the link
part is a ‘LinkPartStruct’, and the reference part is a ‘RefPartStruct’
(which is just a *note Word: 653.).  The link part is used by the pool,
the reference part forms the object visible to clients of the pool.  The
reference part is the reference of ‘RankFINAL’ that refers to objects
registered for finalization and is how the MPS detects finalizable
objects.

*note .guardian.two-part.union;: 119c. The ‘LinkPartStruct’ is a
discriminated union of a ‘RingStruct’ and a ‘MessageStruct’.  The
‘RingStruct’ is used when the guardian is either Free or Prefinal.  The
MessageStruct is used when the guardian is Final.  Neither part of the
union is used when the guardian is in the Postfinal state.

*note .guardian.two-part.justify;: 119d. This may seem a little
profligate with space, but this is okay as we are not required to make
finalization extremely space efficient.

*note .guardian.parts.separate;: 119e. The two parts will be stored in
separate segments.

*note .guardian.parts.separate.justify;: 119f. This is so that the data
structures the pool uses to manage the objects can be separated from the
objects themselves.  This avoids the pool having to manipulate data
structures that are on shielded segments
(analysis.mps.poolmrg.hazard.shield).

*note .guardian.assoc;: 11a0. Ref part number 'n' (from the beginning of
the segment) in one segment will correspond with link part number 'n' in
another segment.  The association between the two segments will be
managed by the additional fields in pool-specific segment subclasses
(see *note .mrgseg: 11a1.).

*note .guardian.ref;: 11a2. Guardians that are either Prefinal or Final
are live and have valid references (possibly ‘NULL’) in their ref parts.
Guardians that are free are dead and always have ‘NULL’ in their ref
parts (see *note .free.overwrite: 11a3. and *note .scan.free: 11a4.).

*note .guardian.ref.free;: 11a5. When freeing an object, it is a pointer
to the reference part that will be passed (internally in the pool).

*note .guardian.init;: 11a6. Guardians are initialized when the pool is
grown (*note .alloc.grow: 11a7.).  The initial state has the ref part
‘NULL’ and the link part is attached to the free ring.  Freeing an
object returns a guardian to its initial state.

*note .poolstruct;: 11a8. The Pool structure, ‘MRGStruct’ will have:

   - *note .poolstruct.entry;: 11a9. the head of the entry list.

   - *note .poolstruct.free;: 11aa. the head of the free list.

   - *note .poolstruct.rings;: 11ab. The entry list, the exit list, and
     the free list will each be implemented as a *note Ring: 85c.  Each
     ring will be maintained using the link part of the guardian.

     *note .poolstruct.rings.justify;: 11ac. This is because rings are
     convenient to use and are well tested.  It is possible to implement
     all three lists using a singly linked list, but the saving is
     certainly not worth making at this stage.

   - *note .poolstruct.refring;: 11ad. a ring of “ref” segments in use
     for links or messages (see .mrgseg.ref.mrgring below).

   - *note .poolstruct.extend;: 11ae. a precalculated ‘extendBy’ field
     (see *note .init.extend: 11af.).  This value is used to determine
     how large a segment should be requested from the arena for the
     reference part segment when the pool needs to grow (see *note
     .alloc.grow.size: 11b0.).

     *note .poolstruct.extend.justify;: 11b1. Calculating a reasonable
     value for this once and remembering it simplifies the allocation
     (*note .alloc.grow: 11a7.).

*note .poolstruct.init;: 11b2. poolstructs are initialized once for each
pool instance by *note MRGInit(): 11b3. (*note .init: 11b4.).  The
initial state has all the rings initialized to singleton rings, and the
‘extendBy’ field initialized to some value (see *note .init.extend:
11af.).

*note .mrgseg;: 11a1. The pool defines two segment subclasses:
‘MRGRefSegClass’ and ‘MRGLinkSegClass’.  Segments of the former class
will be used to store the ref parts of guardians, segments of the latter
will be used to store the link parts of guardians (see *note
.guardian.two-part: 119b.).  Segments are always allocated in pairs,
with one of each class, by the function ‘MRGSegPairCreate()’.  Each
segment contains a link to its pair.

*note .mrgseg.ref;: 11b5. ‘MRGRefSegClass’ is a subclass of
‘MutatorSegClass’.  Instances are of type ‘MRGRefSeg’, and contain:

   - *note .mrgseg.ref.mrgring;: 11b6. a field for the ring of ref part
     segments in the pool.

   - *note .mrgseg.ref.linkseg;: 11b7. a pointer to the paired link
     segment.

   - *note .mrgseg.ref.grey;: 11b8. a set describing the greyness of the
     segment for each trace.

*note .mrgseg.ref.init;: 11b9. A segment is created and initialized once
every time the pool is grown (*note .alloc.grow: 11a7.).  The initial
state has the segment ring node initialized and attached to the pool’s
segment ring, the linkseg field points to the relevant link segment, the
grey field is initialized such that the segment is not grey for all
traces.

*note .mrgseg.link;: 11ba. ‘MRGLinkSegClass’ is a subclass of
‘SegClass’.  Instances are of type ‘MRGLinkSeg’, and contain:

   - *note .mrgseg.link.refseg;: 11bb. a pointer to the paired ref
     segment.  This may be ‘NULL’ during initialization, while the
     pairing is being established.

   - *note .mrgseg.link.init;: 11bc. The initial state has the ‘linkseg’
     field pointing to the relevant ref segment.


File: MemoryPoolSystem.info,  Node: Functions<10>,  Next: Transgressions,  Prev: Data structures<6>,  Up: MRG pool class

5.20.8 Functions
----------------

 -- C Function: *note Bool: 3a9. MRGCheck (MRG mrg)

*note .check;: 11bf. Check the signatures, the class, and each field of
the ‘MRGStruct’.  Each field is checked as being appropriate for its
type.

*note .check.justify;: 11c0. There are no non-trivial invariants that
can be easily checked.

 -- C Function: *note Res: 55f. MRGRegister (Pool pool, Ref ref)

*note .alloc;: 11c2. Add a guardian for ‘ref’.

*note .alloc.grow;: 11a7. If the free list is empty then two new
segments are allocated and the free list filled up from them (note that
the reference fields of the new guardians will need to be overwritten
with ‘NULL’, see *note .free.overwrite: 11a3.)

*note .alloc.grow.size;: 11b0. The size of the reference part segment
will be the pool’s ‘extendBy’ (*note .poolstruct.extend: 11ae.) value.
The link part segment will be whatever size is necessary to accommodate
'N' link parts, where 'N' is the number of reference parts that fit in
the reference part segment.

*note .alloc.error;: 11c3. If any of the requests for more resource
(there are two; one for each of two segments) fail then the successful
requests will be retracted and the result code from the failing request
will be returned.

*note .alloc.pop;: 11c4. *note MRGRegister(): 11c1. pops a ring node off
the free list, and add it to the entry list.

 -- C Function: *note Res: 55f. MRGDeregister (Pool pool, Ref obj)

*note .free;: 11c6. Remove the guardian from the message queue and add
it to the free list.

*note .free.push;: 11c7. The guardian will simply be added to the front
of the free list (that is, no keeping the free list in address order or
anything like that).

*note .free.inadequate;: 11c8. No attempt will be made to return unused
free segments to the arena (although see
analysis.mps.poolmrg.improve.free.* for suggestions).

*note .free.overwrite;: 11a3. *note MRGDeregister(): 11c5. also writes
over the reference with ‘NULL’.  *note .free.overwrite.justify;: 11c9.
This is so that when the segment is subsequently scanned (*note
.scan.free: 11a4.), the reference that used to be in the object is not
accidentally fixed.

 -- C Function: *note Res: 55f. MRGInit (Pool pool, ArgList args)

*note .init;: 11b4. Initializes the entry list, the free ring, the ref
ring, and the ‘extendBy’ field.

*note .init.extend;: 11af. The ‘extendBy’ field is initialized to the
arena grain size.

*note .init.extend.justify;: 11ca. This is adequate as the pool is not
expected to grow very quickly.

 -- C Function: void MRGFinish (Pool pool)

*note .finish;: 11cc. Iterate over all the segments, returning all the
segments to the arena.

 -- C Function: *note Res: 55f. mrgRefSegScan (Bool *totalReturn, Pool
          pool, Seg seg, ScanState ss)

*note .scan;: 11ce. *note mrgRefSegScan(): 11cd. scans a segment of
guardians.

*note .scan.trivial;: 11cf. Scan will do nothing (that is, return
immediately) if the tracing rank is anything other than final.

     Note: This optimization is missing.  impl.c.trace.scan.conservative
     is not a problem because there are no faults on these segs, because
     there are no references into them.  But that’s why ‘TraceScan()’
     can’t do it.  Pekka P. Pirinen, 1997-09-19.

*note .scan.trivial.justify;: 11d0. If the rank is lower than final then
scanning is detrimental, it will only delay finalization.  If the rank
is higher than final there is nothing to do, the pool only contains
final references.

*note .scan.guardians;: 11d1. *note mrgRefSegScan(): 11cd. will iterate
over all guardians in the segment.  Every guardian’s reference will be
fixed (*note .scan.free;: 11a4. note that guardians that are on the free
list have ‘NULL’ in their reference part).

*note .scan.wasold;: 11d2. If the object referred to had not been fixed
previously (that is, was unmarked) then the object is not referenced by
a reference of a lower rank (than ‘RankFINAL’) and hence is finalizable.

*note .scan.finalize;: 11d3. The guardian will be finalized.  This
entails moving the guardian from state Prefinal to Final; it is removed
from the entry list and initialized as a message and posted on the
arena’s message queue.

*note .scan.finalize.idempotent;: 11d4. In fact this will only happen if
the guardian has not already been finalized (which is determined by
examining the state of the guardian).

*note .scan.unordered;: 11d5. Because scanning occurs a segment at a
time, the order in which objects are finalized is “random” (it cannot be
predicted by considering only the references between objects registered
for finalization).  See analysis.mps.poolmrg.improve.semantics for how
this can be improved.

*note .scan.unordered.justify;: 11d6. Unordered finalization is all that
is required.

See analysis.mps.poolmrg.improve.scan.nomove for a suggested improvement
that avoids redundant unlinking and relinking.

 -- C Function: *note Res: 55f. MRGDescribe (Pool pool, mps_lib_FILE
          *stream, Count depth)

*note .describe;: 11d8. Describes an MRG pool.  Iterates along each of
the entry and exit lists and prints the guardians in each.  The location
of the guardian and the value of the reference in it will be printed
out.  Provided for debugging only.


File: MemoryPoolSystem.info,  Node: Transgressions,  Next: Future<3>,  Prev: Functions<10>,  Up: MRG pool class

5.20.9 Transgressions
---------------------

*note .trans.no-finish;: 11da. The MRG pool does not trouble itself to
tidy up its internal rings properly when being destroyed.

*note .trans.free-seg;: 11db. No attempt is made to release free
segments to the arena.  A suggested strategy for this is as follows:

   - Add a count of free guardians to each segment, and maintain it in
     appropriate places.

   - Add a free segment ring to the pool.

   - In *note mrgRefSegScan(): 11cd, if the segment is entirely free,
     don’t scan it, but instead detach its links from the free ring, and
     move the segment to the free segment ring.

   - At some appropriate point, such as the end of ‘MRGAlloc()’, destroy
     free segments.

   - In ‘MRGAlloc()’, if there are no free guardians, check the free
     segment ring before creating a new pair of segments.  Note that
     this algorithm would give some slight measure of segment
     hysteresis.  It is not the place of the pool to support general
     segment hysteresis.


File: MemoryPoolSystem.info,  Node: Future<3>,  Next: Tests<2>,  Prev: Transgressions,  Up: MRG pool class

5.20.10 Future
--------------

*note .future.array;: 11dd. In future, for speed or simplicity, this
pool could be rewritten to use an array.  See
mail.gavinm.1997-09-04.13-08(1).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1997/09/04/13-08/0.txt


File: MemoryPoolSystem.info,  Node: Tests<2>,  Next: Notes<8>,  Prev: Future<3>,  Up: MRG pool class

5.20.11 Tests
-------------

     Note: This section is utterly out of date.  Pekka P. Pirinen,
     1997-09-19.

*note .test;: 11e0. The test impl.c.finalcv is similar to the weakness
test (see design.mps.weakness, impl.c.weakcv).

* Menu:

* Functionality::
* Attributes::
* Implementation: Implementation<24>.


File: MemoryPoolSystem.info,  Node: Functionality,  Next: Attributes,  Up: Tests<2>

5.20.11.1 Functionality
.......................

This is the functionality to be tested:

   - *note .fun.alloc;: 11e2. Can allocate objects.

   - *note .fun.free;: 11e3. Can free objects that were allocated.

   - *note .prot.write;: 11e4. Can write a reference into an allocated
     object.

   - *note .prot.read;: 11e5. Can read the reference from an allocated
     object.

   - *note .promise.faithful;: 11e6. A reference stored in an allocated
     object will continue to refer to the same object.

   - *note .promise.live;: 11e7. A reference stored in an allocated
     object will preserve the object referred to.

   - *note .promise.unreachable;: 11e8. Any objects referred to in
     finalization messages are not (at the time of reading the message)
     reachable via a chain of ambiguous or exact references.  (we will
     not be able to test this at first as there is no messaging
     interface)

   - *note .promise.try;: 11e9. The pool will make a “good faith” effort
     to finalize objects that are not reachable via a chain of ambiguous
     or exact references.


File: MemoryPoolSystem.info,  Node: Attributes,  Next: Implementation<24>,  Prev: Functionality,  Up: Tests<2>

5.20.11.2 Attributes
....................

The following attributes will be tested:

   - *note .attr.none;: 11eb. There are no attribute requirements.


File: MemoryPoolSystem.info,  Node: Implementation<24>,  Prev: Attributes,  Up: Tests<2>

5.20.11.3 Implementation
........................

The test will simply allocate a number of objects in the AMC pool and
finalize each one, throwing away the reference to the objects.  Churn.

*note .test.mpm;: 11ed. The test will use the MPM interface
(impl.h.mpm).

*note .test.mpm.justify;: 11ee. This is because it is not intended to
provide an MPS interface to this pool directly, and the MPS interface to
finalization has not been written yet (impl.h.mps).

*note .test.mpm.change;: 11ef. Later on it may use the MPS interface, in
which case, where the following text refers to allocating objects in the
MRG pool it will need adjusting.

*note .test.two-pools;: 11f0. The test will use two pools, an AMC pool,
and an MRG pool.

*note .test.alloc;: 11f1. A number of objects will be allocated in the
MRG pool.

*note .test.free;: 11f2. They will then be freed.  This will test *note
.fun.alloc: 11e2. and *note .fun.free: 11e3, although not very much.

*note .test.rw.a;: 11f3. An object, “A”, will be allocated in the AMC
pool, a reference to it will be kept in a root.

*note .test.rw.alloc;: 11f4. A number of objects will be allocated in
the MRG pool.

*note .test.rw.write;: 11f5. A reference to “A” will be written into
each object.

*note .test.rw.read;: 11f6. The reference in each object will be read
and checked to see if it refers to “A”.

*note .test.rw.free;: 11f7. All the objects will be freed.

*note .test.rw.drop;: 11f8. The reference to “A” will be dropped.  This
will test *note .prot.write: 11e4. and *note .prot.read: 11e5.

*note .test.promise.fl.alloc;: 11f9. A number of objects will be
allocated in the AMC pool.

*note .test.promise.fl.tag;: 11fa. Each object will be tagged uniquely.

*note .test.promise.fl.refer;: 11fb. a reference to it will be stored in
an object allocated in the MRG pool.

*note .test.promise.fl.churn;: 11fc. A large amount of garbage will be
allocated in the AMC pool.  Regularly, whilst this garbage is being
allocated, a check will be performed that all the objects allocated in
the MRG pool refer to valid objects and that they still refer to the
same objects.  All objects from the MRG pool will then be freed (thus
dropping all references to the AMC objects).  This will test *note
.promise.faithful: 11e6. and *note .promise.live: 11e7.

*note .test.promise.ut.alloc;: 11fd. A number of objects will be
allocated in the AMC pool.

*note .test.promise.ut.refer;: 11fe. Each object will be referred to by
a root and also referred to by an object allocated in the MRG pool.

*note .test.promise.ut.drop;: 11ff. References to a random selection of
the objects from the AMC pool will be deleted from the root.

*note .test.promise.ut.churn;: 1200. A large amount of garbage will be
allocated in the AMC pool.

*note .test.promise.ut.message;: 1201. The message interface will be
used to receive finalization messages.

*note .test.promise.ut.final.check;: 1202. For each finalization message
received it will check that the object referenced in the message is not
referred to in the root.

*note .test.promise.ut.nofinal.check;: 1203. After some amount of
garbage has been allocated it will check to see if any objects are not
in the root and haven’t been finalized.  This will test *note
.promise.unreachable: 11e8. and *note .promise.try: 11e9.


File: MemoryPoolSystem.info,  Node: Notes<8>,  Prev: Tests<2>,  Up: MRG pool class

5.20.12 Notes
-------------

*note .access.inadequate;: 1205. ‘SegAccess()’ will scan segments at
‘RankEXACT'’. Really it should be scanned at whatever the minimum rank
of all grey segments is (the trace rank phase), however there is no way
to find this out.  As a consequence we will sometimes scan pages at
‘RankEXACT’ when the pages could have been scanned at ‘RankFINAL’.  This
means that finalization of some objects may sometimes get delayed.


File: MemoryPoolSystem.info,  Node: Manual Variable Temporal MVT pool design,  Next: MVFF pool class,  Prev: MRG pool class,  Up: Old design

5.21 Manual Variable Temporal (MVT) pool design
===============================================

* Menu:

* Introduction: Introduction<67>.
* Definitions: Definitions<13>.
* Abbreviations::
* Overview: Overview<28>.
* Requirements: Requirements<42>.
* Architecture: Architecture<10>.
* Analysis: Analysis<5>.
* Ideas::
* Implementation: Implementation<25>.
* Testing: Testing<9>.
* Text::


File: MemoryPoolSystem.info,  Node: Introduction<67>,  Next: Definitions<13>,  Up: Manual Variable Temporal MVT pool design

5.21.1 Introduction
-------------------

*note .intro;: 120b. This is a second-generation design for a pool that
manually manages variable-sized objects.  It is intended as a
replacement for poolmv (except in its control pool role) and poolepdl,
and it is intended to satisfy the requirements of the Dylan “misc” pool
and the product malloc/new drop-in replacement.

*note .readership;: 120c. MM developers

*note .source;: 120d. req.dylan(6), req.epcore(16), req.product(2)

*note .background;: 120e. design.mps.poolmv, design.mps.poolepdl(0),
design.product.soft.drop(0), paper.wil95(1), paper.vo96(0),
paper.grun92(1), paper.beck82(0), mail.ptw.1998-02-25.22-18(1).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/02/25/22-18/0.txt


File: MemoryPoolSystem.info,  Node: Definitions<13>,  Next: Abbreviations,  Prev: Introduction<67>,  Up: Manual Variable Temporal MVT pool design

5.21.2 Definitions
------------------

*note .def.alignment;: 1211. Alignment is a constraint on an object’s
address, typically to be a power of 2 (see also, glossary.alignment )

*note .def.bit-map;: 1212. A bitmap is a boolean-valued vector (see
also, glossary.bitmap ).

*note .def.block;: 1213. A block is a contiguous extent of memory.  In
this document, block is used to mean a contiguous extent of memory
managed by the pool for the pool client, typically a subset of a segment
(compare with *note .def.segment: 1214.).

*note .def.cartesian-tree;: 1215. A cartesian tree is a binary tree
ordered by two keys (paper.stephenson83(0)).

*note .def.crossing-map;: 1216. A mechanism that supports finding the
start of an object from any address within the object, typically only
required on untagged architectures (see also, glossary.crossing.map ).

*note .def.footer;: 1217. A block of descriptive information describing
and immediately following another block of memory (see also *note
.def.header: 1218.).

*note .def.fragmentation;: 1219. Fragmented memory is memory reserved to
the program but not usable by the program because of the arrangement of
memory already in use (see also, glossary.fragmentation ).

*note .def.header;: 1218. A block of descriptive information describing
and immediately preceding another block of memory (see also,
glossary.in-band.header ).

*note .def.in-band;: 121a. From “in band signalling”, when descriptive
information about a data structure is stored in the data structure
itself (see also, glossary.in-band.header ).

*note .def.out-of-band;: 121b. When descriptive information about a data
structure is stored separately from the structure itself (see also,
glossary.out-of-band.header ).

*note .def.refcount;: 121c. A refcount is a count of the number of users
of an object (see also, glossary.reference.count ).

*note .def.segment;: 1214. A segment is a contiguous extent of memory.
In this document, segment is used to mean a contiguous extent of memory
managed by the MPS arena (design.mps.arena(1)) and subdivided by the
pool to provide blocks (see *note .def.block: 1213.) to its clients.

*note .def.splay-tree;: 121d. A splay tree is a self-adjusting binary
tree (paper.st85(0), paper.sleator96(0)).

*note .def.splinter;: 121e. A splinter is a fragment of memory that is
too small to be useful (see also, glossary.splinter )

*note .def.subblock;: 121f. A subblock is a contiguous extent of memory.
In this document, subblock is used to mean a contiguous extent of memory
manage by the client for its own use, typically a subset of a block
(compare with *note .def.block: 1213.).

   ---------- Footnotes ----------

   (1) arena.html


File: MemoryPoolSystem.info,  Node: Abbreviations,  Next: Overview<28>,  Prev: Definitions<13>,  Up: Manual Variable Temporal MVT pool design

5.21.3 Abbreviations
--------------------

*note .abbr.abq;: 1221. ABQ = Available Block Queue

*note .abbr.ap;: 1222. AP = Allocation Point

*note .abbr.cbs;: 1223. CBS = Coalescing Block Structure

*note .abbr.mps;: 1224. MPS = Memory Pool System

*note .abbr.ps;: 1225. PS = PostScript


File: MemoryPoolSystem.info,  Node: Overview<28>,  Next: Requirements<42>,  Prev: Abbreviations,  Up: Manual Variable Temporal MVT pool design

5.21.4 Overview
---------------

*note .overview;: 1227. MVT is intended to satisfy the requirements of
the clients that need manual-variable pools, improving on the
performance of the existing manual-variable pool implementations, and
reducing the duplication of code that currently exists.  The expected
clients of MVT are: Dylan (currently for its misc pool), EP
(particularly the dl pool, but all pools other than the PS object pool),
and Product (initially the malloc/new pool, but also other manual pool
classes).


File: MemoryPoolSystem.info,  Node: Requirements<42>,  Next: Architecture<10>,  Prev: Overview<28>,  Up: Manual Variable Temporal MVT pool design

5.21.5 Requirements
-------------------

*note .req.cat;: 1229. Requirements are categorized per guide.req(2).

*note .req.risk;: 122a. req.epcore(16) is known to be obsolete, but the
revised document has not yet been accepted.

* Menu:

* Critical requirements::
* Essential requirements::
* Nice requirements::


File: MemoryPoolSystem.info,  Node: Critical requirements,  Next: Essential requirements,  Up: Requirements<42>

5.21.5.1 Critical requirements
..............................

*note .req.fun.man-var;: 122c. The pool class must support manual
allocation and freeing of variable-sized blocks (source:
req.dylan.fun.misc.alloc,
req.epcore.fun.{dl,gen,tmp,stat,cache,trap}.{alloc,free},
req.product.fun.{malloc,new,man.man}).

*note .non-req.fun.gc;: 122d. There is not a requirement that the pool
class support formatted objects, scanning, or collection objects; but it
should not be arbitrarily precluded.

*note .req.fun.align;: 122e. The pool class must support aligned
allocations to client-specified alignments.  An individual instance need
only support a single alignment; multiple instances may be used to
support more than one alignment (source: req.epcore.attr.align).

*note .req.fun.reallocate;: 122f. The pool class must support resizing
of allocated blocks (source req.epcore.fun.dl.promise.free,
req.product.dc.env.{ansi-c,cpp}).

*note .non-req.fun.reallocate.in-place;: 1230. There is not a
requirement blocks must be resized in place (where possible); but it
seems like a good idea.

*note .req.fun.thread;: 1231. Each instance of the pool class must
support multiple threads of allocation (source req.epcore.fun.dl.multi,
req.product.dc.env.{ansi-c,cpp}).

*note .req.attr.performance;: 1232. The pool class must meet or exceed
performance of “competitive” allocators (source:
rec.epcore.attr.{run-time,tp}, req.product.attr.{mkt.eval, perform}).
[Dylan does not seem to have any requirement that storage be allocated
with a particular response time or throughput, just so long as we don’t
block for too long.  Clearly there is a missing requirement.]

*note .req.attr.performance.time;: 1233. By inference, the time overhead
must be competitive.

*note .req.attr.performance.space;: 1234. By inference, the space
overhead must be competitive.

*note .req.attr.reliability;: 1235. The pool class must have “rock-solid
reliability” (source: req.dylan.attr.rel.mtbf, req.epcore.attr.rel,
req.product.attr.rel).

*note .req.fun.range;: 1236. The pool class must be able to manage
blocks ranging in size from 1 byte to all of addressable memory
(req.epcore.attr.{dl,gen,tmp,stat,cache,trap}.obj.{min,max}.  The range
requirement may be satisfied by multiple instances each managing a
particular client-specified subrange of sizes.  [Dylan has requirements
req.dylan.attr.{capacity,obj.max}, but no requirement that such objects
reside in a manual pool.]

*note .req.fun.debug;: 1237. The pool class must support debugging
erroneous usage by client programs (source: req.epcore.fun.{dc.variety,
debug.support}, req.product.attr.{mkt.eval,perform}).  Debugging is
permitted to incur additional overhead.

*note .req.fun.debug.boundaries;: 1238. The pool class must support
checking for accesses outside the boundaries of live objects.

*note .req.fun.debug.log;: 1239. The pool class must support logging of
all allocations and deallocations.

*note .req.fun.debug.enumerate;: 123a. The pool class must support
examining all allocated objects.

*note .req.fun.debug.free;: 123b. The pool class must support detecting
incorrect, overlapping, and double frees.

*note .req.fun.tolerant;: 123c. The pool class must support tolerance of
erroneous usage (source req.product.attr.use.level.1).


File: MemoryPoolSystem.info,  Node: Essential requirements,  Next: Nice requirements,  Prev: Critical requirements,  Up: Requirements<42>

5.21.5.2 Essential requirements
...............................

*note .req.fun.profile;: 123e. The pool class should support memory
usage profiling (source: req.product.attr.{mkt.eval, perform}).

*note .req.attr.flex;: 123f. The pool class should be flexible so that
it can be tuned to specific allocation and freeing patterns (source:
req.product.attr.flex,req.epcore.attr.{dl,cache,trap}.typ).  The
flexibility requirement may be satisfied by multiple instances each
optimizing a specific pattern.

*note .req.attr.adapt;: 1240. The pool class should be adaptive so that
it can accommodate changing allocation and freeing patterns (source:
req.epcore.fun.{tmp,stat}.policy, req.product.attr.{mkt.eval,perform}).


File: MemoryPoolSystem.info,  Node: Nice requirements,  Prev: Essential requirements,  Up: Requirements<42>

5.21.5.3 Nice requirements
..........................

*note .req.fun.suballocate;: 1242. The pool class may support freeing of
any aligned, contiguous subset of an allocated block (source
req.epcore.fun.dl.free.any, req.product.attr.{mkt.eval,perform}).


File: MemoryPoolSystem.info,  Node: Architecture<10>,  Next: Analysis<5>,  Prev: Requirements<42>,  Up: Manual Variable Temporal MVT pool design

5.21.6 Architecture
-------------------

*note .arch.overview;: 1244. The pool has several layers: client
allocation is by Allocation Points (APs).

*note .arch.overview.ap;: 1245. APs acquire storage from the pool
available-block queue (ABQ).

*note .arch.overview.abq;: 1246. The ABQ holds blocks of a minimum
configurable size: “reuse size”.

*note .arch.overview.storage;: 1247. The ABQ acquires storage from the
arena, and from its internal free block managers.

*note .arch.overview.storage.contiguous;: 1248. The arena storage is
requested to be contiguous to maximize opportunities for coalescing
(Loci will be used when available).

*note .arch.overview.cbs;: 1249. The free block managers hold blocks
freed by the client until, through coalescing, they have reached the
reuse size, at which point they are made available on the ABQ.

*note .arch.ap;: 124a. The pool will use allocation points as the
allocation interface to the client.

*note .arch.ap.two-phase;: 124b. Allocation points will request blocks
from the pool and suballocate those blocks (using the existing AP,
compare and increment, 2-phase mechanism) to satisfy client requests.

*note .arch.ap.fill;: 124c. The pool will have a configurable “fill
size” that will be the preferred size block used to fill the allocation
point.

*note .arch.ap.fill.size;: 124d. The fill size should be chosen to
amortize the cost of refill over a number of typical reserve/commit
operations, but not so large as to exceed the typical object population
of the pool.

*note .arch.ap.no-fit;: 124e. When an allocation does not fit in the
remaining space of the allocation point, there may be a remaining
fragment.

*note .arch.ap.no-fit.sawdust;: 124f. If the fragment is below a
configurable threshold (minimum size), it will be left unused (but
returned to the free block managers so it will be reclaimed when
adjacent objects are freed).

*note .arch.ap.no-fit.splinter;: 1250. otherwise, the remaining fragment
will be (effectively) returned to the head of the available-block queue,
so that it will be used as soon as possible (that is, by objects of
similar birthdate).

*note .arch.ap.no-fit.oversize;: 1251. If the requested allocation
exceeds the fill size it is treated exceptionally (this may indicate the
client has either misconfigured or misused the pool and should either
change the pool configuration or create a separate pool for these
exceptional objects for best performance).

*note .arch.ap.no-fit.oversize.policy;: 1252. Oversize blocks are
assumed to have exceptional lifetimes, hence are allocated to one side
and do not participate in the normal storage recycling of the pool.

*note .arch.ap.refill.overhead;: 1253. If reuse size is small, or
becomes small due to *note .arch.adapt: 1254, all allocations will
effectively be treated exceptionally (the AP will trip and a oldest-fit
block will be chosen on each allocation).  This mode will be within a
constant factor in overhead of an unbuffered pool.

*note .arch.abq;: 1255. The available block queue holds blocks that have
coalesced sufficiently to reach reuse size.

*note .arch.abq.reuse.size;: 1256. A multiple of the quantum of virtual
memory is used as the reuse size (*note .analysis.policy.size: 1257.).

*note .arch.abq.fifo;: 1258. It is a FIFO queue (recently coalesced
blocks go to the tail of the queue, blocks are taken from the head of
the queue for reuse).

*note .arch.abq.delay-reuse;: 1259. By thus delaying reuse, coalescing
opportunities are greater.

*note .arch.abq.high-water;: 125a. It has a configurable high water
mark, which when reached will cause blocks at the head of the queue to
be returned to the arena, rather than reused.

*note .arch.abq.return;: 125b. When the MPS supports it, the pool will
be able to return free blocks from the ABQ to the arena on demand.

*note .arch.abq.return.segment;: 125c. *note .arch.abq.return: 125b. can
be guaranteed to be able to return a segment by setting reuse size to
twice the size of the segments the pool requests from the arena.

*note .arch.cbs;: 125d. The coalescing block structure holds blocks that
have been freed by the client.

*note .arch.cbs.optimize;: 125e. The data structure is optimized for
coalescing.

*note .arch.cbs.abq;: 125f. When a block reaches reuse size, it is added
to the ABQ.

*note .arch.cbs.data-structure;: 1260. The data structures are organized
so that a block can be both in the free block managers and on the ABQ
simultaneously to permit additional coalescing, up until the time the
block is removed from the ABQ and assigned to an AP.

*note .arch.fragmentation.internal;: 1261. Internal fragmentation
results from The pool will request large segments from the arena to
minimize the internal fragmentation due to objects not crossing segment
boundaries.

*note .arch.modular;: 1262. The architecture will be modular, to allow
building variations on the pool by assembling different parts.

*note .arch.modular.example;: 1263. For example, it should be possible
to build pools with any of the freelist mechanisms, with in-band or
out-of-band storage (where applicable), that do or do not support
derived object descriptions, etc.

*note .arch.modular.initial;: 1264. The initial architecture will use
*note .sol.mech.free-list: 1265. for the free block managers, *note
.sol.mech.storage.out-of-band: 1266, *note .sol.mech.desc.derived: 1267,
and *note .sol.mech.allocate.buffer: 1268.

*note .arch.segregate;: 1269. The architecture will support segregated
allocation through the use of multiple allocation points.  The client
will choose the appropriate allocation point either at run time, or when
possible, at compile time.

*note .arch.segregate.initial;: 126a. The initial architecture will
segregate allocations into two classes: large and small.  This will be
implemented by creating two pools with different parameters.

*note .arch.segregate.initial.choice;: 126b. The initial architecture
will provide glue code to choose which pool to allocate from at run
time.  If possible this glue code will be written in a way that a good
compiler can optimize the selection of pool at compile time.  Eventually
this glue code should be subsumed by the client or generated
automatically by a tool.

*note .arch.debug;: 126c. Debugging features such as tags, fenceposts,
types, creators will be implemented in a layer above the pool and APs.
A generic pool debugging interface will be developed to support
debugging in this outer layer.

*note .arch.debug.initial;: 126d. The initial architecture will have
counters for objects/bytes allocated/freed and support for detecting
overlapping frees.

*note .arch.dependency.loci;: 126e. The architecture depends on the
arena being able to efficiently provide segments of varying sizes
without excessive fragmentation.  The locus mechanism should satisfy
this dependency.  (See *note .analysis.strategy.risk: 126f.)

*note .arch.dependency.mfs;: 1270. The architecture internal data
structures depend on efficient manual management of small, fixed-sized
objects (2 different sizes).  The MFS pool should satisfy this
dependency.

*note .arch.contingency;: 1271. Since the strategy we propose is new, it
may not work.

*note .arch.contingency.pathological;: 1272. In particular, pathological
allocation patterns could result in fragmentation such that no blocks
recycle from the free bock managers to the ABQ.

*note .arch.contingency.fallback;: 1273. As a fallback, there will be a
pool creation parameter for a high water mark for the free space.

*note .arch.contingency.fragmentation-limit;: 1274. When the free space
as a percentage of all the memory managed by the pool (a measure of
fragmentation) reaches that high water mark, the free block managers
will be searched oldest-fit before requesting additional segments from
the arena.

*note .arch.contingency.alternative;: 1275. We also plan to implement
*note .sol.mech.free-list.cartesian-tree: 1276. as an alternative free
block manager, which would permit more efficient searching of the free
blocks.

*note .arch.parameters;: 1277. The architecture supports several
parameters so that multiple pools may be instantiated and tuned to
support different object cohorts.  The important parameters are: reuse
size, minimum size, fill size, ABQ high water mark, free block
fragmentation limit (see *note .arch.contingency.fragmentation-limit:
1274.).

*note .arch.parameters.client-visible;: 1278. The client-visible
parameters of the pool are the minimum object size, the mean object
size, the maximum object size, the reserve depth and fragmentation
limit.  The minimum object size determines when a splinter is kept on
the head of the ABQ (*note .arch.ap.no-fit.splinter: 1250.).  The
maximum object size determines the fill size (*note .arch.ap.fill.size:
124d.) and hence when a block is allocated exceptionally (*note
.arch.ap.no-fit.oversize: 1251.).  The mean object size is the most
likely object size.  The reserve depth is a measure of the hysteresis of
the object population.  The mean object size, reserve depth and, maximum
object size are used to determine the size of the ABQ (*note
.arch.abq.high-water: 125a.).  The fragmentation limit is used to
determine when contingency mode is used to satisfy an allocation request
(*note .arch.contingency: 1271.).

*note .arch.adapt;: 1254. We believe that an important adaptation to
explore is tying the reuse size inversely to the fragmentation (as
measured in *note .arch.contingency.fragmentation-limit: 1274.).

*note .arch.adapt.reuse;: 1279. By setting reuse size low when
fragmentation is high, smaller blocks will be available for reuse, so
fragmentation should diminish.

*note .arch.adapt.overhead;: 127a. This will result in higher overhead
as the AP will need to be refilled more often, so reuse size should be
raised again as fragmentation diminishes.

*note .arch.adapt.oldest-fit;: 127b. In the limit, if reuse size goes to
zero, the pool will implement a “oldest-fit” policy: the oldest free
block of sufficient size will be used for each allocation.

*note .arch.adapt.risk;: 127c. This adaptation is an experimental policy
and should not be delivered to clients until thoroughly tested.


File: MemoryPoolSystem.info,  Node: Analysis<5>,  Next: Ideas,  Prev: Architecture<10>,  Up: Manual Variable Temporal MVT pool design

5.21.7 Analysis
---------------

*note .analysis.discard;: 127e. We have discarded many traditional
solutions based on experience and analysis in paper.wil95(1).  In
particular, managing the free list as a linear list arranged by address
or size and basing policy on searching such a linear list in a
particular direction, from a particular starting point, using fit and/or
immediacy as criteria.  We believe that none of these solutions is
derived from considering the root of the problem to be solved (as
described in *note .analysis.strategy: 127f.), although their behavior
as analyzed by Wilson gives several insights.

*note .analysis.strategy;: 127f. For any program to run in the minimum
required memory (with minimal overhead – we discard solutions such as
compression for now), fragmentation must be eliminated.  To eliminate
fragmentation, simply place blocks in memory so that they die “in order”
and can be immediately coalesced.  This ideal is not achievable, but we
believe we can find object attributes that correlate with deathtime and
exploit them to approximate the ideal.  Initially we believe birth time
and type (as approximated by size) will be useful attributes to explore.

*note .analysis.strategy.perform;: 1280. To meet *note
.req.attr.performance: 1232, the implementation of *note .sol.strategy:
1281. must be competitive in both time and space.

*note .analysis.strategy.risk;: 126f. The current MPS segment substrate
can cause internal fragmentation which an individual pool can do nothing
about.  We expect that request.epcore.170193.sugg.loci(1) will be
implemented to remove this risk.

*note .analysis.policy;: 1282. Deferred coalescing, when taken to the
extreme will not minimize the memory consumption of a program, as no
memory would ever be reused.  Eager reuse appears to lead to more
fragmentation, whereas delayed reuse appears to reduce fragmentation
(paper.wil95(1)).  The systems studied by Wilson did not directly
address deferring reuse.  Our proposed policy is to reuse blocks when
they reach a (configurable) size.  We believe that this policy along
with the policy of segregating allocations by death time, will greatly
reduce fragmentation.

*note .analysis.policy.risk;: 1283. This policy could lead to
pathological behavior if allocations cannot be successfully segregated.

*note .analysis.policy.allocate.segregate;: 1284. This policy has some
similarities to CustomAlloc (paper.grun92(1)).  CustomAlloc segregates
objects by size classes, and then within those classes chooses a
different allocator depending on whether that size class has a stable or
unstable population.  Classes with stable population recycle storage
within the class, whereas classes with unstable populations return their
storage to the general allocation pool for possible reuse by another
class.  CustomAlloc, however, requires profiling the application and
tuning the allocator according to those profiles.  Although we intend to
support such tuning, we do not want to require it.

*note .analysis.policy.reallocate;: 1285. For reallocation, *note
.req.fun.suballocate: 1242. can be used to free the remainder if a block
is made smaller.  Doing so will cause the freed block to obey *note
.sol.policy.allocate: 1286. (that is, the freed block will not be
treated specially, it will be subject to the normal policy on reuse).
Copying can be used if a block is made larger.  paper.vo96(0) reports
success in over-allocating a block the first time it is resized larger,
presumably because blocks that are resized once tend to be resized again
and over-allocating may avoid a subsequent copy.  If each object that
will be reallocated can be given its own allocation point until its
final reallocation, the allocation point can be used to hold released or
spare storage.

*note .analysis.policy.size;: 1257. We believe that this will take
advantage of the underlying virtual memory system’s ability to compact
the physical memory footprint of the program by discarding free
fragments that align with the virtual memory quantum.  (In a VM system
one can approximate compaction by sparse mapping.  If every other page
of a segment is unused, the unused pages can be unmapped, freeing up
physical memory that can be mapped to a new contiguous vm range.)

*note .analysis.mech.free-list;: 1287. The literature (paper.grun92(1),
paper.vo96(0)) indicate that *note .sol.mech.free-list.cartesian-tree:
1276. provides a space-efficient implementation at some cost in speed.
*note .sol.mech.free-list.splay-tree: 1288. is faster but less
space-efficient.  *note .sol.mech.free-list.bitmap: 1289. is unstudied.
Many of the faster allocators maintain caches of free blocks by size to
speed allocation of “popular” sizes.  We intend to initially explore not
doing so, as we believe that policy ultimately leads to fragmentation by
mixing objects of varying death times.  Instead we intend to use a free
list mechanism to support fast coalescing, deferring reuse of blocks
until a minimum size has been reached.

*note .analysis.mech.allocate.optimize-small;: 128a. Wilson
(paper.wil95(1)) notes that small blocks typically have short lifetimes
and that overall performance is improved if you optimize the management
of small blocks, e.g., *note .sol.mech.allocate.lookup-table: 128b. for
all small blocks.  We believe that *note .sol.mech.allocate.buffer:
1268. does exactly that.

*note .analysis.mech.allocate.optimize-new;: 128c. Wilson
(paper.wil95(1)) reports some benefit from “preserving wilderness”, that
is, when a block of memory must be requested from the system to satisfy
an allocation, only the minimum amount of that block is used, the
remainder is preserved (effectively by putting it at the tail of the
free list).  This mechanism may or may not implement *note
.sol.policy.allocate: 1286.  We believe a better mechanism is to choose
to preserve or not, based on *note .sol.policy.allocate: 1286.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/epcore/170193/


File: MemoryPoolSystem.info,  Node: Ideas,  Next: Implementation<25>,  Prev: Analysis<5>,  Up: Manual Variable Temporal MVT pool design

5.21.8 Ideas
------------

*note .sol;: 128e. Many solution ideas for manual management of
variable-sized memory blocks are enumerated by paper.wil95(1).  Here we
list the most promising, and some of our own.

* Menu:

* Strategy::
* Policy::
* Mechanism: Mechanism<2>.


File: MemoryPoolSystem.info,  Node: Strategy,  Next: Policy,  Up: Ideas

5.21.8.1 Strategy
.................

*note .sol.strategy;: 1281. To run a program in the minimal required
memory, with minimal overhead, utilize memory efficiently.  Memory
becomes unusable when fragmented.  Strategy is to minimize
fragmentation.  So place blocks where they won’t cause fragmentation
later.

*note .sol.strategy.death;: 1290. Objects that will die together (in
time) should be allocated together (in space); thus they will coalesce,
reducing fragmentation.

*note .sol.strategy.death.birth;: 1291. Assume objects allocated near
each other in time will have similar deathtimes (paper.beck82(0)).

*note .sol.strategy.death.type;: 1292. Assume objects of different type
may have different deathtimes, even if born together.

*note .sol.strategy.death.predict;: 1293. Find and use program features
to predict deathtimes.

*note .sol.strategy.reallocate;: 1294. Reallocation implies rebirth, or
at least a change in lifetime

*note .sol.strategy.debug;: 1295. As much of the debugging functionality
as possible should be implemented as a generally available MPS utility;
the pool will provide support for debugging that would be expensive or
impossible to allocate outside the pool.


File: MemoryPoolSystem.info,  Node: Policy,  Next: Mechanism<2>,  Prev: Strategy,  Up: Ideas

5.21.8.2 Policy
...............

Policy is an implementable decision procedure, hopefully approximating
the strategy.

*note .sol.policy.reuse;: 1297. Defer reusing blocks, to encourage
coalescing.

*note .sol.policy.split;: 1298. When a block is split to satisfy an
allocation, use the remainder as soon as possible.

*note .sol.policy.size;: 1299. Prevent *note .sol.policy.reuse: 1297.
from consuming all of memory by choosing a (coalesced) block for reuse
when it reaches a minimum size.

*note .sol.policy.size.fixed;: 129a. Use the quantum of virtual memory
(e.g., one page) as minimum size.

*note .sol.policy.size.tune;: 129b. Allow tuning minimum size.

*note .sol.policy.size.adapt;: 129c. Adaptively change minimum size.

*note .sol.policy.allocate;: 1286. Allocate objects with similar
birthdate and lifetime together.

*note .sol.policy.allocate.segregate;: 129d. Segregate allocations by
type.

*note .sol.policy.allocate.segregate.size;: 129e. Use size as a
substitute for type.

*note .sol.policy.allocate.segregate.tune;: 129f. Permit tuning of
segregation.

*note .sol.policy.allocate.segregate.adapt;: 12a0. Adaptively segregate
allocations.

*note .sol.policy.reallocate;: 12a1. Implement reallocation in a central
mechanism outside of the pool, create a generic pool interface in
support of same.

*note .sol.policy.debug;: 12a2. Implement a pool debugging interface.

*note .sol.policy.debug.counters;: 12a3. Implement debugging counters in
the pool that are queried with a generic interface.

*note .sol.policy.debug.verify;: 12a4. Implement debugging error returns
on overlapping frees.


File: MemoryPoolSystem.info,  Node: Mechanism<2>,  Prev: Policy,  Up: Ideas

5.21.8.3 Mechanism
..................

Mechanisms are algorithms or data structures used to implement policy.

*note .sol.mech.free-list;: 1265. Mechanisms that can be used to
describe the free list.

*note .sol.mech.free-list.cartesian-tree;: 1276. Using address and size
as keys supports fast coalescing of adjacent blocks and fast searching
for optimal-sized blocks.  Unfortunately, because the shape of the tree
is constrained by the second key, it can become unbalanced.  This data
structure is used in the SunOS 4.1 malloc (paper.grun92(1)).

*note .sol.mech.free-list.splay-tree;: 1288. The amortized cost of a
splay tree is competitive with balanced binary trees in the worst case,
but can be significantly better for regular patterns of access because
recently-accessed keys are moved to the root of the tree and hence can
be re-accessed quickly.  This data structure is used in the System Vr4
malloc (paper.vo96(0)).  (For a complete analysis of the splay tree
algorithm time bounds see paper.st85(0).)

*note .sol.mech.free-list.bitmap;: 1289. Using address as an index and
fix-sized blocks, the booleans can represent whether a block is free or
not.  Adjacent blocks can be used to construct larger blocks.  Efficient
algorithms for searching for runs in a vector are known.  This data
structure is used in many file system disk block managers.

*note .sol.mech.free-list.refcount;: 12a6. A count of the number of
allocated but not freed subblocks of a block can be used to determine
when a block is available for reuse.  This is an extremely compact data
structure, but does not support subblock reuse.

*note .sol.mech.free-list.hybrid;: 12a7. Bitmaps appear suited
particularly to managing small, contiguous blocks.  The tree structures
appear suited particularly to managing varying-sized, discontiguous
blocks.  A refcount can be very efficient if objects can be placed
accurately according to death time.  A hybrid mechanism may offer better
performance for a wider range of situations.

*note .sol.mech.free-list.linked;: 12a8. An address-ordered singly
linked free list using space in each free block to store the block’s
size and a pointer to the next block.

*note .sol.mech.storage;: 12a9. Methods that can be used to store the
free list description.

*note .sol.mech.storage.in-band;: 12aa. The tree data structures (and
*note .sol.mech.free-list.linked: 12a8.) are amenable to being stored in
the free blocks themselves, minimizing the space overhead of management.
To do so imposes a minimum size on free blocks and reduces the locality
of the data structure.

*note .sol.mech.storage.out-of-band;: 1266. The bit-map data structure
must be stored separately.

*note .sol.mech.desc;: 12ab. for an allocated block to be freed, its
base and bound must be known

*note .sol.mech.desc.derived;: 1267. Most clients can supply the base of
the block.  Some clients can supply the bound.

*note .sol.mech.desc.in-band;: 12ac. When the bound cannot be supplied,
it can be stored as an in-band “header”.  If neither the base nor bound
can be supplied (e.g., the client may only have an interior pointer to
the block), a header and footer may be required.

*note .sol.mech.desc.out-of-band;: 12ad. In un-tagged architectures, it
may be necessary to store the header and footer out-of-band to
distinguish them from client data.  Out-of-band storage can improve
locality and reliability.  Any of the free-list structures can also be
used to describe allocated blocks out-of-band.

*note .sol.mech.desc.crossing-map;: 12ae. An alternative for untagged
architectures is to store a “crossing map” which records an encoding of
the start of objects and then store the descriptive information in-band.

*note .sol.mech.allocate;: 12af. Mechanisms that can be used to allocate
blocks (these typically sit on top of a more general free-list manager).

*note .sol.mech.allocate.lookup-table;: 128b. Use a table of popular
sizes to cache free blocks of those sizes.

*note .sol.mech.allocate.buffer;: 1268. Allocate from contiguous blocks
using compare and increment.

*note .sol.mech.allocate.optimize-small;: 12b0. Use a combination of
techniques to ensure the time spent managing a block is small relative
to the block’s lifetime; assume small blocks typically have short
lifetimes.

*note .sol.mech.allocate.optimize-new;: 12b1. When “virgin” memory is
acquired from the operating system to satisfy a request, try to preserve
it (that is, use only what is necessary).

*note .sol.mech.allocate.segregate.size;: 12b2. Use size as a substitute
for type.

*note .sol.mech.reallocate;: 12b3. use *note .req.fun.suballocate: 1242.
to return unused memory when a block shrinks, but differentiate this
from an erroneous overlapping free by using separate interfaces.


File: MemoryPoolSystem.info,  Node: Implementation<25>,  Next: Testing<9>,  Prev: Ideas,  Up: Manual Variable Temporal MVT pool design

5.21.9 Implementation
---------------------

The implementation consists of the following separable modules:

* Menu:

* Splay Tree::
* Coalescing Block Structure::
* Fail-over to address-ordered free list::
* Available Block Queue::
* Pool implementation::
* AP Dispatch::


File: MemoryPoolSystem.info,  Node: Splay Tree,  Next: Coalescing Block Structure,  Up: Implementation<25>

5.21.9.1 Splay Tree
...................

*note .impl.c.splay;: 12b6. The implementation of *note
.sol.mech.free-list.splay-tree: 1288.  See design.mps.splay(1).

   ---------- Footnotes ----------

   (1) splay.txt.html


File: MemoryPoolSystem.info,  Node: Coalescing Block Structure,  Next: Fail-over to address-ordered free list,  Prev: Splay Tree,  Up: Implementation<25>

5.21.9.2 Coalescing Block Structure
...................................

*note .impl.c.cbs;: 12b9. The initial implementation will use *note
.sol.mech.free-list.splay-tree: 1288. and *note
.sol.mech.storage.out-of-band: 1266.  For locality, this storage should
be managed as a linked free list of splay nodes suballocated from blocks
acquired from a pool shared by all CBS’s.  Must support creation and
destruction of an empty tree.  Must support search, insert and delete by
key of type Addr.  Must support finding left and right neighbors of a
failed search for a key.  Must support iterating over the elements of
the tree with reasonable efficiency.  Must support storing and
retrieving a value of type Size associated with the key.  Standard
checking and description should be provided.  See design.mps.cbs(1).

   ---------- Footnotes ----------

   (1) cbs.txt.html


File: MemoryPoolSystem.info,  Node: Fail-over to address-ordered free list,  Next: Available Block Queue,  Prev: Coalescing Block Structure,  Up: Implementation<25>

5.21.9.3 Fail-over to address-ordered free list
...............................................

*note .impl.c.freelist;: 12bc. Because the CBS uses out-of-band storage,
it may be unable to handle insert
(design.mps.cbs.function.cbs.insert.fail(1)) and delete
(design.mps.cbs.function.cbs.delete.fail(2)) operations.  When this
happen MVT fails over to an address-ordered singly linked free list.
This uses in-band storage and so cannot run out of memory, but it has
much worse performance than the CBS. Therefore MVT eagerly attempts to
flush blocks from the free list back to the CBS. See
design.mps.freelist(3) for the design and implementation of the free
list.

   ---------- Footnotes ----------

   (1) cbs.html#design.mps.cbs.function.cbs.insert.fail

   (2) cbs.html#design.mps.cbs.function.cbs.delete.fail

   (3) freelist.html


File: MemoryPoolSystem.info,  Node: Available Block Queue,  Next: Pool implementation,  Prev: Fail-over to address-ordered free list,  Up: Implementation<25>

5.21.9.4 Available Block Queue
..............................

*note .impl.c.abq;: 12bf. The initial implementation will be a queue of
fixed size (determined at pool creation time from the high water mark).
Must support creation and destruction of an empty queue.  Must support
insertion at the head or tail of the queue (failing if full), peeking at
the head of the queue, and removal of the head (failing if empty) or any
element of the queue (found by a search).  Standard checking and
description should be provided.  See design.mps.abq(1).

   ---------- Footnotes ----------

   (1) abq.txt.html


File: MemoryPoolSystem.info,  Node: Pool implementation,  Next: AP Dispatch,  Prev: Available Block Queue,  Up: Implementation<25>

5.21.9.5 Pool implementation
............................

*note .impl.c;: 12c2. The initial implementation will use the above
modules to implement a buffered pool.  Must support creation and
destruction of the pool.  Creation takes parameters: minimum size, mean
size, maximum size, reserve depth and fragmentation limit.  Minimum,
mean, and maximum size are used to calculate the internal fill and reuse
sizes.  Reserve depth and mean size are used to calculate the ABQ high
water mark.  Fragmentation limit is used to set the contingency mode.
Must support buffer initialization, filling and emptying.  Must support
freeing.  Standard checking and description should be provided.
[Eventually, it should support scanning, so it can be used with
collected pools, but no manual pool currently does.]

*note .impl.c.future;: 12c3. The implementation should not preclude
“buffered free” (mail.ptw.1997-12-05.19-07(1)) being added in the
future.

*note .impl.c.parameters;: 12c4. The pool parameters are calculated as
follows from the input parameters: minimum, mean, and maximum size are
taken directly from the parameters.

*note .impl.c.parameter.fill-size;: 12c5. The fill size is set to the
maximum size times the reciprocal of the fragmentation limit, rounded up
to the arena grain size.

*note .imple.c.parameter.reuse-size;: 12c6. The reuse size is set to
twice the fill size (see *note .arch.abq.return.segment: 125c, *note
.impl.c.free.merge.segment: 12c7.).

*note .impl.c.parameter.abq-limit;: 12c8. The ABQ high-water limit is
set to the reserve depth times the mean size (that is, the queue should
hold as many reuse blocks as would take to cover the population
hysteresis if the population consisted solely of mean-sized blocks, see
*note .arch.abq.high-water: 125a.).

*note .impl.c.parameter.avail-limit;: 12c9. The free block high-water
limit is implemented by comparing the available free space to an
“available limit”.  The available limit is updated each time a segment
is allocated from or returned to the arena by setting it to the total
size of the pool times the fragmentation limit divide vy 100 (see *note
.arch.contingency.fallback: 1273.).

*note .impl.c.ap.fill;: 12ca. An AP fill request will be handled as
follows:

   - If the request is larger than fill size, attempt to request a
     segment from the arena sufficient to satisfy the request.

   - Use any previously returned splinter (from *note .impl.c.ap.empty:
     12cb.), if large enough.

   - Attempt to retrieve a free block from the head of the ABQ (removing
     it from ABQ and the free block managers if found).

   - If above fragmentation limit, attempt to find a block in the free
     block managers, using oldest-fit search.

   - Attempt to request a segment of fill size from the arena.

   - Attempt to find a block in the free block managers, using
     oldest-fit search.

   - Otherwise, fail.

*note .impl.c.ap.empty;: 12cb. An AP empty request will be handled as
follows:

   - If remaining free is less than min size, return it to the free
     block managers.

   - If the remaining free is larger than any previous splinter, return
     that splinter to the free block managers and save this one for use
     by a subsequent fill.

   - Otherwise return the remaining block to the free block managers.

*note .impl.c.free;: 12cc. When blocks are returned to the free block
managers they may be merged with adjacent blocks.  If a merge occurs
with a block on the ABQ, the ABQ must be adjusted to reflect the merge.

*note .impl.c.free.exception;: 12cd. Exceptional blocks are returned
directly to the arena.

*note .impl.c.free.merge;: 12ce. If a merge occurs and the merged block
is larger than reuse size:

   - If the ABQ is full, remove the block at the head of the ABQ from
     the ABQ and the free block managers and return it to the arena(*).

   - Insert the newly merged block at the tail of the ABQ, leaving it in
     the free block managers for further merging.

*note .impl.c.free.merge.segment;: 12c7. (*) Merged blocks may not align
with arena segments.  If necessary, return the interior segments of a
block to the arena and return the splinters to the free block managers.

*note .impl.c.free.merge.segment.reuse;: 12cf. If the reuse size (the
size at which blocks recycle from the free block managers to the ABQ) is
at least twice the fill size (the size of segments the pool allocates
from the arena), we can guarantee that there will always be a returnable
segment in every ABQ block.

*note .impl.c.free.merge.segment.overflow;: 12d0. If the reuse size is
set smaller (see *note .arch.adapt: 1254.), there may not be a
returnable segment in an ABQ block, in which case the ABQ has
“overflowed”.  Whenever this occurs, the ABQ will be refilled by
searching the free block managers for dropped reusable blocks when
needed.

*note .impl.c.free.merge.segment.risk;: 12d1. The current segment
structure does not really support what we would like to do.  Loci should
do better: support reserving contiguous address space and
mapping/unmapping any portion of that address space.

*note .impl.c.free.merge.alternative;: 12d2. Alternatively, if the MPS
segment substrate permitted mapping/unmapping of pages, the pool could
use very large segments and map/unmap pages as needed.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1997/12/05/19-07/0.txt


File: MemoryPoolSystem.info,  Node: AP Dispatch,  Prev: Pool implementation,  Up: Implementation<25>

5.21.9.6 AP Dispatch
....................

*note .impl.c.multiap;: 12d4. The initial implementation will be a glue
layer that selects among several AP’s for allocation according to the
predicted deathtime (as approximated by size) of the requested
allocation.  Each AP will be filled from a pool instance tuned to the
range of object sizes expected to be allocated from that AP. [For bonus
points provide an interface that creates a batch of pools and AP’s
according to some set of expected object sizes.  Eventually expand to
understand object lifetimes and general lifetime prediction keys.]

*note .impl.c.multiap.sample-code;: 12d5. This glue code is not properly
part of the pool or MPS interface.  It is a layer on top of the MPS
interface, intended as sample code for unsophisticated clients.
Sophisticated clients will likely want to choose among multiple AP’s
more directly.


File: MemoryPoolSystem.info,  Node: Testing<9>,  Next: Text,  Prev: Implementation<25>,  Up: Manual Variable Temporal MVT pool design

5.21.10 Testing
---------------

*note .test.component;: 12d7. Components *note .impl.c.splay: 12b6,
*note .impl.c.cbs: 12b9, and *note .impl.c.abq: 12bf. will be subjected
to individual component tests to verify their functionality.

*note .test.qa;: 12d8. Once poolmvt is integrated into the MPS, the
standard MPS QA tests will be applied to poolmvt prior to each release.

*note .test.customer;: 12d9. Customer acceptance tests will be performed
on a per-customer basis before release to that customer (cf.
proc.release.epcore(2).test)


File: MemoryPoolSystem.info,  Node: Text,  Prev: Testing<9>,  Up: Manual Variable Temporal MVT pool design

5.21.11 Text
------------

Possible tweaks (from mail.pekka.1998-04-15.13-10(1)):

   - Try to coalesce splinters returned from AP’s with the front (or
     any) block on the ABQ.

   - Sort ABQ in some other way to minimize splitting/splinters.  For
     example, proximity to recently allocated blocks.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/04/15/13-10/0.txt


File: MemoryPoolSystem.info,  Node: MVFF pool class,  Next: Protocol inheritance,  Prev: Manual Variable Temporal MVT pool design,  Up: Old design

5.22 MVFF pool class
====================

* Menu:

* Introduction: Introduction<68>.
* Overview: Overview<29>.
* Methods: Methods<2>.
* Implementation: Implementation<26>.
* Details::


File: MemoryPoolSystem.info,  Node: Introduction<68>,  Next: Overview<29>,  Up: MVFF pool class

5.22.1 Introduction
-------------------

*note .intro;: 12e0. This is the design of the MVFF (Manual Variable
First-Fit) pool class.  This pool implements a first (or last) fit
policy for variable-sized manually-managed objects, with control over
first/last, segment preference high/low, and slot fit low/high.

*note .background;: 12e1. The pool was created in a response to a belief
that the ScriptWorks EPDL/EPDR’s first fit policy is beneficial for some
classes of client behaviour, but the performance of a linear free list
was unacceptable.


File: MemoryPoolSystem.info,  Node: Overview<29>,  Next: Methods<2>,  Prev: Introduction<68>,  Up: MVFF pool class

5.22.2 Overview
---------------

*note .over;: 12e3. This pool implements certain variants of the
address-ordered first-fit policy.  The implementation allows allocation
across segment boundaries.

*note .over.buffer;: 12e4. Buffered allocation is also supported, but in
that case, the buffer-filling policy is worst-fit.  Buffered and
unbuffered allocation can be used at the same time, but in that case,
the first ap must be created before any allocations.

*note .over.buffer.class;: 12e5. The pool uses the simplest buffer
class, ‘BufferClass’.  This is appropriate since these buffers don’t
attach to segments, and hence don’t constrain buffered regions to lie
within segment boundaries.


File: MemoryPoolSystem.info,  Node: Methods<2>,  Next: Implementation<26>,  Prev: Overview<29>,  Up: MVFF pool class

5.22.3 Methods
--------------

*note .method.buffer;: 12e7. The buffer methods implement a worst-fit
fill strategy.


File: MemoryPoolSystem.info,  Node: Implementation<26>,  Next: Details,  Prev: Methods<2>,  Up: MVFF pool class

5.22.4 Implementation
---------------------

*note .impl.alloc_list;: 12e9. The pool stores the address ranges that
it has acquired from the arena in a CBS (see design.mps.cbs(1)).

*note .impl.free-list;: 12ea. The pool stores its free list in a CBS
(see design.mps.cbs(2)), failing over in emergencies to a Freelist (see
design.mps.freelist(3)) when the CBS cannot allocate new control
structures.  This is the reason for the alignment restriction above.

   ---------- Footnotes ----------

   (1) cbs.html

   (2) cbs.html

   (3) freelist.html


File: MemoryPoolSystem.info,  Node: Details,  Prev: Implementation<26>,  Up: MVFF pool class

5.22.5 Details
--------------

*note .design.acquire-size;: 12ed. When acquiring memory from the arena,
we use ‘extendBy’ as the unit of allocation unless the object won’t fit,
in which case we use the object size (in both cases we align up to the
arena alignment).

*note .design.acquire-fail;: 12ee. If allocating ‘extendBy’, we try
again with an aligned size just large enough for the object we’re
allocating.  This is in response to request.mps.170186(1).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/mps/170186


File: MemoryPoolSystem.info,  Node: Protocol inheritance,  Next: POSIX thread extensions,  Prev: MVFF pool class,  Up: Old design

5.23 Protocol inheritance
=========================

* Menu:

* Introduction: Introduction<69>.
* Purpose: Purpose<4>.
* Requirements: Requirements<43>.
* Overview: Overview<30>.
* Interface: Interface<27>.
* Implementation: Implementation<27>.
* Common instance methods::
* References: References<23>.


File: MemoryPoolSystem.info,  Node: Introduction<69>,  Next: Purpose<4>,  Up: Protocol inheritance

5.23.1 Introduction
-------------------

*note .intro;: 12f5. This document explains the design of the support
for class inheritance in MPS.

*note .readership;: 12f6. This document is intended for any MPS
developer.


File: MemoryPoolSystem.info,  Node: Purpose<4>,  Next: Requirements<43>,  Prev: Introduction<69>,  Up: Protocol inheritance

5.23.2 Purpose
--------------

*note .purpose.code-maintain;: 12f8. The purpose of the protocol
inheritance design is to ensure that the MPS code base can make use of
the benefits of object-oriented class inheritance to maximize code
reuse, minimize code maintenance and minimize the use of boilerplate
code.

*note .purpose.related;: 12f9. For related discussion, see
mail.tony.1998-08-28.16-26(1), mail.tony.1998-09-01.11-38(2),
mail.tony.1998-10-06.11-03(3) and other messages in the same threads.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1998/08/28/16-26/0.txt

   (2) 
https://info.ravenbrook.com/project/mps/mail/1998/09/01/11-38/0.txt

   (3) 
https://info.ravenbrook.com/project/mps/mail/1998/10/06/11-03/0.txt


File: MemoryPoolSystem.info,  Node: Requirements<43>,  Next: Overview<30>,  Prev: Purpose<4>,  Up: Protocol inheritance

5.23.3 Requirements
-------------------

*note .req.implicit;: 12fc. The object system should provide a means for
classes to inherit the methods of their direct superclasses implicitly
for all functions in the protocol without having to write any explicit
code for each inherited function.

*note .req.override;: 12fd. There must additionally be a way for classes
to override the methods of their superclasses.

*note .req.next-method;: 12fe. As a result of *note .req.implicit: 12fc,
classes cannot make static assumptions about methods used by direct
superclasses.  The object system must provide a means for classes to
extend (not just replace) the behaviour of protocol functions, such as a
mechanism for invoking the “next-method”.

*note .req.ideal.extend;: 12ff. The object system must provide a
standard way for classes to implement the protocol supported by their
superclass and additionally add new methods of their own which can be
specialized by subclasses.

*note .req.ideal.multiple-inheritance;: 1300. The object system should
support multiple inheritance such that sub-protocols can be “mixed in”
with several classes which do not themselves support identical
protocols.


File: MemoryPoolSystem.info,  Node: Overview<30>,  Next: Interface<27>,  Prev: Requirements<43>,  Up: Protocol inheritance

5.23.4 Overview
---------------

*note .overview.inst;: 1302. The key concept in the design is the
relationship between an “instance” and its “class”.  Every structure
that participates in the protocol system begins with an ‘InstStruct’
structure that contains a pointer to an ‘InstClassStruct’ that describes
it, like this:

      instance          class

     .----------.      .----------.
     |  class   |----->|  class   |
     ------------      ------------
     |  ...     |      |  sig     |
     ------------      ------------
     |  ...     |      |  name    |
     ------------      ------------
     |  ...     |      |superclass|
     ------------      ------------
     |          |      |   ...    |

*note .overview.prefix;: 1303. We make use of the fact that we can cast
between structures with common prefixes, or between structures and their
first members, to provide dynamic typing and subtyping (see *note
[Kernighan_1988]: 1304, A.8.3).

*note .overview.method;: 1305. The ‘InstClassStruct’ it itself at the
start of a class structure contains pointers to functions that can be
called to manipulate the instance as an abstract data type.  We refer to
these functions as “methods” to distinguish them from functions not
involved in the object-oriented protocol.  The macro ‘Method’ is
provided for calling methods.

*note .overview.subclass;: 1306. An instance structure can be extended
by using it as the first field of another structure, and by overriding
its class pointer with a pointer to a “subclass” that provides different
behavior.

*note .overview.inherit;: 1307. Classes inherit the methods from their
superclasses when they are initialized, so by default they have the same
methods as the class from which they inherit.  Methods on the superclass
can be re-used, providing polymorphism.

*note .overview.inherit.specialize;: 1308. Classes may specialize the
behaviour of their superclass.  They do this by by overriding methods or
other fields in the class object.

*note .overview.mixin;: 1309. Groups of related overrides are provided
by “mixins”, and this provides a limited form of multiple inheritance.

*note .overview.extend;: 130a. Classes may extend the protocols
supported by their superclasses by adding new fields for methods or
other data.  Extending a class creates a new kind of class.

*note .overview.kind;: 130b. Classes are themselves instance objects,
and have classes of their own.  A class of a class is referred to as a
“kind”, but is not otherwise special.  Classes which share the same set
of methods (or other class fields) are instances of the same kind.  If a
class is extended, it becomes a member of a different kind.  Kinds allow
subtype checking to be applied to classes as well as instances, to
determine whether methods are available.

      instance          class             kind
      (e.g. CBS)        (e.g. CBSClass)   (e.g. LandClassClass)
     .----------.      .----------.      .----------.
     |  class   |----->|  class   |----->|  class   |-->InstClassClass
     ------------      ------------      ------------
     |  ...     |      |  sig     |      |  sig     |
     ------------      ------------      ------------
     |  ...     |      |  name    |      |  name    |
     ------------      ------------      ------------
     |  ...     |      |superclass|-.    |superclass|-->InstClassClass
     ------------      ------------ |    ------------
     |          |      |   ...    | |    |   ...    |
                                    |
                                    |
                         LandClass<-'

*note .overview.sig.inherit;: 130c. Instances (and therefore classes)
will contain signatures.  Classes must not specialize (override) the
signatures they inherit from their superclasses, as they are used to
check the actual type (not sub- or supertype) of the object they’re in.

*note .overview.sig.extend;: 130d. When extending an instance or class,
it is normal policy for the new structure to include a new signature as
the last field.

*note .overview.superclass;: 130e. Each class contains a ‘superclass’
field.  This enables classes to call “next-method”.

*note .overview.next-method;: 130f. A specialized method in a class can
make use of an overridden method from a superclass using the *note
NextMethod: 791. macro, statically naming the superclass.

*note .overview.next-method.dynamic;: 1310. It is possible to write a
method which does not statically know its superclass, and call the next
method by extracting a class from one of its arguments using
‘ClassOfPoly’ and finding the superclass using ‘SuperclassPoly’.  Debug
pool mixins do this.  However, this is not fully general, and combining
such methods is likely to cause infinite recursion.  Take care!

*note .overview.access;: 1311. Classes must be initialized by calls to
functions, since there is no way to express overrides statically in C89.
*note DEFINE_CLASS: 1312. defines an “ensure” function that initializes
and returns the canonical copy of the class.  The canonical copy may
reside in static storage, but no MPS code may refer to that static
storage by name.

*note .overview.init;: 1313. In addition to the “ensure” function, each
class must provide an “init” function, which initialises its argument as
a fresh copy of the class.  This allows subclasses to derive their
methods and other fields from superclasses.

*note .overview.naming;: 1314. There are some strict naming conventions
which must be followed when defining and using classes.  The use is
obligatory because it is assumed by the macros which support the
definition and inheritance mechanism.  For every kind ‘Foo’, we insist
upon the following naming conventions:

   * ‘Foo’ names a type that points to a ‘FooStruct’.

   * ‘FooStruct’ is the type of the instance structure, the first field
     of which is the structure it inherits from (ultimately an
     ‘InstStruct’).

   * ‘FooClass’ names the type that points to a ‘FooClassStruct’.

   * ‘FooClassStruct’ names the structure for the class pointed to by
     ‘FooStruct’, containing the methods that operate on ‘Foo’.


File: MemoryPoolSystem.info,  Node: Interface<27>,  Next: Implementation<27>,  Prev: Overview<30>,  Up: Protocol inheritance

5.23.5 Interface
----------------

* Menu:

* Class declaration::
* Class definition::
* Class access::
* Single inheritance::
* Specialization::
* Extension::
* Methods: Methods<3>.
* Conversion::
* Introspection::
* Protocol guidelines::
* Example: Example<3>.


File: MemoryPoolSystem.info,  Node: Class declaration,  Next: Class definition,  Up: Interface<27>

5.23.5.1 Class declaration
..........................

 -- C Macro: DECLARE_CLASS (kind, className)

*note .if.declare-class;: 1318. Class declaration is performed by the
macro *note DECLARE_CLASS: 1317, which declares the existence of the
class definition elsewhere.  It is intended for use in headers.


File: MemoryPoolSystem.info,  Node: Class definition,  Next: Class access,  Prev: Class declaration,  Up: Interface<27>

5.23.5.2 Class definition
.........................

 -- C Macro: DEFINE_CLASS (kind, className, var)

*note .if.define-class;: 131a. Class definition is performed by the
macro *note DEFINE_CLASS: 1312.  A call to the macro must be followed by
a function body of initialization code.  The parameter ‘className’ is
used to name the class being defined.  The parameter ‘var’ is used to
name a local variable of type of classes of kind ‘kind’, which is
defined by the macro; it refers to the canonical storage for the class
being defined.  This variable may be used in the initialization code.
(The macro doesn’t just pick a name implicitly because of the danger of
a name clash with other names used by the programmer).  A call to the
macro defines the ensure function for the class along with some static
storage for the canonical class object, and some other things to ensure
the class gets initialized at most once.


File: MemoryPoolSystem.info,  Node: Class access,  Next: Single inheritance,  Prev: Class definition,  Up: Interface<27>

5.23.5.3 Class access
.....................

 -- C Macro: CLASS (className)

*note .if.class;: 131c. To get the canonical class object, use the *note
CLASS: 780. macro, e.g.  ‘CLASS(Land)’.


File: MemoryPoolSystem.info,  Node: Single inheritance,  Next: Specialization,  Prev: Class access,  Up: Interface<27>

5.23.5.4 Single inheritance
...........................

 -- C Macro: INHERIT_CLASS (this, className, parentName)

*note .if.inheritance;: 131f. Class inheritance details must be provided
in the class initialization code (see *note .if.define-class: 131a.).
Inheritance is performed by the macro *note INHERIT_CLASS: 131e.  A call
to this macro will make the class being defined a direct subclass of
‘parentClassName’ by ensuring that all the fields of the embedded parent
class (pointed to by the ‘this’ argument) are initialized as the parent
class, and setting the superclass field of ‘this’ to be the canonical
parent class object.  The parameter ‘this’ must be the same kind as
‘parentClassName’.


File: MemoryPoolSystem.info,  Node: Specialization,  Next: Extension,  Prev: Single inheritance,  Up: Interface<27>

5.23.5.5 Specialization
.......................

*note .if.specialize;: 1321. Fields in the class structure must be
assigned explicitly in the class initialization code (see *note
.if.define-class: 131a.).  This must happen 'after' inheritance details
are given (see *note .if.inheritance: 131f.), so that overrides work.


File: MemoryPoolSystem.info,  Node: Extension,  Next: Methods<3>,  Prev: Specialization,  Up: Interface<27>

5.23.5.6 Extension
..................

*note .if.extend;: 1323. To extend the protocol when defining a new
class, a new type must be defined for the class structure.  This must
embed the structure for the primarily inherited class as the first field
of the structure.  Extension fields in the class structure must be
assigned explicitly in the class initialization code (see *note
.if.define-class: 131a.).  This should be done 'after' the inheritance
details are given for consistency with *note .if.inheritance: 131f.
This is, in fact, how all the useful classes extend ‘Inst’.

*note .if.extend.kind;: 1324. In addition, a class must be defined for
the new kind of class.  This is just an unspecialized subclass of the
kind of the class being specialized by the extension.  For example:

     typedef struct LandClassStruct {
       InstClassStruct instClass;  /* inherited class */
       LandInsertMethod insert;
       ...
     } LandClassStruct;

     DEFINE_CLASS(Inst, LandClass, class)
     {
       INHERIT_CLASS(class, LandClass, InstClass);
     }

     DEFINE_CLASS(Land, Land, class)
     {
       INHERIT_CLASS(&class->instClass, Land, Inst);
       class->insert = landInsert;
       ...
     }


File: MemoryPoolSystem.info,  Node: Methods<3>,  Next: Conversion,  Prev: Extension,  Up: Interface<27>

5.23.5.7 Methods
................

 -- C Macro: Method (kind, inst, meth)

*note .if.method;: 1327. To call a method on an instance of a class, use
the ‘Method’ macro to retrieve the method.  This macro may assert if the
class is not of the kind requested.  For example, to call the ‘insert’
method on ‘land’:

     res = Method(Land, land, insert)(rangeReturn, land, range);

 -- C Macro: NextMethod (kind, className, meth)

*note .if.next-method;: 1328. To call a method from a superclass of a
class, use the *note NextMethod: 791. macro to retrieve the method.
This macro may assert if the superclass is not of the kind requested.
For example, the function to split AMS segments wants to split the
segments they are based on, so does:

     res = NextMethod(Seg, AMSSeg, split)(seg, segHi, base, mid, limit);


File: MemoryPoolSystem.info,  Node: Conversion,  Next: Introspection,  Prev: Methods<3>,  Up: Interface<27>

5.23.5.8 Conversion
...................

 -- C Macro: IsA (className, inst)

if.isa: Returns non-zero iff the class of ‘inst’ is a member of the
class or any of its subclasses.

 -- C Macro: MustBeA (className, inst)

*note .if.must-be-a;: 132d. To convert the C type of an instance to that
of a compatible class (the class of the actual object or any
superclass), use the ‘MustBeA’ macro.  In hot varieties this macro
performs a fast dynamic type check and will assert if the class is not
compatible.  It is like C++ “dynamic_cast” with an assert.  In cool
varieties, the class check method is called on the object.  For example,
in a specialized Land method in the CBS class:

     static Res cbsInsert(Range rangeReturn, Land land, Range range)
     {
       CBS cbs = MustBeA(CBS, land);
       ...

 -- C Macro: MustBeA_CRITICAL (className, inst)

*note .if.must-be-a.critical;: 132f. When the cost of a type check is
too expensive in hot varieties, use ‘MustBeA_CRITICAL’ in place of
‘MustBeA’.  This only performs the check in cool varieties.  Compare
with ‘AVER_CRITICAL’.

 -- C Macro: CouldBeA (className, inst)

*note .if.could-be-a;: 1331. To make an unsafe conversion equivalent to
‘MustBeA’, use the ‘CouldBeA’ macro.  This is in effect a simple pointer
cast, but it expresses the intention of class compatibility in the
source code.  It is mainly intended for use when initializing an object,
when a class compatibility check would fail, when checking an object, or
in debugging code such as describe methods, where asserting is
inappropriate.  It is intended to be equivalent to the C++
‘static_cast’, although since this is C there is no actual static
checking, so in fact it’s more like ‘reinterpret_cast’.


File: MemoryPoolSystem.info,  Node: Introspection,  Next: Protocol guidelines,  Prev: Conversion,  Up: Interface<27>

5.23.5.9 Introspection
......................

*note .introspect.c-lang;: 1333. The design includes a number of
introspection functions for dynamically examining class relationships.
These functions are polymorphic and accept arbitrary subclasses of
‘InstClass’.  C doesn’t support such polymorphism.  So although these
have the semantics of functions (and could be implemented as functions
in another language with compatible calling conventions) they are
actually implemented as macros.  The macros are named as function-style
macros despite the fact that this arguably contravenes
guide.impl.c.macro.method.  The justification for this is that this
design is intended to promote the use of polymorphism, and it breaks the
abstraction for the users to need to be aware of what can and can’t be
expressed directly in C function syntax.  These functions all have names
ending in ‘Poly’ to identify them as polymorphic functions.

 -- C Macro: SuperclassPoly (kind, class)

*note .if.superclass-poly;: 1335. An introspection function which
returns the direct superclass of class object ‘class’ as a class of kind
‘kind’.  This may assert if the superclass is not (a subtype of) the
kind requested.

 -- C Macro: ClassOfPoly (kind, inst)

*note .if.class-of-poly;: 1337. An introspection function which returns
the class of which ‘inst’ is a direct instance, as a class of kind
‘kind’.  This may assert if the class is not (a subtype of) the kind
requested.

 -- C Macro: SetClassOfPoly (inst, class)

*note .if.set-class-of-poly;: 1339. An initialization function that sets
the class of ‘inst’ to be ‘class’.  This is intended only for use in
initialization functions, to specialize the instance once its fields
have been initialized.  Each Init function should call its superclass
init, finally reaching InstInit, and then, once it has set up its
fields, use SetClassOfPoly to set the class and check the instance with
its check method.  Compare with design.mps.sig(1).

 -- C Macro: IsSubclass (sub, super)

*note .if.is-subclass;: 133b. An introspection function which returns a
*note Bool: 3a9. indicating whether ‘sub’ is a subclass of ‘super’.
That is, it is a predicate for testing subclass relationships.

   ---------- Footnotes ----------

   (1) sig


File: MemoryPoolSystem.info,  Node: Protocol guidelines,  Next: Example<3>,  Prev: Introspection,  Up: Interface<27>

5.23.5.10 Protocol guidelines
.............................

*note .guide.fail;: 133d. When designing an extensible method which
might fail, the design must permit the correct implementation of the
failure-case code.  Typically, a failure might occur in any method in
the chain.  Each method is responsible for correctly propagating failure
information supplied by superclass methods and for managing it’s own
failures.  This is not really different from the general MPS convention
for unwinding on error paths.  It implies that the design of a class
must include an anti-method for each method that changes the state of an
instance (e.g.  by allocating memory) to allow the state to be reverted
in case of a failure.  See *note .example.fail: 133e. below.


File: MemoryPoolSystem.info,  Node: Example<3>,  Prev: Protocol guidelines,  Up: Interface<27>

5.23.5.11 Example
.................

*note .example.inheritance;: 1340. The following example class
definition shows both inheritance and specialization.  It shows the
definition of the class ‘RankBuf’, which inherits from ‘SegBuf’ of kind
*note Seg: b53. and has specialized ‘varargs’ and ‘init’ method.

     DEFINE_CLASS(Buffer, RankBuf, class)
     {
       INHERIT_CLASS(class, RankBuf, SegBuf);
       class->varargs = rankBufVarargs;
       class->init = rankBufInit;
     }

*note .example.extension;: 1341. The following (hypothetical) example
class definition shows inheritance, specialization and also extension.
It shows the definition of the class ‘EPDLDebugPool’, which inherits
from ‘EPDLPool’ of kind ‘Pool’, but also implements a method for
checking properties of the pool.

     typedef struct EPDLDebugPoolClassStruct {
       EPDLPoolClassStruct epdl;
       DebugPoolCheckMethod check;
       Sig sig;
     } EPDLDebugPoolClassStruct;

     typedef EPDLDebugPoolClassStruct *EPDLDebugPoolClass;

     DEFINE_CLASS(Inst, EPDLDebugPoolClass, class)
     {
       INHERIT_CLASS(class, EPDLPoolClass, InstClass);
     }

     DEFINE_CLASS(EPDLDebugPool, EPDLDebugPool, class)
     {
       INHERIT_CLASS(&class->epdl, EPDLDebugPool, EPDLPoolClass);
       class->check = EPDLDebugCheck;
       class->sig = EPDLDebugSig;
     }

*note .example.fail;: 133e. The following example shows the
implementation of failure-case code for an “init” method, making use of
the “finish” anti-method to clean-up a subsequent failure.

     static Res AMSSegInit(Seg seg, Pool pool,
                           Addr base, Size size,
                           ArgList args)
     {
       AMS ams = MustBeA(AMSPool, pool);
       Arena arena = PoolArena(pool);
       AMSSeg amsseg;
       Res res;

       /* Initialize the superclass fields first via next-method call */
       res = NextMethod(Seg, AMSSeg, init)(seg, pool, base, size, args);
       if (res != ResOK)
         goto failNextMethod;
       amsseg = CouldBeA(AMSSeg, seg);

       amsseg->grains = size >> ams->grainShift;
       amsseg->freeGrains = amsseg->grains;
       amsseg->oldGrains = (Count)0;
       amsseg->newGrains = (Count)0;
       amsseg->marksChanged = FALSE; /* <design/poolams/#marked.unused> */
       amsseg->ambiguousFixes = FALSE;

       res = amsCreateTables(ams, &amsseg->allocTable,
                             &amsseg->nongreyTable, &amsseg->nonwhiteTable,
                             arena, amsseg->grains);
       if (res != ResOK)
         goto failCreateTables;

       /* start off using firstFree, see <design/poolams/#no-bit> */
       amsseg->allocTableInUse = FALSE;
       amsseg->firstFree = 0;
       amsseg->colourTablesInUse = FALSE;

       amsseg->ams = ams;
       RingInit(&amsseg->segRing);
       RingAppend((ams->allocRing)(ams, SegRankSet(seg), size),
                  &amsseg->segRing);

       SetClassOfPoly(seg, CLASS(AMSSeg));
       amsseg->sig = AMSSegSig;
       AVERC(AMSSeg, amsseg);

       return ResOK;

     failCreateTables:
       NextMethod(Seg, AMSSeg, finish)(seg);
     failNextMethod:
       AVER(res != ResOK);
       return res;
     }


File: MemoryPoolSystem.info,  Node: Implementation<27>,  Next: Common instance methods,  Prev: Interface<27>,  Up: Protocol inheritance

5.23.6 Implementation
---------------------

*note .impl.define-class.lock;: 1343. The *note DEFINE_CLASS: 1312.
macro ensures that each class is initialized at most once (even in
multi-threaded programs) by claiming the global recursive lock (see
design.mps.thread-safety.arch.global.recursive(1)).

*note .impl.derived-names;: 1344. The *note DEFINE_CLASS(): 1312. macro
derives some additional names from the class name as part of it’s
implementation.  These should not appear in the source code, but it may
be useful to know about this for debugging purposes.  For each class
definition for class ‘SomeClass’ of kind ‘SomeKind’, the macro defines
the following:

   * ‘extern SomeKind SomeClassGet(void);’

     The class ensure function.  See *note .overview.naming: 1314.  This
     function handles local static storage for the canonical class
     object and a guardian to ensure the storage is initialized at most
     once.  This function is invoked by the *note CLASS: 780. macro
     (*note .if.class: 131c.).

   * ‘static void SomeClassInit(SomeKind);’

     A function called by ‘SomeClassGet()’.  All the class
     initialization code is actually in this function.

*note .impl.subclass;: 1345. The subclass test *note .if.is-subclass:
133b. is implemented using an array of superclasses *note [Cohen_1991]:
1346. giving a fast constant-time test.  (RB(2) tried an approach using
prime factors *note [Gibbs_2004]: 1347. but found that they overflowed
in 32-bits too easily to be useful.)  Each class is assigned a “level”
which is the distance from the root of the class hierarchy.  The
‘InstClass’ structure contains an array of class ids indexed by level,
representing the inheritance of this class.  A class is a subclass of
another if and only if the superclass id is present in the array at the
superclass level.  The level is statically defined using enum constants,
and the id is the address of the canonical class object, so the test is
fast and simple.

   ---------- Footnotes ----------

   (1) thread-safety.html#design.mps.thread-safety.arch.global.recursive

   (2) https://www.ravenbrook.com/consultants/rb/


File: MemoryPoolSystem.info,  Node: Common instance methods,  Next: References<23>,  Prev: Implementation<27>,  Up: Protocol inheritance

5.23.7 Common instance methods
------------------------------

*note .method;: 134a. These methods are available on all instances.

 -- C Type: typedef void (*FinishMethod)(Inst inst)

*note .method.finish;: 134c. The ‘finish’ method should finish the
instance data structure (releasing any resources that were acquired by
the instance during its lifetime) and then call its superclass method
via the *note NextMethod(): 791. macro.

 -- C Type: typedef *note Res: 55f. (*DescribeMethod)(Inst inst, *note
          mps_lib_FILE: 2d3. *stream, *note Count: 3af. depth)

*note .method.describe;: 134e. The ‘describe’ field should print out a
description of the instance to ‘stream’ (by calling *note WriteF():
446.).


File: MemoryPoolSystem.info,  Node: References<23>,  Prev: Common instance methods,  Up: Protocol inheritance

5.23.8 References
-----------------

(Cohen_1991) “Type-Extension Type Tests Can Be Performed In Constant
Time”; Norman H Cohen; IBM Thomas J Watson Research Center; ACM
Transactions on Programming Languages and Systems, Vol.  13 No.  4, pp.
626-629; 1991-10.

(Gibbs_2004) Michael Gibbs, Bjarne Stroustrup.  2004.  “Fast Dynamic
Casting(1)”.

(Kernighan_1988) Brian W. Kernighan, Dennis M. Ritchie.  1988.  “The C
Programming language 2nd Edition”.

   ---------- Footnotes ----------

   (1) http://www.stroustrup.com/fast_dynamic_casting.pdf


File: MemoryPoolSystem.info,  Node: POSIX thread extensions,  Next: Root manager,  Prev: Protocol inheritance,  Up: Old design

5.24 POSIX thread extensions
============================

* Menu:

* Introduction: Introduction<70>.
* Definitions: Definitions<14>.
* Requirements: Requirements<44>.
* Analysis: Analysis<6>.
* Interface: Interface<28>.
* Implementation: Implementation<28>.
* Attachments: Attachments<2>.
* References: References<24>.


File: MemoryPoolSystem.info,  Node: Introduction<70>,  Next: Definitions<14>,  Up: POSIX thread extensions

5.24.1 Introduction
-------------------

*note .readership;: 1355. Any MPS developer.

*note .intro;: 1356. This is the design of the Pthreads extension
module, which provides some low-level threads support for use by MPS
(notably suspend and resume).


File: MemoryPoolSystem.info,  Node: Definitions<14>,  Next: Requirements<44>,  Prev: Introduction<70>,  Up: POSIX thread extensions

5.24.2 Definitions
------------------

*note .pthreads;: 1358. The term “Pthreads” means an implementation of
the POSIX 1003.1c-1995 thread standard.  (Or the Single UNIX
Specification, Version 2, aka USV2 or UNIX98.)

*note .context;: 1359. The “context” of a thread is a
(platform-specific) OS-defined structure which describes the current
state of the registers for that thread.


File: MemoryPoolSystem.info,  Node: Requirements<44>,  Next: Analysis<6>,  Prev: Definitions<14>,  Up: POSIX thread extensions

5.24.3 Requirements
-------------------

*note .req.suspend;: 135b. A means to suspend threads, so that they
don’t make any progress.

*note .req.suspend.why;: 135c. Needed by the thread manager so that
other threads registered with an arena can be suspended (see
design.mps.thread-manager(1)).  Not directly provided by Pthreads.

*note .req.resume;: 135d. A means to resume suspended threads, so that
they are able to make progress again.  *note .req.resume.why;: 135e.
Needed by the thread manager.  Not directly provided by Pthreads.

*note .req.suspend.multiple;: 135f. Allow a thread to be suspended on
behalf of one arena when it has already been suspended on behalf of one
or more other arenas.  *note .req.suspend.multiple.why;: 1360. The
thread manager contains no design for cooperation between arenas to
prevent this.

*note .req.resume.multiple;: 1361. Allow requests to resume a thread on
behalf of each arena which had previously suspended the thread.  The
thread must only be resumed when requests from all such arenas have been
received.  *note .req.resume.multiple.why;: 1362. A thread manager for
an arena must not permit a thread to make progress before it explicitly
resumes the thread.

*note .req.suspend.context;: 1363. Must be able to access the context
for a thread when it is suspended.

*note .req.suspend.protection;: 1364. Must be able to suspend a thread
which is currently handling a protection fault (i.e., an arena access).
Such a thread might even own an arena lock.

*note .req.legal;: 1365. Must use the Pthreads and other POSIX APIs in a
legal manner.

   ---------- Footnotes ----------

   (1) thread-manager.html


File: MemoryPoolSystem.info,  Node: Analysis<6>,  Next: Interface<28>,  Prev: Requirements<44>,  Up: POSIX thread extensions

5.24.4 Analysis
---------------

*note .analysis.suspend;: 1367. Thread suspension is inherently
asynchronous.  MPS needs to be able to suspend another thread without
prior knowledge of the code that thread is running.  (That is, we can’t
rely on cooperation between threads.)  The only asynchronous
communication available on POSIX is via signals – so the suspend and
resume mechanism must ultimately be built from signals.

*note .analysis.signal.safety;: 1368. POSIX imposes some restrictions on
what a signal handler function might do when invoked asynchronously (see
the sigaction(1) documentation, and search for the string “reentrant”).
In summary, a small number of POSIX functions are defined to be
“async-signal safe”, which means they may be invoked without restriction
in signal handlers.  All other POSIX functions are considered to be
unsafe.  Behaviour is undefined if an unsafe function is interrupted by
a signal and the signal handler then proceeds to call another unsafe
function.  See mail.tony.1999-08-24.15-40(2) and followups for some
further analysis.

*note .analysis.signal.safety.implication;: 1369. Since we can’t assume
that we won’t attempt to suspend a thread while it is running an unsafe
function, we must limit the use of POSIX functions in the suspend signal
handler to those which are designed to be “async-signal safe”.  One of
the few such functions related to synchronization is ‘sem_post()’.

*note .analysis.signal.example;: 136a. An example of how to suspend
threads in POSIX was posted to newsgroup comp.programming.threads in
August 1999 *note [Lau_1999-08-16]: 136b.  The code in the post was
written by David Butenhof, who contributed some comments on his
implementation *note [Butenhof_1999-08-16]: 136c.

*note .analysis.signal.linux-hack;: 136d. In the current implementation
of Linux Pthreads, it would be possible to implement suspend/resume
using ‘SIGSTOP’ and ‘SIGCONT’.  This is, however, nonportable and will
probably stop working on Linux at some point.

*note .analysis.component;: 136e. There is no known way to meet the
requirements above in a way which cooperates with another component in
the system which also provides its own mechanism to suspend and resume
threads.  The best bet for achieving this is to provide the
functionality in shared low-level component which may be used by MPS and
other clients.  This will require some discussion with other potential
clients and/or standards bodies.

*note .analysis.component.dylan;: 136f. Note that such cooperation is
actually a requirement for Dylan (req.dylan.dc.env.self), though this is
not a problem, since all the Dylan components share the MPS mechanism.

   ---------- Footnotes ----------

   (1) 
https://pubs.opengroup.org/onlinepubs/007908799/xsh/sigaction.html

   (2) 
https://info.ravenbrook.com/project/mps/mail/1999/08/24/15-40/0.txt


File: MemoryPoolSystem.info,  Node: Interface<28>,  Next: Implementation<28>,  Prev: Analysis<6>,  Up: POSIX thread extensions

5.24.5 Interface
----------------

 -- C Type: typedef *note PThreadextStruct: 1372. *PThreadext

*note .if.pthreadext.abstract;: 1373. A thread is represented by the
abstract type *note PThreadext: 1371.  A *note PThreadext: 1371. object
corresponds directly with a thread (of type ‘pthread_t’).  There may be
more than one *note PThreadext: 1371. object for the same thread.

*note .if.pthreadext.structure;: 1374. The structure definition of *note
PThreadext: 1371. (*note PThreadextStruct: 1372.) is exposed by the
interface so that it may be embedded in a client datastructure (for
example, ‘ThreadStruct’).  This means that all storage management can be
left to the client (which is important because there might be multiple
arenas involved).  Clients may not access the fields of a *note
PThreadextStruct: 1372. directly.

 -- C Function: void PThreadextInit (PThreadext pthreadext, pthread_t
          id)

*note .if.init;: 1376. Initializes a *note PThreadext: 1371. object for
a thread with the given ‘id’.

 -- C Function: *note Bool: 3a9. PThreadextCheck (PThreadext pthreadext)

*note .if.check;: 1378. Checks a *note PThreadext: 1371. object for
consistency.  Note that this function takes the mutex, so it must not be
called with the mutex held (doing so will probably deadlock the thread).

 -- C Function: *note Res: 55f. PThreadextSuspend (PThreadext
          pthreadext, struct sigcontext **contextReturn)

*note .if.suspend;: 1379. Suspends a *note PThreadext: 1371. object
(puts it into a suspended state).  Meets *note .req.suspend: 135b.  The
object must not already be in a suspended state.  If the function
returns ‘ResOK’, the context of the thread is returned in contextReturn,
and the corresponding thread will not make any progress until it is
resumed.

 -- C Function: *note Res: 55f. PThreadextResume (PThreadext pthreadext)

*note .if.resume;: 137a. Resumes a *note PThreadext: 1371. object.
Meets *note .req.resume: 135d.  The object must already be in a
suspended state.  Puts the object into a non-suspended state.  Permits
the corresponding thread to make progress again, although that might not
happen immediately if there is another suspended *note PThreadext: 1371.
object corresponding to the same thread.

 -- C Function: void PThreadextFinish (PThreadext pthreadext)

*note .if.finish;: 137c. Finishes a PThreadext object.


File: MemoryPoolSystem.info,  Node: Implementation<28>,  Next: Attachments<2>,  Prev: Interface<28>,  Up: POSIX thread extensions

5.24.6 Implementation
---------------------

 -- C Type: typedef struct *note PThreadextStruct: 1372.
          PThreadextStruct

*note .impl.pthreadext;: 137e. The structure definition for a *note
PThreadext: 1371. object is:

     struct PThreadextStruct {
       Sig sig;                         /* <design/sig/> */
       pthread_t id;                    /* Thread ID */
       MutatorContext context;          /* context if suspended */
       RingStruct threadRing;           /* ring of suspended threads */
       RingStruct idRing;               /* duplicate suspensions for id */
     };

*note .impl.field.id;: 137f. The ‘id’ field shows which PThread the
object corresponds to.

*note .impl.field.context;: 1380. The ‘context’ field contains the
context when in a suspended state.  Otherwise it is ‘NULL’.

*note .impl.field.threadring;: 1381. The ‘threadRing’ field is used to
chain the object onto the suspend ring when it is in the suspended state
(see *note .impl.global.suspend-ring: 1382.).  When not in a suspended
state, this ring is single.

*note .impl.field.idring;: 1383. The ‘idRing’ field is used to group the
object with other objects corresponding to the same thread (same ‘id’
field) when they are in the suspended state.  When not in a suspended
state, or when this is the only *note PThreadext: 1371. object with this
‘id’ in the suspended state, this ring is single.

*note .impl.global.suspend-ring;: 1382. The module maintains a global
variable ‘suspendedRing’, a ring of *note PThreadext: 1371. objects
which are in a suspended state.  This is primarily so that it’s possible
to determine whether a thread is currently suspended anyway because of
another *note PThreadext: 1371. object, when a suspend attempt is made.

*note .impl.global.victim;: 1384. The module maintains a global variable
‘suspendingVictim’ which is used to indicate which *note PThreadext:
1371. is the current victim during suspend operations.  This is used to
communicate information between the controlling thread and the thread
being suspended (the victim).  The variable has value ‘NULL’ at other
times.

*note .impl.static.mutex;: 1385. We use a lock (mutex) around the
suspend and resume operations.  This protects the state data (the
suspend-ring and the victim: see *note .impl.global.suspend-ring: 1382.
and *note .impl.global.victim: 1384. respectively).  Since only one
thread can be suspended at a time, there’s no possibility of two arenas
suspending each other by concurrently suspending each other’s threads.

*note .impl.static.semaphore;: 1386. We use a semaphore to synchronize
between the controlling and victim threads during the suspend operation.
See *note .impl.suspend: 1387. and *note .impl.suspend-handler: 1388.).

*note .impl.static.init;: 1389. The static data and global variables of
the module are initialized on the first call to *note
PThreadextSuspend(): 7dd, using ‘pthread_once()’ to avoid concurrency
problems.  We also enable the signal handlers at the same time (see
*note .impl.suspend-handler: 1388. and *note .impl.resume-handler:
138a.).

*note .impl.suspend;: 1387. *note PThreadextSuspend(): 7dd. first
ensures the module is initialized (see *note .impl.static.init: 1389.).
After this, it claims the mutex (see *note .impl.static.mutex: 1385.).
It then checks to see whether thread of the target *note PThreadext:
1371. object has already been suspended on behalf of another *note
PThreadext: 1371. object.  It does this by iterating over the suspend
ring.

*note .impl.suspend.already-suspended;: 138b. If another object with the
same id is found on the suspend ring, then the thread is already
suspended.  The context of the target object is updated from the other
object, and the other object is linked into the ‘idRing’ of the target.

*note .impl.suspend.not-suspended;: 138c. If the thread is not already
suspended, then we forcibly suspend it using a technique similar to
Butenhof’s (see *note .analysis.signal.example: 136a.): First we set the
victim variable (see *note .impl.global.victim: 1384.) to indicate the
target object.  Then we send the signal ‘PTHREADEXT_SIGSUSPEND’ to the
thread (see *note .impl.signals: 138d.), and wait on the semaphore for
it to indicate that it has received the signal and updated the victim
variable with the context.  If either of these operations fail (for
example, because of thread termination) we unlock the mutex and return
‘ResFAIL’.

*note .impl.suspend.update;: 138e. Once we have ensured that the thread
is definitely suspended, we add the target *note PThreadext: 1371.
object to the suspend ring, unlock the mutex, and return the context to
the caller.

*note .impl.suspend-handler;: 1388. The suspend signal handler is
invoked in the target thread during a suspend operation, when a
‘PTHREADEXT_SIGSUSPEND’ signal is sent by the controlling thread (see
*note .impl.suspend.not-suspended: 138c.).  The handler determines the
context (received as a parameter, although this may be
platform-specific) and stores this in the victim object (see *note
.impl.global.victim: 1384.).  The handler then masks out all signals
except the one that will be received on a resume operation
(‘PTHREADEXT_SIGRESUME’) and synchronizes with the controlling thread by
posting the semaphore.  Finally the handler suspends until the resume
signal is received, using ‘sigsuspend()’.

*note .impl.resume;: 138f. *note PThreadextResume(): a89. first claims
the mutex (see *note .impl.static.mutex: 1385.).  It then checks to see
whether thread of the target *note PThreadext: 1371. object has also
been suspended on behalf of another *note PThreadext: 1371. object (in
which case the id ring of the target object will not be single).

*note .impl.resume.also-suspended;: 1390. If the thread is also
suspended on behalf of another *note PThreadext: 1371, then the target
object is removed from the id ring.

*note .impl.resume.not-also;: 1391. If the thread is not also suspended
on behalf of another *note PThreadext: 1371, then the thread is resumed
using the technique proposed by Butenhof (see *note
.analysis.signal.example: 136a.).  I.e.  we send it the signal
‘PTHREADEXT_SIGRESUME’ (see *note .impl.signals: 138d.) and expect it to
wake up.  If this operation fails (for example, because of thread
termination) we unlock the mutex and return ‘ResFAIL’.

*note .impl.resume.update;: 1392. Once the target thread is in the
appropriate state, we remove the target *note PThreadext: 1371. object
from the suspend ring, set its context to ‘NULL’ and unlock the mutex.

*note .impl.resume-handler;: 138a. The resume signal handler is invoked
in the target thread during a resume operation, when a
‘PTHREADEXT_SIGRESUME’ signal is sent by the controlling thread (see
*note .impl.resume.not-also: 1391.).  The resume signal handler simply
returns.  This is sufficient to unblock the suspend handler, which will
have been blocking the thread at the time of the signal.  The Pthreads
implementation ensures that the signal mask is restored to the value it
had before the signal handler was invoked.

*note .impl.finish;: 1393. *note PThreadextFinish(): 137b. supports the
finishing of objects in the suspended state, and removes them from the
suspend ring and id ring as necessary.  It must claim the mutex for the
removal operation (to ensure atomicity of the operation).  Finishing of
suspended objects is supported so that clients can dispose of resources
if a resume operation fails (which probably means that the PThread has
terminated).

*note .impl.signals;: 138d. The choice of which signals to use for
suspend and restore operations may need to be platform-specific.  Some
signals are likely to be generated and/or handled by other parts of the
application and so should not be used (for example, ‘SIGSEGV’).  Some
implementations of PThreads use some signals for themselves, so they may
not be used; for example, LinuxThreads uses ‘SIGUSR1’ and ‘SIGUSR2’ for
its own purposes, and so do popular tools like Valgrind that we would
like to be compatible with the MPS. The design therefore abstractly
names the signals ‘PTHREADEXT_SIGSUSPEND’ and ‘PTHREAD_SIGRESUME’, so
that they may be easily mapped to appropriate real signal values.
Candidate choices are ‘SIGXFSZ’ and ‘SIGXCPU’.

*note .impl.signals.config;: 1394. The identity of the signals used to
suspend and resume threads can be configured at compilation time using
the preprocessor constants *note CONFIG_PTHREADEXT_SIGSUSPEND: 1fb. and
*note CONFIG_PTHREADEXT_SIGRESUME: 1fc. respectively.


File: MemoryPoolSystem.info,  Node: Attachments<2>,  Next: References<24>,  Prev: Implementation<28>,  Up: POSIX thread extensions

5.24.7 Attachments
------------------

[missing attachment “posix.txt”]

[missing attachment “susp.c”]


File: MemoryPoolSystem.info,  Node: References<24>,  Prev: Attachments<2>,  Up: POSIX thread extensions

5.24.8 References
-----------------

(Butenhof_1999-08-16) Dave Butenhof.  comp.programming.threads.
1999-08-16.  “Re: Problem with Suspend & Resume Thread Example(1)”.

(Lau_1999-08-16) Raymond Lau.  comp.programming.threads.  1999-08-16.
“Problem with Suspend & Resume Thread Example(2)”.

   ---------- Footnotes ----------

   (1) 
https://groups.google.com/group/comp.programming.threads/msg/2a604c5f03f388d0

   (2) 
https://groups.google.com/group/comp.programming.threads/msg/dc4d9a45866331eb


File: MemoryPoolSystem.info,  Node: Root manager,  Next: The generic scanner,  Prev: POSIX thread extensions,  Up: Old design

5.25 Root manager
=================

* Menu:

* Basics::
* Details: Details<2>.


File: MemoryPoolSystem.info,  Node: Basics,  Next: Details<2>,  Up: Root manager

5.25.1 Basics
-------------

*note .root.def;: 139c. The root node of the object graph is the node
which defines whether objects are accessible, and the place from which
the mutator acts to change the graph.  In the MPS, a root is an object
which describes part of the root node.  The root node is the total of
all the roots attached to the space.

     Note: Note that this combines two definitions of root: the
     accessibility is what defines a root for tracing (see
     analysis.tracer.root.* and the mutator action for barriers (see
     analysis.async-gc.root).  Pekka P. Pirinen, 1998-03-20.

*note .root.repr;: 139d. Functionally, roots are defined by their
scanning functions.  Roots 'could' be represented as function closures:
that is, a pointer to a C function and some auxiliary fields.  The most
general variant of roots is just that.  However, for reasons of
efficiency, some special variants are separated out.


File: MemoryPoolSystem.info,  Node: Details<2>,  Prev: Basics,  Up: Root manager

5.25.2 Details
--------------

* Menu:

* Creation::
* Destruction::
* Invariants: Invariants<2>.
* Scanning: Scanning<3>.


File: MemoryPoolSystem.info,  Node: Creation,  Next: Destruction,  Up: Details<2>

5.25.2.1 Creation
.................

*note .create;: 13a0. A root becomes “active” as soon as it is created.

*note .create.col;: 13a1. The root inherits its colour from the mutator,
since it can only contain references copied there by the mutator from
somewhere else.  If the mutator is grey for a trace when a root is
created then that root will be used to determine accessibility for that
trace.  More specifically, the root will be scanned when that trace
flips.


File: MemoryPoolSystem.info,  Node: Destruction,  Next: Invariants<2>,  Prev: Creation,  Up: Details<2>

5.25.2.2 Destruction
....................

*note .destroy;: 13a3. It’s OK to destroy a root at any time, except
perhaps concurrently with scanning it, but that’s prevented by the arena
lock.  If a root is destroyed the references in it become invalid and
unusable.


File: MemoryPoolSystem.info,  Node: Invariants<2>,  Next: Scanning<3>,  Prev: Destruction,  Up: Details<2>

5.25.2.3 Invariants
...................

*note .inv.white;: 13a5. Roots are never white for any trace, because
they cannot be condemned.

*note .inv.rank;: 13a6. Roots always have a single rank.  A root without
ranks would be a root without references, which would be pointless.  The
tracer doesn’t support multiple ranks in a single colour.


File: MemoryPoolSystem.info,  Node: Scanning<3>,  Prev: Invariants<2>,  Up: Details<2>

5.25.2.4 Scanning
.................

*note .method;: 13a8. Root scanning methods are provided by the client
so that the MPS can locate and scan the root set.  See protocol.mps.root
for details.

     Note: There are some more notes about root methods in
     meeting.qa.1996-10-16.


File: MemoryPoolSystem.info,  Node: The generic scanner,  Next: Segment data structure,  Prev: Root manager,  Up: Old design

5.26 The generic scanner
========================

* Menu:

* Summaries::


File: MemoryPoolSystem.info,  Node: Summaries,  Up: The generic scanner

5.26.1 Summaries
----------------

* Menu:

* Scanned summary::
* Partial scans::


File: MemoryPoolSystem.info,  Node: Scanned summary,  Next: Partial scans,  Up: Summaries

5.26.1.1 Scanned summary
........................

*note .summary.subset;: 13af. The summary of reference seens by scan
(‘ss.unfixedSummary’) is a subset of the summary previously computed
(‘SegSummary()’).

There are two reasons that it is not an equality relation:

  1. If the segment has had objects forwarded onto it then its summary
     will get unioned with the summary of the segment that the object
     was forwarded from.  This may increase the summary.  The forwarded
     object of course may have a smaller summary (if such a thing were
     to be computed) and so subsequent scanning of the segment may
     reduce the summary.  (The forwarding process may erroneously
     introduce zones into the destination’s summary).

  2. A write barrier hit will set the summary to ‘RefSetUNIV’.

The reason that ‘ss.unfixedSummary’ is always a subset of the previous
summary is due to an “optimization” which has not been made in
‘TraceFix()’.  See design.mps.trace.fix.fixed.all(1).

   ---------- Footnotes ----------

   (1) trace.html#design.mps.trace.fix.fixed.all


File: MemoryPoolSystem.info,  Node: Partial scans,  Prev: Scanned summary,  Up: Summaries

5.26.1.2 Partial scans
......................

*note .clever-summary;: 13b2. With enough cleverness, it’s possible to
have partial scans of condemned segments contribute to the segment
summary.

     Note: We had a system which nearly worked – see
     MMsrc(MMdevel_poolams at 1997/08/14 13:02:55 BST), but it did not
     handle the situation in which a segment was not under the write
     barrier when it was condemned.

*note .clever-summary.acc;: 13b3. Each time we partially scan a segment,
we accumulate the post-scan summary of the scanned objects into a field
in the group, called ‘summarySoFar’.  The post-scan summary is (summary
white) ∪ fixed.

*note .clever-summary.acc.condemn;: 13b4. The cumulative summary is only
meaningful while the segment is condemned.  Otherwise it is set to
‘RefSetEMPTY’ (a value which we can check).

*note .clever-summary.acc.reclaim;: 13b5. Then when we reclaim the
segment, we set the segment summary to the cumulative summary, as it is
a post-scan summary of all the scanned objects.

*note .clever-summary.acc.other-trace;: 13b6. If the segment is scanned
by another trace while it is condemned, the cumulative summary must be
set to the post-scan summary of this scan (otherwise it becomes
out-of-date).

*note .clever-summary.scan;: 13b7. The scan summary is expected to be a
summary of all scanned references in the segment.  We don’t know this
accurately until we’ve scanned everything in the segment.  So we add in
the segment summary each time.

*note .clever-summary.scan.fix;: 13b8. ‘traceScanSeg()’ also expects the
scan state fixed summary to include the post-scan summary of all
references which were white.  Since we don’t scan all white references,
we need to add in an approximation to the summary of all white
references which we didn’t scan.  This is the intersection of the
segment summary and the white summary.

*note .clever-summary.wb;: 13b9. If the cumulative summary is smaller
than the mutator’s summary, a write-barrier is needed to prevent the
mutator from invalidating it.  This means that sometimes we’d have to
put the segment under the write-barrier at condemn, which might not be
very efficient

     Note: This is not an operation currently available to pool class
     implementations Pekka P. Pirinen, 1998-02-26.

*note .clever-summary.method.wb;: 13ba. We need a new pool class method,
called when the write barrier is hit (or possibly any barrier hit).  The
generic method will do the usual TraceAccess work, the trivial method
will do nothing.

*note .clever-summary.acc.wb;: 13bb. When the write barrier is hit, we
need to correct the cumulative summary to the mutator summary.  This is
approximated by setting the summary to ‘RefSetUNIV’.


File: MemoryPoolSystem.info,  Node: Segment data structure,  Next: MPS Strategy,  Prev: The generic scanner,  Up: Old design

5.27 Segment data structure
===========================

* Menu:

* Introduction: Introduction<71>.
* Overview: Overview<31>.
* Data Structure::
* Interface: Interface<29>.
* Extensibility::


File: MemoryPoolSystem.info,  Node: Introduction<71>,  Next: Overview<31>,  Up: Segment data structure

5.27.1 Introduction
-------------------

*note .intro;: 13c1. This is the design of the segment data structure.


File: MemoryPoolSystem.info,  Node: Overview<31>,  Next: Data Structure,  Prev: Introduction<71>,  Up: Segment data structure

5.27.2 Overview
---------------

*note .over.segments;: 13c3. Segments are the basic units of tracing and
shielding.  The MPM also uses them as units of scanning and colour,
although pool classes may subdivide segments and be able to maintain
colour on a finer grain (down to the object level, for example).

*note .over.objects;: 13c4. The mutator’s objects are stored in
segments.  Segments are contiguous blocks of memory managed by some
pool.

*note .segments.pool;: 13c5. The arrangement of objects within a segment
is determined by the class of the pool which owns the segment.  The pool
is associated with the segment indirectly via the first tract of the
segment.

*note .over.memory;: 13c6. The relationship between segments and areas
of memory is maintained by the segment module.  Pools acquire tracts
from the arena, and release them back to the arena when they don’t need
them any longer.  The segment module can associate contiguous tracts
owned by the same pool with a segment.  The segment module provides the
methods SegBase, SegLimit, and SegSize which map a segment onto the
addresses of the memory block it represents.

*note .over.hierarchy;: 13c7. The Segment datastructure is designed to
be subclassable (see design.mps.protocol(1)).  The basic segment class
(*note Seg: b53.) supports colour and protection for use by the tracer,
as well as support for a pool ring, and all generic segment functions.
Clients may use *note Seg: b53. directly, but will most probably want to
use a subclass with additional properties.

*note .over.hierarchy.gcseg;: 13c8. ‘GCSeg’ is a subclass of *note Seg:
b53. which implements garbage collection, including buffering and the
ability to be linked onto the grey ring.  It does not implement hardware
barriers, and so can only be used with software barriers, for example
internally in the MPS.

*note .over.hierarchy.mutatorseg;: 13c9. ‘MutatorSeg’ is a subclass of
‘GCSeg’ implementing hardware barriers.  It is suitable for handing out
to the mutator.

   ---------- Footnotes ----------

   (1) protocol.html


File: MemoryPoolSystem.info,  Node: Data Structure,  Next: Interface<29>,  Prev: Overview<31>,  Up: Segment data structure

5.27.3 Data Structure
---------------------

 -- C Type: typedef struct SegStruct *Seg

 -- C Type: typedef struct GCSegStruct *GCSeg

The implementations are as follows:

     typedef struct SegStruct {      /* segment structure */
       Sig sig;                      /* <code/misc.h#sig> */
       SegClass class;               /* segment class structure */
       Tract firstTract;             /* first tract of segment */
       RingStruct poolRing;          /* link in list of segs in pool */
       Addr limit;                   /* limit of segment */
       unsigned depth : ShieldDepthWIDTH; /* see <code/shield.c#def.depth> */
       AccessSet pm : AccessLIMIT;   /* protection mode, <code/shield.c> */
       AccessSet sm : AccessLIMIT;   /* shield mode, <code/shield.c> */
       TraceSet grey : TraceLIMIT;   /* traces for which seg is grey */
       TraceSet white : TraceLIMIT;  /* traces for which seg is white */
       TraceSet nailed : TraceLIMIT; /* traces for which seg has nailed objects */
       RankSet rankSet : RankLIMIT;  /* ranks of references in this seg */
     } SegStruct;

     typedef struct GCSegStruct {    /* GC segment structure */
       SegStruct segStruct;          /* superclass fields must come first */
       RingStruct greyRing;          /* link in list of grey segs */
       RefSet summary;               /* summary of references out of seg */
       Buffer buffer;                /* non-NULL if seg is buffered */
       Sig sig;                      /* design.mps.sig */
     } GCSegStruct;

*note .field.rankSet;: 13cc. The ‘rankSet’ field represents the set of
ranks of the references in the segment.  It is initialized to empty by
‘SegInit()’.

*note .field.rankSet.single;: 13cd. The Tracer only permits one rank per
segment [ref?]  so this field is either empty or a singleton.

*note .field.rankSet.empty;: 13ce. An empty ‘rankSet’ indicates that
there are no references.  If there are no references in the segment then
it cannot contain black or grey references.

*note .field.rankSet.start;: 13cf. If references are stored in the
segment then it must be updated, along with the summary (*note
.field.summary.start: 13d0.).

*note .field.depth;: 13d1. The ‘depth’ field is used by the Shield
(impl.c.shield) to manage protection of the segment.  It is initialized
to zero by ‘SegInit()’.

*note .field.sm;: 13d2. The ‘sm’ field is used by the Shield
(impl.c.shield) to manage protection of the segment.  It is initialized
to ‘AccessSetEMPTY’ by ‘SegInit()’.

*note .field.pm;: 13d3. The ‘pm’ field is used by the Shield
(impl.c.shield) to manage protection of the segment.  It is initialized
to ‘AccessSetEMPTY’ by ‘SegInit()’.  The field is used by both the
shield and the ANSI fake protection (impl.c.protan).

*note .field.black;: 13d4. The ‘black’ field is the set of traces for
which there may be black objects (that is, objects containing
references, but no references to white objects) in the segment.  More
precisely, if there is a black object for a trace in the segment then
that trace will appear in the ‘black’ field.  It is initialized to
‘TraceSetEMPTY’ by ‘SegInit()’.

*note .field.grey;: 13d5. The ‘grey’ field is the set of traces for
which there may be grey objects (i.e containing references to white
objects) in the segment.  More precisely, if there is a reference to a
white object for a trace in the segment then that trace will appear in
the ‘grey’ field.  It is initialized to ‘TraceSetEMPTY’ by ‘SegInit()’.

*note .field.white;: 13d6. The ‘white’ field is the set of traces for
which there may be white objects in the segment.  More precisely, if
there is a white object for a trace in the segment then that trace will
appear in the ‘white’ field.  It is initialized to ‘TraceSetEMPTY’ by
‘SegInit()’.

*note .field.summary;: 13d7. The ‘summary’ field is an approximation to
the set of all references in the segment.  If there is a reference ‘R’
in the segment, then ‘RefSetIsMember(summary, R)’ is ‘TRUE’.  The
summary is initialized to ‘RefSetEMPTY’ by ‘SegInit()’.

*note .field.summary.start;: 13d0. If references are stored in the
segment then it must be updated, along with ‘rankSet’ (*note
.field.rankSet.start: 13cf.).

*note .field.buffer;: 13d8. The ‘buffer’ field is either ‘NULL’, or
points to the descriptor structure of the buffer which is currently
allocating in the segment.  The field is initialized to ‘NULL’ by
‘SegInit()’.

*note .field.buffer.owner;: 13d9. This buffer must belong to the same
pool as the segment, because only that pool has the right to attach it.


File: MemoryPoolSystem.info,  Node: Interface<29>,  Next: Extensibility,  Prev: Data Structure,  Up: Segment data structure

5.27.4 Interface
----------------

* Menu:

* Splitting and merging::


File: MemoryPoolSystem.info,  Node: Splitting and merging,  Up: Interface<29>

5.27.4.1 Splitting and merging
..............................

*note .split-and-merge;: 13dc. There is support for splitting and
merging segments, to give pools the flexibility to rearrange their
tracts among segments as they see fit.

 -- C Function: *note Res: 55f. SegSplit (Seg *segLoReturn, Seg
          *segHiReturn, Seg seg, Addr at)

*note .split;: 13dd. If successful, segment ‘seg’ is split at address
‘at’, yielding two segments which are returned in segLoReturn and
segHiReturn for the low and high segments respectively.  The base of the
low segment is the old base of ‘seg’.  The limit of the low segment is
‘at’.  The base of the high segment is ‘at’.  This limit of the high
segment is the old limit of ‘seg’.  ‘seg’ is effectively destroyed
during this operation (actually, it might be reused as one of the
returned segments).  Segment subclasses may make use of the optional
arguments; the built-in classes do not.

*note .split.invariants;: 13de. The client must ensure some invariants
are met before calling *note SegSplit(): 10b3.:

   - *note .split.inv.align;: 13df. ‘at’ must be a multiple of the arena
     grain size, and lie between the base and limit of ‘seg’.
     Justification: the split segments cannot be represented if this is
     not so.

   - *note .split.inv.buffer;: 13e0. If ‘seg’ is attached to a buffer,
     the buffered region must not include address ‘at’.  Justification:
     the segment module is not in a position to know how (or whether) a
     pool might wish to split a buffer.  This permits the buffer to
     remain attached to just one of the returned segments.

*note .split.state;: 13e1. Except as noted above, the segments returned
have the same properties as ‘seg’.  That is, their colour, summary,
rankset, nailedness etc.  are set to the values of ‘seg’.

 -- C Function: *note Res: 55f. SegMerge (Seg *mergedSegReturn, Seg
          segLo, Seg segHi)

*note .merge;: 13e2. If successful, segments ‘segLo’ and ‘segHi’ are
merged together, yielding a segment which is returned in
mergedSegReturn.  ‘segLo’ and ‘segHi’ are effectively destroyed during
this operation (actually, one of them might be reused as the merged
segment).  Segment subclasses may make use of the optional arguments;
the built-in classes do not.

*note .merge.invariants;: 13e3. The client must ensure some invariants
are met before calling *note SegMerge(): 10b2.:

   - *note .merge.inv.abut;: 13e4. The limit of ‘segLo’ must be the same
     as the base of ‘segHi’.  Justification: the merged segment cannot
     be represented if this is not so.

   - *note .merge.inv.buffer;: 13e5. One or other of ‘segLo’ and ‘segHi’
     may be attached to a buffer, but not both.  Justification: the
     segment module does not support attachment of a single seg to 2
     buffers.

   - *note .merge.inv.similar;: 13e6. ‘segLo’ and ‘segHi’ must be
     sufficiently similar.  Two segments are sufficiently similar if
     they have identical values for each of the following fields:
     ‘class’, ‘grey’, ‘white’, ‘nailed’, ‘rankSet’.  Justification:
     There has yet to be a need to implement default behaviour for these
     cases.  Pool classes should arrange for these values to be the same
     before calling *note SegMerge(): 10b2.

*note .merge.state;: 13e7. The merged segment will share the same state
as ‘segLo’ and ‘segHi’ for those fields which are identical (see *note
.merge.inv.similar: 13e6.).  The summary will be the union of the
summaries of ‘segLo’ and ‘segHi’.


File: MemoryPoolSystem.info,  Node: Extensibility,  Prev: Interface<29>,  Up: Segment data structure

5.27.5 Extensibility
--------------------

* Menu:

* Allocation: Allocation<6>.
* Garbage collection: Garbage collection<2>.
* Splitting and merging: Splitting and merging<2>.


File: MemoryPoolSystem.info,  Node: Allocation<6>,  Next: Garbage collection<2>,  Up: Extensibility

5.27.5.1 Allocation
...................

 -- C Type: typedef *note Bool: 3a9. (*SegBufferFillMethod)(*note Addr:
          632. *baseReturn, *note Addr: 632. *limitReturn, *note Seg:
          b53. seg, *note Size: 40e. size, *note RankSet: b21. rankSet)

*note .method.buffer-fill;: 13eb. Allocate a block in the segment, of at
least ‘size’ bytes, with the given set of ranks.  If successful, update
‘*baseReturn’ and ‘*limitReturn’ to the block and return ‘TRUE’.
Otherwise, return ‘FALSE’.  The allocated block must be accounted as
buffered (see design.mps.strategy.account.buffered(1)).

 -- C Type: typedef void (*SegBufferEmptyMethod)(*note Seg: b53. seg,
          Buffer buffer)

*note .method.buffer-empty;: 13ed. Free the unused part of the buffer to
the segment.  Account the used part as new (see
design.mps.strategy.account.new(2)) and the unused part as free (see
design.mps.strategy.account.free(3)).

   ---------- Footnotes ----------

   (1) strategy.html#design.mps.strategy.account.buffered

   (2) strategy.html#design.mps.strategy.account.new

   (3) strategy.html#design.mps.strategy.account.free


File: MemoryPoolSystem.info,  Node: Garbage collection<2>,  Next: Splitting and merging<2>,  Prev: Allocation<6>,  Up: Extensibility

5.27.5.2 Garbage collection
...........................

 -- C Type: typedef *note Res: 55f. (*SegAccessMethod)(*note Seg: b53.
          seg, *note Arena: 796. arena, *note Addr: 632. addr, *note
          AccessSet: 8ce. mode, *note MutatorContext: 7bc. context)

*note .method.access;: 13f1. The ‘access’ method indicates that the
client program attempted to access the address ‘addr’, but has been
denied due to a protection fault.  The ‘mode’ indicates whether the
client program was trying to read (‘AccessREAD’) or write
(‘AccessWRITE’) the address.  If this can’t be determined, ‘mode’ is
‘AccessREAD | AccessWRITE’.  The segment should perform any work
necessary to remove the protection whilst still preserving appropriate
invariants (this might scanning the region containing ‘addr’).  Segment
classes are not required to provide this method, and not doing so
indicates they never protect any memory managed by the pool.  This
method is called via the generic function ‘SegAccess()’.

 -- C Type: typedef *note Res: 55f. (*SegWhitenMethod)(*note Seg: b53.
          seg, Trace trace)

*note .method.whiten;: 13f3. The ‘whiten’ method requests that the
segment ‘seg’ condemn (a subset of, but typically all) its objects for
the trace ‘trace’.  That is, prepare them for participation in the trace
to determine their liveness.  The segment should expect fix requests
(*note .method.fix: 13f4.) during the trace and a reclaim request (*note
.method.reclaim: 13f5.) at the end of the trace.  Segment classes that
automatically reclaim dead objects must provide this method, and pools
that use these segment classes must additionally set the ‘AttrGC’
attribute.  This method is called via the generic function
‘SegWhiten()’.

 -- C Type: typedef void (*SegGreyenMethod)(*note Seg: b53. seg, Trace
          trace)

*note .method.grey;: 13f7. The ‘greyen’ method requires the segment
‘seg’ to colour its objects grey for the trace ‘trace’ (excepting
objects that were already condemned for this trace).  That is, make them
ready for scanning by the trace ‘trace’.  The segment must arrange that
any appropriate invariants are preserved, possibly by using the
protection interface (see design.mps.prot(1)).  Segment classes are not
required to provide this method, and not doing so indicates that all
instances of this class will have no fixable or traceable references in
them.  This method is called via the generic function ‘SegGreyen()’.

 -- C Type: typedef void (*SegBlackenMethod)(*note Seg: b53. seg, *note
          TraceSet: b3e. traceSet)

*note .method.blacken;: 13f9. The ‘blacken’ method is called if it is
known that the segment ‘seg’ cannot refer to the white set for any of
the traces in ‘traceSet’.  The segment must blacken all its grey objects
for those traces.  Segment classes are not required to provide this
method, and not doing so indicates that all instances of this class will
have no fixable or traceable references in them.  This method is called
via the generic function ‘SegBlacken()’.

 -- C Type: typedef *note Res: 55f. (*SegScanMethod)(*note Bool: 3a9.
          *totalReturn, *note Seg: b53. seg, ScanState ss)

*note .method.scan;: 13fb. The ‘scan’ method scans all the grey objects
on the segment ‘seg’, passing the scan state ‘ss’ to
‘TraceScanFormat()’.  The segment may additionally accumulate a summary
of 'all' its objects.  If it succeeds in accumulating such a summary it
must indicate that it has done so by setting the ‘*totalReturn’
parameter to ‘TRUE’.  Otherwise it must set ‘*totalReturn’ to ‘FALSE’.
This method is called via the generic function ‘SegScan()’.

*note .method.scan.required;: 13fc. Automatically managed segment
classes are required to provide this method, even if all instances of
this class will have no fixable or traceable references in them, in
order to support *note mps_pool_walk(): 1a6.

 -- C Type: typedef *note Res: 55f. (*SegFixMethod)(*note Seg: b53. seg,
          ScanState ss, *note Ref: b24. *refIO)

*note .method.fix;: 13f4. The ‘fix’ method indicates that the reference
‘*refIO’ has been discovered at rank ‘ss->rank’ by the traces in
‘ss->traces’, and the segment must handle this discovery according to
the fix protocol (design.mps.fix(2)).  If the method moves the object,
it must update ‘*refIO’ to refer to the new location of the object.  If
the method determines that the referenced object died (for example,
because the highest-ranking references to the object were weak), it must
update ‘*refIO’ to ‘NULL’.  Segment classes that automatically reclaim
dead objects must provide this method, and pools that use these classes
must additionally set the ‘AttrGC’ attribute.  Pool classes that use
segment classes that may move objects must also set the ‘AttrMOVINGGC’
attribute.  The ‘fix’ method is on the critical path (see
design.mps.critical-path(3)) and so must be fast.  This method is called
via the function ‘TraceFix()’.

*note .method.fixEmergency;: 13fe. The ‘fixEmergency’ method is used to
perform fixing in “emergency” situations.  Its specification is
identical to the ‘fix’ method, but it must complete its work without
allocating memory (perhaps by using some approximation, or by running
more slowly).  Segment classes must provide this method if and only if
they provide the ‘fix’ method.  If the ‘fix’ method does not need to
allocate memory, then it is acceptable for ‘fix’ and ‘fixEmergency’ to
be the same.

 -- C Type: typedef void (*SegReclaimMethod)(*note Seg: b53. seg, Trace
          trace)

*note .method.reclaim;: 13f5. The ‘reclaim’ method indicates that any
remaining white objects in the segment ‘seg’ have now been proved
unreachable by the trace ‘trace’, and so are dead.  The segment should
reclaim the resources associated with the dead objects.  Segment classes
are not required to provide this method.  If they do, pools that use
them must set the ‘AttrGC’ attribute.  This method is called via the
generic function ‘SegReclaim()’.

 -- C Type: typedef void (*SegWalkMethod)(*note Seg: b53. seg, Format
          format, FormattedObjectsVisitor f, void *v, size_t s)

*note .method.walk;: 1401. The ‘walk’ method must call the visitor
function ‘f’ (along with its closure parameters ‘v’ and ‘s’ and the
format ‘format’) once for each of the 'black' objects in the segment
‘seg’.  Padding objects may or may not be included in the walk, at the
segment’s discretion: it is the responsibility of the client program to
handle them.  Forwarding objects must not be included in the walk.
Segment classes need not provide this method.  This method is called by
the generic function ‘SegWalk()’, which is called by the deprecated
public functions *note mps_arena_formatted_objects_walk(): 322. and
*note mps_amc_apply(): 324.

*note .method.walk.deprecated;: 1402. The ‘walk’ method is deprecated
along with the public functions *note
mps_arena_formatted_objects_walk(): 322. and *note mps_amc_apply(): 324.
and will be removed along with them in a future release.

 -- C Type: typedef void (*SegFlipMethod)(*note Seg: b53. seg, Trace
          trace)

*note .method.flip;: 1404. Raise the read barrier, if necessary, for a
trace that’s about to flip and for which the segment is grey and
potentially contains references.

   ---------- Footnotes ----------

   (1) prot.html

   (2) fix.html

   (3) critical-path.html


File: MemoryPoolSystem.info,  Node: Splitting and merging<2>,  Prev: Garbage collection<2>,  Up: Extensibility

5.27.5.3 Splitting and merging
..............................

 -- C Type: typedef *note Res: 55f. (*SegSplitMethod)(*note Seg: b53.
          seg, *note Seg: b53. segHi, *note Addr: 632. base, *note Addr:
          632. mid, *note Addr: 632. limit)

*note .method.split;: 1407. Segment subclasses may extend the support
for segment splitting by defining their own “split” method.  On entry,
‘seg’ is a segment with region ‘[base,limit)’, ‘segHi’ is uninitialized,
‘mid’ is the address at which the segment is to be split.  The method is
responsible for destructively modifying ‘seg’ and initializing ‘segHi’
so that on exit ‘seg’ is a segment with region ‘[base,mid)’ and ‘segHi’
is a segment with region ‘[mid,limit)’.  Usually a method would only
directly modify the fields defined for the segment subclass.

*note .method.split.next;: 1408. A split method should always call the
next method, either before or after any class-specific code (see
design.mps.protocol.overview.next-method(1)).

*note .method.split.accounting;: 1409. If ‘seg’ belongs to a generation
in a chain, then the pool generation accounting must be updated.  In the
simple case where the split segments remain in the same generation, this
can be done by calling ‘PoolGenAccountForSegSplit()’.

 -- C Type: typedef *note Res: 55f. (*SegMergeMethod)(*note Seg: b53.
          seg, *note Seg: b53. segHi, *note Addr: 632. base, *note Addr:
          632. mid, *note Addr: 632. limit)

*note .method.merge;: 140b. Segment subclasses may extend the support
for segment merging by defining their own ‘merge’ method.  On entry,
‘seg’ is a segment with region ‘[base,mid)’, ‘segHi’ is a segment with
region ‘[mid,limit)’, The method is responsible for destructively
modifying ‘seg’ and finishing ‘segHi’ so that on exit ‘seg’ is a segment
with region ‘[base,limit)’ and ‘segHi’ is garbage.  Usually a method
would only modify the fields defined for the segment subclass.

*note .method.merge.next;: 140c. A merge method should always call the
next method, either before or after any class-specific code (see
design.mps.protocol.overview.next-method(2)).

*note .method.merge.accounting;: 140d. If ‘seg’ belongs to a generation
in a chain, then the pool generation accounting must be updated.  In the
simple case where the two segments started in the same generation and
the merged segment remains in that generation, this can be done by
calling ‘PoolGenAccountForSegMerge()’.

*note .split-merge.shield;: 140e. Split and merge methods may assume
that the segments they are manipulating are not in the shield queue.

*note .split-merge.shield.flush;: 140f. The shield queue is flushed
before any split or merge methods are invoked.

*note .split-merge.shield.re-flush;: 1410. If a split or merge method
performs an operation on a segment which might cause the segment to be
queued, the method must flush the shield queue before returning or
calling another split or merge method.

*note .split-merge.fail;: 1411. Split and merge methods might fail, in
which case segments ‘seg’ and ‘segHi’ must be equivalently valid and
configured at exit as they were according to the entry conditions.  It’s
simplest if the failure can be detected before calling the next method
(for example, by allocating any objects early in the method).

*note .split-merge.fail.anti;: 1412. If it’s not possible to detect
failure before calling the next method, the appropriate anti-method must
be used (see design.mps.protocol.guide.fail.after-next(3)).  Split
methods are anti-methods for merge methods, and vice-versa.

*note .split-merge.fail.anti.constrain;: 1413. In general, care should
be taken when writing split and merge methods to ensure that they really
are anti-methods for each other.  The anti-method must not fail if the
initial method succeeded.  The anti-method should reverse any side
effects of the initial method, except where it’s known to be safe to
avoid this (see *note .split-merge.fail.summary: 1414. for an example of
a safe case).

*note .split-merge.fail.anti.no;: 1415. If this isn’t possible (it might
not be) then the methods won’t support after-next failure.  This fact
should be documented, if the methods are intended to support further
specialization.  Note that using va_arg with the ‘args’ parameter is
sufficient to make it impossible to reverse all side effects.

*note .split-merge.fail.summary;: 1414. The segment summary might not be
restored exactly after a failed merge operation.  Each segment would be
left with a summary which is the union of the original summaries (see
*note .merge.state: 13e7.).  This increases the conservatism in the
summaries, but is otherwise safe.

*note .split-merge.unsupported;: 1416. Segment classes need not support
segment merging at all.  The function ‘SegClassMixInNoSplitMerge()’ is
supplied to set the split and merge methods to unsupporting methods that
will report an error in checking varieties.

   ---------- Footnotes ----------

   (1) protocol.html#design.mps.protocol.overview.next-method

   (2) protocol.html#design.mps.protocol.overview.next-method

   (3) protocol.html#design.mps.protocol.guide.fail.after-next


File: MemoryPoolSystem.info,  Node: MPS Strategy,  Next: Telemetry<3>,  Prev: Segment data structure,  Up: Old design

5.28 MPS Strategy
=================

* Menu:

* Introduction: Introduction<72>.
* Overview: Overview<32>.
* Requirements: Requirements<45>.
* Generations: Generations<2>.
* Policy: Policy<2>.
* References: References<25>.


File: MemoryPoolSystem.info,  Node: Introduction<72>,  Next: Overview<32>,  Up: MPS Strategy

5.28.1 Introduction
-------------------

.intro This is the design of collection strategy for the MPS.

.readership MPS developers.


File: MemoryPoolSystem.info,  Node: Overview<32>,  Next: Requirements<45>,  Prev: Introduction<72>,  Up: MPS Strategy

5.28.2 Overview
---------------

.overview The MPS uses “strategy” code to make three decisions:

   - when to start a collection trace;

   - what to condemn;

   - how to schedule tracing work.

This document describes the current strategy, identifies some weaknesses
in it, and outlines some possible future development directions.


File: MemoryPoolSystem.info,  Node: Requirements<45>,  Next: Generations<2>,  Prev: Overview<32>,  Up: MPS Strategy

5.28.3 Requirements
-------------------

[TODO: source some from req.dylan, or do an up-to-date requirements
analysis – NB 2013-03-25]

Garbage collection is a trade-off between time and space: it consumes
some [CPU] time in order to save some [memory] space.  Strategy shifts
the balance point.  A better strategy will take less time to produce
more space.  Examples of good strategy might include:

   - choosing segments to condemn which contain high proportions of dead
     objects;

   - starting a trace when a large number of objects have just died;

   - doing enough collection soon enough that the client program never
     suffers low-memory problems;

   - using otherwise-idle CPU resources for tracing.

Conversely, it would be bad strategy to do the reverse of each of these
(condemning live objects; tracing when there’s very little garbage; not
collecting enough; tracing when the client program is busy).

Abstracting from these notions, requirements on strategy would relate
to:

   - Maximum pause time and other utilization metrics (for example,
     bounded mutator utilization, minimum mutator utilization, total MPS
     CPU usage);

   - Collecting enough garbage (for example: overall heap size;
     low-memory requirements).

   - Allowing client control (for example, client recommendations for
     collection timing or condemnation).

There are other possible strategy considerations which are so far
outside the scope of current strategy and MPS design that this document
disregards them.  For example, either inferring or allowing the client
to specify preferred relative object locations (“this object should be
kept in the same cache line as that one”), to improve cache locality.


File: MemoryPoolSystem.info,  Node: Generations<2>,  Next: Policy<2>,  Prev: Requirements<45>,  Up: MPS Strategy

5.28.4 Generations
------------------

The largest part of the current MPS strategy implementation is the
support for generational garbage collections.

* Menu:

* General data structures::
* AMC data structures::
* Collections::
* Zones::
* Parameters::
* Accounting::
* Ramps: Ramps<2>.


File: MemoryPoolSystem.info,  Node: General data structures,  Next: AMC data structures,  Up: Generations<2>

5.28.4.1 General data structures
................................

The fundamental structure of generational garbage collection is the
‘Chain’, which describes a sequence of generations.

A chain specifies the “capacity” and “mortality” for each generation.
When creating an automatically collected pool, the client code may
specify the chain which will control collections for that pool.  The
same chain may be used for multiple pools.  If no chain is specified,
the pool uses the arena’s default generation chain.

Each generation in a chain has a ‘GenDesc’ structure, allocated in an
array pointed to from the chain.  In addition to the generations in the
chains, the arena has a unique ‘GenDesc’ structure, named ‘topGen’ and
described in comments as “the dynamic generation” (misleadingly: in fact
it is the 'least' dynamic generation).

Each automatically collected pool has a set of ‘PoolGen’ structures, one
for each generation that it can allocate or promote into.  The ‘PoolGen’
structures for each generation point to the ‘GenDesc’ for that
generation, and are linked together in a ring on the ‘GenDesc’.  These
structures are used to gather accounting information for strategy
decisions.

The non-moving automatic pool classes (AMS, AWL and LO) do not support
generational collection, so they allocate into a single generation.  The
moving automatic pool classes (AMC and AMCZ) have one pool generations
for each generation in the chain, plus one pool generation for the
arena’s “top generation”.


File: MemoryPoolSystem.info,  Node: AMC data structures,  Next: Collections,  Prev: General data structures,  Up: Generations<2>

5.28.4.2 AMC data structures
............................

An AMC pool creates an array of pool generation structures of type
‘amcGen’ (a subclass of ‘PoolGen’).  Each pool generation points to the
'forwarding buffer' for that generation: this is the buffer that
surviving objects are copied into.

AMC segments point to the AMC pool generation that the segment belongs
to, and AMC buffers point to the AMC pool generation that the buffer
will be allocating into.

The forwarding buffers are set up during AMC pool creation.  Each
generation forwards into the next higher generation in the chain, except
for the top generation, which forwards to itself.  Thus, objects are
“promoted” up the chain of generations until they end up in the top
generations, which is shared between all generational pools.


File: MemoryPoolSystem.info,  Node: Collections,  Next: Zones,  Prev: AMC data structures,  Up: Generations<2>

5.28.4.3 Collections
....................

Collections in the MPS start in one of two ways:

  1. A collection of the world starts via ‘TraceStartCollectAll()’.
     This simply condemns all segments in all automatic pools.

  2. A collection of some set of generations starts via *note
     PolicyStartTrace(): 1425.  See *note .policy.start: 1426.


File: MemoryPoolSystem.info,  Node: Zones,  Next: Parameters,  Prev: Collections,  Up: Generations<2>

5.28.4.4 Zones
..............

Each generation in each chain has a zoneset associated with it
(‘gen->zones’); the condemned zoneset is the union of some number of
generation’s zonesets.

An attempt is made to use distinct zonesets for different generations.
Segments in automatic pools are allocated using ‘PoolGenAlloc()’ which
creates a ‘LocusPref’ using the zoneset from the generation’s ‘GenDesc’.
The zoneset for each generation starts out empty.  If the zoneset is
empty, an attempt is made to allocate from a free zone.  The ‘GenDesc’
zoneset is augmented with whichever zones the new segment occupies.

Note that this zoneset can never shrink.


File: MemoryPoolSystem.info,  Node: Parameters,  Next: Accounting,  Prev: Zones,  Up: Generations<2>

5.28.4.5 Parameters
...................

*note .param.intro;: 1429. A generation has two parameters, 'capacity'
and 'mortality', specified by the client program.

*note .param.capacity;: 142a. The 'capacity' of a generation is the
amount of 'new' allocation in that generation (that is, allocation since
the last time the generation was condemned) that will cause the
generation to be collected by ‘TracePoll()’.

*note .param.capacity.misnamed;: 142b. The name 'capacity' is
unfortunate since it suggests that the total amount of memory in the
generation will not exceed this value.  But that will only be the case
for pool classes that always promote survivors to another generation.
When there is 'old' allocation in the generation (that is, prior to the
last time the generation was condemned), as there is in the case of
non-moving pool classes, the size of a generation is unrelated to its
capacity.

*note .param.mortality;: 142c. The 'mortality' of a generation is the
proportion (between 0 and 1) of memory in the generation that is
expected to be dead when the generation is collected.  It is used in
‘TraceStart()’ to estimate the amount of data that will have to be
scanned in order to complete the trace.


File: MemoryPoolSystem.info,  Node: Accounting,  Next: Ramps<2>,  Prev: Parameters,  Up: Generations<2>

5.28.4.6 Accounting
...................

*note .accounting.intro;: 142e. Pool generations maintain the sizes of
various categories of data allocated in that generation for that pool.
This accounting information is reported via the event system, but also
used in two places:

*note .accounting.poll;: 142f. ‘ChainDeferral()’ uses the 'new size' of
each generation to determine which generations in the chain are over
capacity and so might need to be collected by *note PolicyStartTrace():
1425.

*note .accounting.condemn;: 1430. *note PolicyStartTrace(): 1425. uses
the 'new size' of each generation to determine which generations in the
chain will be collected; it also uses the 'total size' of the generation
to compute the mortality.

*note .accounting.check;: 1431. Computing the new size for a pool
generation is far from straightforward: see job003772(1) and
job004007(2) for some (former) errors in this code.  In order to assist
with checking that this has been computed correctly, the locus module
uses a double-entry book-keeping system to account for every byte in
each pool generation.  This uses seven accounts:

*note .account.total;: 1432. Memory acquired from the arena.

*note .account.total.negated;: 1433. From the point of view of the
double-entry system, the 'total' should be negative as it is owing to
the arena, but it is inconvenient to represent negative sizes, and so
the positive value is stored instead.

*note .account.total.negated.justification;: 1434. We don’t have a type
for signed sizes; but if we represented it in two’s complement using the
unsigned *note Size: 40e. type then Clang’s unsigned integer overflow
detector would complain.

*note .account.free;: 1435. Memory that is not in use (free or lost to
fragmentation).

*note .account.buffered;: 1436. Memory in a buffer that was handed out
to the client program via *note BufferFill(): 7a2, and which has not yet
been condemned.

*note .account.new;: 1437. Memory in use by the client program,
allocated since the last time the generation was condemned.

*note .account.old;: 1438. Memory in use by the client program,
allocated prior to the last time the generation was condemned.

*note .account.newDeferred;: 1439. Memory in use by the client program,
allocated since the last time the generation was condemned, but which
should not cause collections via ‘TracePoll()’.  (Due to ramping; see
below.)

*note .account.oldDeferred;: 143a. Memory in use by the client program,
allocated prior to the last time the generation was condemned, but which
should not cause collections via ‘TracePoll()’.  (Due to ramping; see
below.)

*note .accounting.op;: 143b. The following operations are provided:

*note .accounting.op.alloc;: 143c. Allocate a segment in a pool
generation.  Debit 'total', credit 'free'.  (But see *note
.account.total.negated: 1433.)

*note .accounting.op.free;: 143d. Free a segment.  First, ensure that
the contents of the segment are accounted as free, by artificially
ageing any memory accounted as 'new' or 'newDeferred' (see *note
.accounting.op.age: 143e.) and then artificially reclaiming any memory
accounted as 'old' or 'oldDeferred' (see *note .accounting.op.reclaim:
143f.).  Finally, debit 'free', credit 'total'.  (But see *note
.account.total.negated: 1433.)

*note .accounting.op.fill;: 1440. Fill a buffer.  Debit 'free', credit
'buffered'.

*note .accounting.op.empty;: 1441. Empty a buffer.  Debit 'buffered',
credit 'new' or 'newDeferred' with the allocated part of the buffer,
credit 'free' with the unused part of the buffer.

*note .accounting.op.age;: 143e. Condemn memory.  Debit 'buffered' (if
part or all of a buffer was condemned) and either 'new' or
'newDeferred', credit 'old' or 'oldDeferred'.  Note that the condemned
part of the buffer remains part of the buffer until the buffer is
emptied, but is now accounted as 'old' or 'oldDeferred'.  The
uncondemned part of the buffer, if any, remains accounted as 'buffered'
until it is either emptied or condemned in its turn.

*note .accounting.op.reclaim;: 143f. Reclaim dead memory.  Debit 'old'
or 'oldDeferred', credit 'free'.

*note .accounting.op.undefer;: 1442. Stop deferring the accounting of
memory.  Debit 'oldDeferred', credit 'old'.  Debit 'newDeferred', credit
'new'.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job003772/

   (2) https://www.ravenbrook.com/project/mps/issue/job004007/


File: MemoryPoolSystem.info,  Node: Ramps<2>,  Prev: Accounting,  Up: Generations<2>

5.28.4.7 Ramps
..............

The intended semantics of ramping are pretty simple.  It allows the
client to advise us of periods of large short-lived allocation on a
particular AP. Stuff allocated using that AP during its “ramp” will
probably be dead when the ramp finishes.  How the MPS makes use of this
advice is up to us, but for instance we might segregate those objects,
collect them less enthusiastically during the ramp and then more
enthusiastically soon after the ramp finishes.  Ramps can nest.

A ramp is entered by calling:

     mps_ap_alloc_pattern_begin(ap, mps_alloc_pattern_ramp())

or similar, and left in a similar way.

This is implemented on a per-pool basis, for AMC only (it’s ignored by
the other automatic pools).  PoolAMC throws away the identity of the AP
specified by the client.  The implementation is intended to work by
changing the generational forwarding behaviour, so that there is a “ramp
generation” - one of the regular AMC generations - which forwards to
itself if collected during a ramp (instead of promoting to an older
generation).  It also tweaks the strategy calculation code, in a way
with consequences I am documenting elsewhere.

Right now, the code sets this ramp generation to the last generation
specified in the pool’s “chain”: it ordinarily forwards to the
“after-ramp” generation, which is the “dynamic generation” (i.e.  the
least dynamic generation, i.e.  the arena-wide “top generation”).  My
recollection, and some mentions in design/poolamc, suggests that the
ramp generation used to be chosen differently from this.

So far, it doesn’t sound too ghastly, I guess, although the subversion
of the generational system seems a little daft.  Read on….

An AMC pool has a ‘rampMode’ (which is really a state of a state
machine), taking one of five values: OUTSIDE, BEGIN, RAMPING, FINISH,
and COLLECTING (actually the enum values are called RampX for these X).
We initialize in OUTSIDE. The pool also has a ‘rampCount’, which is the
ramp nesting depth and is used to allow us to ignore ramp transitions
other than the outermost.  According to design/poolamc, there’s an
invariant (in BEGIN or RAMPING, ‘rampCount > 0’; in COLLECTING or
OUTSIDE, ‘rampCount == 0’), but this isn’t checked in ‘AMCCheck()’ and
in fact is false for COLLECTING (see below).

There is a small set of events causing state machine transitions:

   - entering an outermost ramp;

   - leaving an outermost ramp;

   - condemning any segment of a ramp generation (detected in
     AMCWhiten);

   - reclaiming any AMC segment.

Here’s pseudo-code for all the transition events:

Entering an outermost ramp:

     if not FINISH, go to BEGIN.

Leaving an outermost ramp:

     if RAMPING, go to FINISH. Otherwise, go to OUTSIDE.

Condemning a ramp generation segment:

     If BEGIN, go to RAMPING and make the ramp generation forward to
     itself (detach the forwarding buffer and reset its generation).  If
     FINISH, go to COLLECTING and make the ramp generation forward to
     the after-ramp generation.

Reclaiming any AMC segment:

     If COLLECTING:

          if ‘rampCount > 0’, go to BEGIN. Otherwise go to OUTSIDE.

Now, some deductions:

  1. When OUTSIDE, the count is always zero, because (a) it starts that
     way, and the only ways to go OUTSIDE are (b) by leaving an
     outermost ramp (count goes to zero) or (c) by reclaiming when the
     count is zero.

  2. When BEGIN, the count is never zero (consider the transitions to
     BEGIN and the transition to zero).

  3. When RAMPING, the count is never zero (again consider transitions
     to RAMPING and the transition to zero).

  4. When FINISH, the count can be anything (the transition to FINISH
     has zero count, but the Enter transition when FINISH can change
     that and then it can increment to any value).

  5. When COLLECTING, the count can be anything (from the previous fact,
     and the transition to COLLECTING).

  6. 'This is a bug!!'  The ramp generation is not always reset (to
     forward to the after-ramp generation).  If we get into FINISH and
     then see another ramp before the next condemnation of the ramp
     generation, we will Enter followed by Leave.  The Enter will keep
     us in FINISH, and the Leave will take us back to OUTSIDE, skipping
     the transition to the COLLECTING state which is what resets the
     ramp generation forwarding buffer.  [TODO: check whether I made an
     issue and/or fixed it; NB 2013-06-04]

The simplest change to fix this is to change the behaviour of the Leave
transition, which should only take us OUTSIDE if we are in BEGIN or
COLLECTING. We should also update design/poolamc to tell the truth, and
check the invariants, which will be these:

     OUTSIDE => zero BEGIN => non-zero RAMPING => non-zero

A cleverer change might radically rearrange the state machine (e.g.
reduce the number of states to three) but that would require closer
design thought and should probably be postponed until we have a clearer
overall strategy plan.

While I’m writing pseudo-code versions of ramp-related code, I should
mention this other snippet, which is the only other code relating to
ramping (these notes are useful when thinking about the broader strategy
code):

     In *note AMCBufferFill(): 903, if we’re RAMPING, and filling the
     forwarding buffer of the ramp generation, and the ramp generation
     is the forwarding buffer’s generation, set ‘amcSeg->new’ to FALSE.
     Otherwise, add the segment size to ‘poolGen.newSize’.

And since I’ve now mentioned the ‘amcSeg->new’ flag, here are the only
other uses of that:

   - it initializes as TRUE.

   - When leaving an outermost ramp, go through all the segments in the
     pool.  Any non-white segment in the rampGen with new set to FALSE
     has its size added to ‘poolGen->newSize’ and gets new set to TRUE.

   - in ‘amcSegWhiten()’, if new is TRUE, the segment size is deducted
     from ‘poolGen.newSize’ and new is set to FALSE.


File: MemoryPoolSystem.info,  Node: Policy<2>,  Next: References<25>,  Prev: Generations<2>,  Up: MPS Strategy

5.28.5 Policy
-------------

*note .policy;: 1445. Functions that make decisions about what action to
take are collected into the policy module (policy.c).  The purpose of
doing so is to make it easier to understand this set of decisions and
how they interact, and to make it easier to maintain and update the
policy.

* Menu:

* Assignment of zones::
* Deciding whether to collect the world::
* Starting a trace::
* Trace progress::


File: MemoryPoolSystem.info,  Node: Assignment of zones,  Next: Deciding whether to collect the world,  Up: Policy<2>

5.28.5.1 Assignment of zones
............................

 -- C Function: *note Res: 55f. PolicyAlloc (Tract *tractReturn, Arena
          arena, LocusPref pref, Size size, Pool pool)

*note .policy.alloc;: 1447. Allocate ‘size’ bytes of memory on behalf of
‘pool’, based on the preferences described by ‘pref’.  If successful,
update ‘*tractReturn’ to point to the first tract in the allocated
memory and return ‘ResOK’.  Otherwise, return a result code describing
the problem, for example ‘ResCOMMIT_LIMIT’.

*note .policy.alloc.impl;: 1448. This tries various methods in
succession until one succeeds.  First, it tries to allocate from the
arena’s free land in the requested zones.  Second, it tries allocating
from free zones.  Third, it tries extending the arena and then trying
the first two methods again.  Fourth, it tries allocating from any zone
that is not blacklisted.  Fifth, it tries allocating from any zone at
all.

*note .policy.alloc.issue;: 1449. This plan performs poorly under
stress.  See for example job003898(1).

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job003898/


File: MemoryPoolSystem.info,  Node: Deciding whether to collect the world,  Next: Starting a trace,  Prev: Assignment of zones,  Up: Policy<2>

5.28.5.2 Deciding whether to collect the world
..............................................

 -- C Function: *note Bool: 3a9. PolicyShouldCollectWorld (Arena arena,
          double availableTime, Clock now, Clock clocks_per_sec)

*note .policy.world;: 144d. Determine whether now is a good time for
*note mps_arena_step(): 19c. to start a collection of the world.  Return
‘TRUE’ if so, ‘FALSE’ if not.  The ‘availableTime’ argument is an
estimate of the time that’s available for the collection, ‘now’ is the
current time as returned by ‘ClockNow()’, and ‘clocks_per_sec’ is the
result of calling ‘ClocksPerSec()’.

*note .policy.world.impl;: 144e. There are two conditions: the estimate
of the available time must be enough to complete the collection, and the
last collection of the world must be long enough in the past that the
*note mps_arena_step(): 19c. won’t be spending more than a certain
fraction of runtime in collections.  (This fraction is given by the
‘ARENA_MAX_COLLECT_FRACTION’ configuration parameter.)


File: MemoryPoolSystem.info,  Node: Starting a trace,  Next: Trace progress,  Prev: Deciding whether to collect the world,  Up: Policy<2>

5.28.5.3 Starting a trace
.........................

 -- C Function: *note Bool: 3a9. PolicyStartTrace (Trace *traceReturn,
          Bool *collectWorldReturn, Arena arena, Bool
          collectWorldAllowed)

*note .policy.start;: 1426. Consider starting a trace.  If a trace was
started, update ‘*traceReturn’ to point to the trace and return TRUE.
Otherwise, leave ‘*traceReturn’ unchanged and return FALSE.

*note .policy.start.world;: 1450. If ‘collectWorldAllowed’ is TRUE,
consider starting a collection of the whole world, and if such a
collection is started, set ‘*collectWorldReturn’ to TRUE.

This decision uses the “Lisp Machine” strategy, which tries to schedule
collections of the world so that the collector just keeps pace with the
mutator: that is, it starts a collection when the predicted completion
time of the collection is around the time when the mutator is predicted
to reach the current memory limit.  See *note [Pirinen]: 1451.

*note .policy.start.world.hack;: 1452. The ‘collectWorldAllowed’ flag
was added to fix job004011(1) by ensuring that the MPS starts at most
one collection of the world in each call to ‘ArenaPoll()’.  But this is
is fragile and inelegant.  Ideally the MPS would be able to deduce that
a collection of a set of generations can’t possibly make progress
(because nothing that refers to this set of generations has changed),
and so not start such a collection.

*note .policy.start.chain;: 1453. If ‘collectWorldAllowed’ is FALSE, or
if it is not yet time to schedule a collection of the world, *note
PolicyStartTrace(): 1425. considers collecting a set of zones
corresponding to a set of generations on a chain.

It picks these generations by calling ‘ChainDeferral()’ for each chain;
this function indicates if the chain needs collecting, and if so, how
urgent it is to collect that chain.  The most urgent chain in need of
collection (if any) is then condemned by calling ‘policyCondemnChain()’,
which chooses the set of generations to condemn, and condemns all the
segments in those generations.

   ---------- Footnotes ----------

   (1) https://www.ravenbrook.com/project/mps/issue/job004011/


File: MemoryPoolSystem.info,  Node: Trace progress,  Prev: Starting a trace,  Up: Policy<2>

5.28.5.4 Trace progress
.......................

 -- C Function: *note Bool: 3a9. PolicyPoll (Arena arena)

*note .policy.poll;: 1456. Return TRUE if the MPS should do some tracing
work; FALSE if it should return to the mutator.

 -- C Function: *note Bool: 3a9. PolicyPollAgain (Arena arena, Clock
          start, Bool moreWork, Work tracedWork)

*note .policy.poll.again;: 1457. Return TRUE if the MPS should do
another unit of work; FALSE if it should return to the mutator.  ‘start’
is the clock time when the MPS was entered; ‘moreWork’ and ‘tracedWork’
are the results of the last call to ‘TracePoll()’.

*note .policy.poll.impl;: 1458. The implementation keep doing work until
either the maximum pause time is exceeded (see
design.mps.arena.pause-time(1)), or there is no more work to do.  Then
it schedules the next collection so that there is approximately one call
to ‘TracePoll()’ for every ‘ArenaPollALLOCTIME’ bytes of allocation.

   ---------- Footnotes ----------

   (1) arena.html#design.mps.arena.pause-time


File: MemoryPoolSystem.info,  Node: References<25>,  Prev: Policy<2>,  Up: MPS Strategy

5.28.6 References
-----------------

(Pirinen) “The Lisp Machine Strategy”; Pekka Pirinin; 1998-04-27;
<‘https://info.ravenbrook.com/project/mps/doc/2002-06-18/obsolete-mminfo/mminfo/strategy/lisp-machine/’>


File: MemoryPoolSystem.info,  Node: Telemetry<3>,  Next: Tracer,  Prev: MPS Strategy,  Up: Old design

5.29 Telemetry
==============

* Menu:

* Introduction: Introduction<73>.
* Overview: Overview<33>.
* Requirements: Requirements<46>.
* Architecture: Architecture<11>.
* Analysis: Analysis<7>.
* Ideas: Ideas<2>.
* Implementation: Implementation<29>.


File: MemoryPoolSystem.info,  Node: Introduction<73>,  Next: Overview<33>,  Up: Telemetry<3>

5.29.1 Introduction
-------------------

*note .intro;: 1460. This documents the design of the telemetry
mechanism within the MPS.

*note .readership;: 1461. This document is intended for any MPS
developer.

*note .source;: 1462. Various meetings and brainstorms, including
meeting.general.1997-03-04(0), mail.richard.1997-07-03.17-01(1),
mail.gavinm.1997-05-01.12-40(2).

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/mail/1997/07/03/17-01/0.txt

   (2) 
https://info.ravenbrook.com/project/mps/mail/1997/05/01/12-40/0.txt


File: MemoryPoolSystem.info,  Node: Overview<33>,  Next: Requirements<46>,  Prev: Introduction<73>,  Up: Telemetry<3>

5.29.2 Overview
---------------

*note .over;: 1465. Telemetry permits the emission of events from the
MPS. These can be used to drive a graphical tool, or to debug, or
whatever.  The system is flexible and robust, but doesn’t require heavy
support from the client.


File: MemoryPoolSystem.info,  Node: Requirements<46>,  Next: Architecture<11>,  Prev: Overview<33>,  Up: Telemetry<3>

5.29.3 Requirements
-------------------

*note .req.simple;: 1467. It must be possible to generate code both for
the MPS and any tool without using complicated build tools.

*note .req.open;: 1468. We must not constrain the nature of events
before we are certain of what we want them to be.

*note .req.multi;: 1469. We must be able to send events to multiple
streams.

*note .req.share;: 146a. It must be possible to share event descriptions
between the MPS and any tool.

*note .req.version;: 146b. It must be possible to version the set of
events so that any tool can detect whether it can understand the MPS.

*note .req.back;: 146c. Tools should be able to understand older and
newer version of the MPS, so far as is appropriate.

*note .req.type;: 146d. It must be possible to transmit a rich variety
of types to the tool, including doubles, and strings.

*note .req.port;: 146e. It must be possible to transmit and receive
events between different platforms.

*note .req.control;: 146f. It must be possible to control whether and
what events are transmitted at least at a coarse level.

*note .req.examine;: 1470. There should be a cheap means to examine the
contents of logs.

*note .req.pm;: 1471. The event mechanism should provide for post mortem
to detect what significant events led up to death.

*note .req.perf;: 1472. Events should not have a significant effect on
performance when unwanted.

*note .req.small;: 1473. Telemetry streams should be small.

*note .req.avail;: 1474. Events should be available in all varieties,
subject to performance requirements.

*note .req.impl;: 1475. The plinth support for telemetry should be easy
to write and flexible.

*note .req.robust;: 1476. The telemetry protocol should be robust
against some forms of corruption, e.g.  packet loss.

*note .req.intern;: 1477. It should be possible to support
string-interning.


File: MemoryPoolSystem.info,  Node: Architecture<11>,  Next: Analysis<7>,  Prev: Requirements<46>,  Up: Telemetry<3>

5.29.4 Architecture
-------------------

*note .arch;: 1479. Event annotations are scattered throughout the code,
but there is a central registration of event types and properties.
Events are written to a buffer via a specialist structure, and are
optionally written to the plinth.  Events can take any number of
parameters of a range of types, indicated as a format both in the
annotation and the registry.


File: MemoryPoolSystem.info,  Node: Analysis<7>,  Next: Ideas<2>,  Prev: Architecture<11>,  Up: Telemetry<3>

5.29.5 Analysis
---------------

*note .analysis;: 147b. The proposed order of development, with summary
of requirements impact is as follows (★ for positive impact, ⇓ for
negative impact):

solution                      si     op     mu     sh     ve     ty     po     co     ex     pm     pe     sm     av     im     ro     in     ba     status
                                                                                                                                                     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                                                                                     
*note .sol.format: 147c.      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.struct: 147d.      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ★      ⇓      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.string: 147e.      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.relation: 147f.    ★      ·      ·      ★      ·      ·      ·      ·      ★      ·      ·      ★      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.dumper: 1480.      ·      ·      ·      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.kind: 1481.        ·      ⇓      ·      ·      ·      ·      ·      ★      ·      ★      ·      ·      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.control: 1482.     ·      ·      ·      ·      ·      ·      ·      ★      ·      ·      ★      ·      ·      ·      ·      ·      ·      merged
                                                                                                                                                     
                                                                                                                                                     
*note .sol.variety: 1483.     ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ★      ·      ★      ·      ·      ·      ·
                                                                                                                                              

The following are not yet ordered:

solution                      si     op     mu     sh     ve     ty     po     co     ex     pm     pe     sm     av     im     ro     in     ba     status
                                                                                                                                                     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                                                                                     
*note .sol.buffer: 1484.      ·      ·      ·      ·      ·      ·      ·      ★      ·      ★      ★      ·      ·      ·      ·      ·      .
                                                                                                                                              
                                                                                                                                                     
*note .sol.traceback: 1485.   ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      .
                                                                                                                                              
                                                                                                                                                     
*note .sol.client: 1486.      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      .
                                                                                                                                              
                                                                                                                                                     
*note .sol.head: 1487.        ·      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ·      ·      ·      .
                                                                                                                                              
                                                                                                                                                     
*note .sol.version: 1488.     ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ★
                                                                                                                                              
                                                                                                                                                     
*note .sol.exit: 1489.        ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      .
                                                                                                                                              
                                                                                                                                                     
*note .sol.block: 148a.       ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ⇓      ·      ·      ★      ·      ·
                                                                                                                                              
                                                                                                                                                     
*note .sol.code: 148b.        ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ·      ★      ·      ·      ·      ·      ★
                                                                                                                                              
                                                                                                                                                     
*note .sol.msg: 148c.         ·      ·      ★      ·      ·      ·      ★      ·      ·      ·      ·      ·      ·      ★      ★      ·      .
                                                                                                                                              

*note .file-format;: 148d. One of the objectives of this plan is to
minimise the impact of the changes to the log file format.  This is to
be achieved firstly by completing all necessary support before changes
are initiated, and secondly by performing all changes at the same time.


File: MemoryPoolSystem.info,  Node: Ideas<2>,  Next: Implementation<29>,  Prev: Analysis<7>,  Up: Telemetry<3>

5.29.6 Ideas
------------

*note .sol.format;: 147c. Event annotations indicate the types of their
arguments, for example, ‘EVENT_WD’ for a *note Word: 653. and a
‘double’.  (*note .req.type: 146d.)

*note .sol.struct;: 147d. Copy event data into a structure of the
appropriate type, for example, ‘EventWDStruct’.  (*note .req.type: 146d,
*note .req.perf: 1472, but not *note .req.small: 1473. because of
padding)

*note .sol.string;: 147e. Permit at most one string per event, at the
end, and use the ‘char[1]’ hack, and specialised code; deduce the string
length from the event length and also ‘NUL’-terminate (*note .req.type:
146d, *note .req.intern: 1477.)

*note .sol.buffer;: 1484. Enter all events initially into internal
buffers, and conditionally send them to the message stream.  (*note
.req.pm: 1471, *note .req.control: 146f, *note .req.perf: 1472.)

*note .sol.variety;: 1483. In optimized varieties, have internal events
(see *note .sol.buffer: 1484.) for a subset of events and no external
events; in normal varieties have all internal events, and the potential
for external events.  (*note .req.avail: 1474, *note .req.pm: 1471,
*note .req.perf: 1472.)

*note .sol.kind;: 1481. Divide events by some coarse type into around 6
groups, probably related to frequency.  (*note .req.control: 146f, *note
.req.pm: 1471, but not *note .req.open: 1468.)

*note .sol.control;: 1482. Hold flags to determine which events are
emitted externally.  (*note .req.control: 146f, *note .req.perf: 1472.)

*note .sol.dumper;: 1480. Write a simple tool to dump event logs as
text.  (*note .req.examine: 1470.)

*note .sol.msg;: 148c. Redesign the plinth interface to send and receive
messages, based on any underlying IPC mechanism, for example, append to
file, TCP/IP, messages, shared memory.  (*note .req.robust: 1476, *note
.req.impl: 1475, *note .req.port: 146e, *note .req.multi: 1469.)

*note .sol.block;: 148a. Buffer the events and send them as fixed size
blocks, commencing with a timestamp, and ending with padding.  (*note
.req.robust: 1476, *note .req.perf: 1472, but not *note .req.small:
1473.)

*note .sol.code;: 148b. Commence each event with two bytes of event
code, and two bytes of length.  (*note .req.small: 1473, *note
.req.back: 146c.)

*note .sol.head;: 1487. Commence each event stream with a
platform-independent header block giving information about the session,
version (see *note .sol.version: 1488.), and file format; file format
will be sufficient to decode the (platform-dependent) rest of the file.
(*note .req.port: 146e.)

*note .sol.exit;: 1489. Provide a mechanism to flush events in the event
of graceful sudden death.  (*note .req.pm: 1471.)

*note .sol.version;: 1488. Maintain a three part version number for the
file comprising major (incremented when the format of the entire file
changes (other than platform differences)), median (incremented when an
existing event changes its form or semantics), and minor (incremented
when a new event type is added); tools should normally fail when the
median or major is unsupported.  (*note .req.version: 146b, *note
.req.back: 146c.)

*note .sol.relation;: 147f. Event types will be defined in terms of a
relation specifying their name, code, optimised behaviour (see *note
.sol.variety: 1483.), kind (see *note .sol.kind: 1481.), and format (see
*note .sol.format: 147c.); both the MPS and tool can use this by
suitable ‘#define’ hacks.  (*note .req.simple: 1467, *note .req.share:
146a, *note .req.examine: 1470, *note .req.small: 1473. (no format
information in messages))

*note .sol.traceback;: 1485. Provide a mechanism to output recent events
(see *note .sol.buffer: 1484.) as a form of backtrace when ‘AVER’
statements fire or from a debugger, or whatever.  (*note .req.pm: 1471.)

*note .sol.client;: 1486. Provide a mechanism for user events.  (*note
.req.intern: 1477.)


File: MemoryPoolSystem.info,  Node: Implementation<29>,  Prev: Ideas<2>,  Up: Telemetry<3>

5.29.7 Implementation
---------------------

* Menu:

* Annotation::
* Registration::
* Control::
* Debugging::
* Dumper tool::
* Allocation replayer tool::


File: MemoryPoolSystem.info,  Node: Annotation,  Next: Registration,  Up: Implementation<29>

5.29.7.1 Annotation
...................

*note .annot;: 1491. An event annotation is of the form:

     EVENT3(FooCreate, pointer, address, word)

*note .annot.string;: 1492. If there is a string in the format, it must
be the last parameter (and hence there can be only one).  There is
currently a maximum string length, defined by ‘EventMaxStringLength’ in
impl.h.eventcom.

*note .annot.type;: 1493. The event type should be given as the first
parameter to the event macro, as registered in impl.h.eventdef.

*note .annot.param;: 1494. The parameters of the event should be given
as the remaining parameters of the event macro, in order as indicated in
the event parameters definition in impl.h.eventdef.


File: MemoryPoolSystem.info,  Node: Registration,  Next: Control,  Prev: Annotation,  Up: Implementation<29>

5.29.7.2 Registration
.....................

*note .reg;: 1496. All event types and parameters should be registered
in impl.h.eventdef, in the form of a higher-order list macros.

*note .reg.just;: 1497. This use of a higher-order macros enables great
flexibility in the use of this file.

*note .reg.rel;: 1498. The event type registration is of the form:

     EVENT(X, FooCreate, 0x1234, TRUE, Arena)

*note .reg.type;: 1499. The first parameter of the relation is the event
type.  This needs no prefix, and should correspond to that used in the
annotation.

*note .reg.code;: 149a. The second parameter is the event code, a 16-bit
value used to represent this event type.  Codes should not be re-used
for new event types, to allow interpretation of event log files of all
ages.

*note .reg.always;: 149b. The third parameter is a boolean value
indicating whether this event type should be implemented in all
varieties.  See *note .control.buffer: 149c.  Unless your event is on
the critical path (typically per reference or per object), you will want
this to be ‘TRUE’.

*note .reg.kind;: 149d. The fourth parameter is a kind keyword
indicating what category this event falls into.  See *note .control:
149e.  The possible values are:

   - *note Arena: 796. – per space or arena or global

   - ‘Pool’ – pool-related

   - ‘Trace’ – per trace or scan

   - *note Seg: b53. – per segment

   - *note Ref: b24. – per reference or fix

   - ‘Object’ – per object or allocation

   - ‘User’ – invoked by the user through the MPS interface

This list can be seen in impl.h.eventcom.

*note .reg.doc;: 149f. Add a docstring column.  [RB 2012-09-03]

*note .reg.params;: 14a0. The event parameters registration is of the
form:

     #define EVENT_FooCreate_PARAMS(PARAM, X) \
       PARAM(X,  0, P, firstParamPointer) \
       PARAM(X,  1, U, secondParamUnsigned)

*note .reg.param.index;: 14a1. The first column is the index, and must
start at zero and increase by one for each row.

*note .reg.param.sort;: 14a2. The second column is the parameter “sort”,
which, when appended to ‘EventF’, yields a type for the parameter.  It
is a letter from the following list:

   - ‘P’ – ‘void *’

   - ‘A’ – *note Addr: 632.

   - ‘W’ – *note Word: 653.

   - ‘U’ – ‘unsigned int’

   - ‘S’ – ‘char *’

   - ‘D’ – ‘double’

   - ‘B’ – *note Bool: 3a9.

The corresponding event parameter must be assignment compatible with the
type.

*note .param.types;: 14a3. When an event has parameters whose type is
not in the above list, use the following guidelines: All ‘C’ pointer
types not representing strings use ‘P’; *note Size: 40e, *note Count:
3af, *note Index: b19. use ‘W’; others should be obvious.

*note .reg.param.name;: 14a4. The third column is the parameter name.
It should be a valid C identifier and is used for debugging display and
human readable output.

*note .reg.param.doc;: 14a5. Add a docstring column.  [RB 2012-09-03]

*note .reg.dup;: 14a6. It is permissible for the one event type to be
used for more than one annotation.  There are generally two reasons for
this:

   - Variable control flow for successful function completion;

   - Platform/Otherwise-dependent implementations of a function.

Note that all annotations for one event type must have the same format
(as implied by *note .sol.format: 147c.).


File: MemoryPoolSystem.info,  Node: Control,  Next: Debugging,  Prev: Registration,  Up: Implementation<29>

5.29.7.3 Control
................

*note .control;: 149e. There are two types of event control, buffer and
output.

*note .control.buffer;: 149c. Buffer control affects whether particular
events implemented at all, and is controlled statically by variety using
the always value (see *note .reg.always: 149b.) for the event type.  The
hot variety does compiles out annotations with ‘always=FALSE’.  The cool
variety does not, so always buffers a complete set of events.

*note .control.output;: 14a8. Output control affects whether events
written to the internal buffer are output via the plinth.  This is set
on a per-kind basis (see *note .reg.kind: 149d.), using a control bit
table stored in EventKindControl.  By default, all event kinds are off.
You may switch some kinds on using a debugger.

For example, to enable ‘Pool’ events using gdb (see impl.h.eventcom for
numeric codes):

     $ gdb ./xci3gc/cool/amcss
     (gdb) break GlobalsInit
     (gdb) run
     ...
     (gdb) print EventKindControl |= 2
     $2 = 2
     (gdb) continue
     ...
     (gdb) quit
     $ mpseventcnv -v | sort | head
     0000178EA03ACF6D PoolInit                9C1E0    9C000 0005E040
     0000178EA03C2825 PoolInitMFS             9C0D8    9C000     1000        C
     0000178EA03C2C27 PoolInitMFS             9C14C    9C000     1000       44
     0000178EA03C332C PoolInitMV              9C080    9C000     1000       20    10000
     0000178EA03F4DB4 BufferInit             2FE2C4   2FE1B0        0
     0000178EA03F4EC8 BufferInitSeg          2FE2C4   2FE1B0        0
     0000178EA03F57DA AMCGenCreate           2FE1B0   2FE288
     0000178EA03F67B5 BufferInit             2FE374   2FE1B0        0
     0000178EA03F6827 BufferInitSeg          2FE374   2FE1B0        0
     0000178EA03F6B72 AMCGenCreate           2FE1B0   2FE338

*note .control.env;: 14a9. The initial value of ‘EventKindControl’ is
read from the C environment when the ANSI Plinth is used, and so event
output can be controlled like this:

     MPS_TELEMETRY_CONTROL=127 amcss

or like this:

     MPS_TELEMETRY_CONTROL="Pool Arena" amcss

where the variable is set to a space-separated list of names defined by
‘EventKindENUM’.

*note .control.just;: 14aa. These controls are coarse, but very cheap.

*note .control.external;: 14ab. The MPS interface functions *note
mps_telemetry_set(): 176. and *note mps_telemetry_reset(): 2b4. can be
used to change ‘EventKindControl’.

*note .control.tool;: 14ac. The tools will be able to control
‘EventKindControl’.


File: MemoryPoolSystem.info,  Node: Debugging,  Next: Dumper tool,  Prev: Control,  Up: Implementation<29>

5.29.7.4 Debugging
..................

*note .debug.buffer;: 14ae. Each event kind is logged in a separate
buffer, ‘EventBuffer[kind]’.

*note .debug.buffer.reverse;: 14af. The events are logged in reverse
order from the top of the buffer, with the last logged event at
‘EventLast[kind]’.  This allows recovery of the list of recent events
using the ‘event->any.size’ field.

*note .debug.dump;: 14b0. The contents of all buffers can be dumped with
the ‘EventDump’ function from a debugger, for example:

     gdb> print EventDump(mps_lib_get_stdout())

*note .debug.describe;: 14b1. Individual events can be described with
the EventDescribe function, for example:

     gdb> print EventDescribe(EventLast[3], mps_lib_get_stdout(), 0)

*note .debug.core;: 14b2. The event buffers are preserved in core dumps
and can be used to work out what the MPS was doing before a crash.
Since the kinds correspond to frequencies, ancient events may still be
available in some buffers, even if they have been flushed to the output
stream.  Some digging may be required.


File: MemoryPoolSystem.info,  Node: Dumper tool,  Next: Allocation replayer tool,  Prev: Debugging,  Up: Implementation<29>

5.29.7.5 Dumper tool
....................

*note .dumper;: 14b4. A primitive dumper tool is available in
impl.c.eventcnv.  For details, see guide.mps.telemetry.


File: MemoryPoolSystem.info,  Node: Allocation replayer tool,  Prev: Dumper tool,  Up: Implementation<29>

5.29.7.6 Allocation replayer tool
.................................

*note .replayer;: 14b6. A tool for replaying an allocation sequence from
a log is available in impl.c.replay.


File: MemoryPoolSystem.info,  Node: Tracer,  Prev: Telemetry<3>,  Up: Old design

5.30 Tracer
===========

* Menu:

* Introduction: Introduction<74>.
* Architecture: Architecture<12>.
* Analysis: Analysis<8>.
* Ideas: Ideas<3>.
* Implementation: Implementation<30>.
* Life cycle of a trace object::
* References: References<26>.


File: MemoryPoolSystem.info,  Node: Introduction<74>,  Next: Architecture<12>,  Up: Tracer

5.30.1 Introduction
-------------------

     Warning: This document is currently a mixture of very old design
     notes (the preformatted section immediately following) and some
     newer stuff.  It doesn’t yet form anything like a complete picture.


File: MemoryPoolSystem.info,  Node: Architecture<12>,  Next: Analysis<8>,  Prev: Introduction<74>,  Up: Tracer

5.30.2 Architecture
-------------------

*note .instance.limit;: 14bd. There is a limit on the number of traces
that can be created at any one time.  This limits the number of
concurrent traces.  This limitation is expressed in the symbol
‘TraceLIMIT’.

     Note: ‘TraceLIMIT’ is currently set to 1 as the MPS assumes in
     various places that only a single trace is active at a time.  See
     request.mps.160020(1) “Multiple traces would not work”.  David
     Jones, 1998-06-15.

*note .rate;: 14be. See mail.nickb.1997-07-31.14-37(2).

     Note: Now revised?  See request.epcore.160062(3) and
     change.epcore.minnow.160062.  David Jones, 1998-06-15.

*note .exact.legal;: 14bf. Exact references must either point outside
the arena (to non-managed address space) or to a tract allocated to a
pool.  Exact references that are to addresses which the arena has
reserved but hasn’t allocated memory to are illegal (such a reference
cannot possibly refer to a real object, and so cannot be exact).  We
check that this is the case in ‘TraceFix()’.

     Note: Depending on the future semantics of ‘PoolDestroy()’ we might
     need to adjust our strategy here.  See mail.dsm.1996-02-14.18-18(4)
     for a strategy of coping gracefully with ‘PoolDestroy()’.

*note .fix.fixed.all;: 14c0. ‘ss->fixedSummary’ is accumulated (in
‘TraceFix()’) for all pointers, whether or not they are genuine
references.  We could accumulate fewer pointers here; if a pointer fails
the *note TractOfAddr(): 4c4. test then we know it isn’t a reference, so
we needn’t accumulate it into the fixed summary.  The design allows
this, but it breaks a useful post-condition on scanning (if the
accumulation of ‘ss->fixedSummary’ was moved the accuracy of
‘ss->fixedSummary’ would vary according to the “width” of the white
summary).  See mail.pekka.1998-02-04.16-48(5) for improvement
suggestions.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/mps/160020

   (2) 
https://info.ravenbrook.com/project/mps/mail/1997/07/31/14-37/0.txt

   (3) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/epcore/160062

   (4) 
https://info.ravenbrook.com/project/mps/mail/1996/02/14/18-18/0.txt

   (5) 
https://info.ravenbrook.com/project/mps/mail/1998/02/04/16-48/0.txt


File: MemoryPoolSystem.info,  Node: Analysis<8>,  Next: Ideas<3>,  Prev: Architecture<12>,  Up: Tracer

5.30.3 Analysis
---------------

*note .fix.copy-fail;: 14c3. Fixing can always succeed, even if copying
the referenced object has failed (due to lack of memory, for example),
by backing off to treating a reference as ambiguous.  Assuming that
fixing an ambiguous reference doesn’t allocate memory (which is no
longer true for AMC for example).  See request.dylan.170560(1) for a
slightly more sophisticated way to proceed when you can no longer
allocate memory for copying.

   ---------- Footnotes ----------

   (1) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170560


File: MemoryPoolSystem.info,  Node: Ideas<3>,  Next: Implementation<30>,  Prev: Analysis<8>,  Up: Tracer

5.30.4 Ideas
------------

*note .flip.after;: 14c6. To avoid excessive barrier impact on the
mutator immediately after flip, we could scan during flip other objects
which are “near” the roots, or otherwise known to be likely to be
accessed in the near future.


File: MemoryPoolSystem.info,  Node: Implementation<30>,  Next: Life cycle of a trace object,  Prev: Ideas<3>,  Up: Tracer

5.30.5 Implementation
---------------------

* Menu:

* Speed::


File: MemoryPoolSystem.info,  Node: Speed,  Up: Implementation<30>

5.30.5.1 Speed
..............

*note .fix;: 14c9. The function implementing the fix operation should be
called ‘TraceFix()’ and this name is pervasive in the MPS and its
documents to describe this function.  Nonethless, optimisation and
strict aliasing rules have meant that we need to use the external name
for it, ‘_mps_fix2()’.

*note .fix.speed;: 14ca. The fix path is critical to garbage collection
speed.  Abstractly, the fix operation is applied to all references in
the non-white heap and all references in the copied heap.  Remembered
sets cut down the number of segments we have to scan.  The zone test
cuts down the number of references we call fix on.  The speed of the
remainder of the fix path is still critical to system performance.
Various modifications to and aspects of the system are concerned with
maintaining the speed along this path.  See design.mps.critical_path(1).

*note .fix.tractofaddr;: 14cb. A reference that passes the zone test is
then looked up to find the tract it points to, an operation equivalent
to calling *note TractOfAddr(): 4c4.

*note .fix.tractofaddr.inline;: 14cc. ‘TraceFix()’ doesn’t actually call
*note TractOfAddr(): 4c4.  Instead, it expands this operation inline
(calling ‘ChunkOfAddr()’, then ‘INDEX_OF_ADDR()’, checking the
appropriate bit in the chunk’s ‘allocTable’, and finally looking up the
tract in the chunk’s page table).  The reason for inlining this code is
that we need to know whether the reference points to a chunk (and not
just whether it points to a tract) in order to check the *note
.exact.legal: 14bf. condition.

*note .fix.whiteseg;: 14cd. The reason for looking up the tract is to
determine whether the reference is to a white segment.

     Note: It is likely to be more efficient to maintain a separate
     lookup table from address to white segment, rather than indirecting
     through the chunk and the tract.  See job003796(2).

*note .fix.noaver;: 14ce. ‘AVER()’ statements in the code add bulk to
the code (reducing I-cache efficacy) and add branches to the path
(polluting the branch pedictors) resulting in a slow down.  Replacing
the ‘AVER()’ statements with ‘AVER_CRITICAL()’ on the critical path
improves the overall speed of the Dylan compiler by as much as 9%.  See
design.mps.critical_path(3).

*note .fix.nocopy;: 14cf. *note amcSegFix(): 4bf. used to copy objects
by using the format’s copy method.  This involved a function call
(through an indirection) and in ‘dylan_copy’ a call to ‘dylan_skip’ (to
recompute the length) and call to ‘memcpy’ with general parameters.
Replacing this with a direct call to ‘memcpy’ removes these overheads
and the call to ‘memcpy’ now has aligned parameters.  The call to
‘memcpy’ is inlined by the C compiler.  This change results in a 4–5%
speed-up in the Dylan compiler.

*note .reclaim;: 14d0. Because the reclaim phase of the trace
(implemented by ‘TraceReclaim()’) examines every segment it is fairly
time intensive.  Richard Tucker’s profiles presented in
request.dylan.170551(4) show a gap between the two varieties variety.hi
and variety.wi.

*note .reclaim.noaver;: 14d1. Accordingly, reclaim methods use
‘AVER_CRITICAL()’ instead of ‘AVER()’.

   ---------- Footnotes ----------

   (1) critical_path.html

   (2) https://www.ravenbrook.com/project/mps/issue/job003796/

   (3) critical_path.html

   (4) 
https://info.ravenbrook.com/project/mps/import/2001-11-05/mmprevol/request/dylan/170551


File: MemoryPoolSystem.info,  Node: Life cycle of a trace object,  Next: References<26>,  Prev: Implementation<30>,  Up: Tracer

5.30.6 Life cycle of a trace object
-----------------------------------

‘TraceCreate()’ creates a trace in state ‘TraceINIT’

Some segments get condemned (made white).

‘TraceStart()’ gets called which:

   - Derives an initial reference partition based on the existing white
     set.  The white zone set and the segments’ summaries are used to
     create an initial grey set.

   - Emits a ‘GCStart()’ message.

   - Initialises ‘trace->rate’ by estimating the required scanning rate.

   - Moves the trace into the state ‘TraceUNFLIPPED’.

   - Immediately calls ‘traceFlip’ which flips the trace and moves it
     into state ‘TraceFLIPPED’.

Whilst a trace is alive every so often its ‘TraceAdvance()’ method gets
invoked (via ‘TracePoll()’) in order to do a step of tracing work.
‘TraceAdvance()’ is responsible for ticking through the trace’s
top-level state machine.  Most of the interesting work, the tracing,
happens in the ‘TraceFLIPPED’ state.

The trace transitions through its states in the following sequence:
‘TraceINIT’ → (‘TraceUNFLIPPED’) → ‘TraceFLIPPED’ → ‘TraceRECLAIM’ →
‘TraceFINISHED’.

Whilst ‘TraceUNFLIPPED’ appears in the code, no trace does any work in
this state; all traces are immediately flipped to be in the
‘TraceFLIPPED’ state (see above).

Once the trace is in the ‘TraceFINISHED’ state it performs no more work
and it can be safely destroyed.  Generally the callers of
‘TraceAdvance()’ will destroy the trace.

* Menu:

* Making progress; scanning grey segments: Making progress scanning grey segments.


File: MemoryPoolSystem.info,  Node: Making progress scanning grey segments,  Up: Life cycle of a trace object

5.30.6.1 Making progress: scanning grey segments
................................................

Most of the interesting work of a trace, the actual tracing, happens in
the ‘TraceFLIPPED’ state (work 'would' happen in the ‘TraceUNFLIPPED’
state, but that is not implemented).

The tracer makes progress by choosing a grey segment to scan, and
scanning it.  The actual scanning is performed by pools.

Note that at all times a reference partition is maintained.

The order in which the trace scans things determines the semantics of
certain types of references (in particular, weak and final references).
Or, to put it another way the desired semantics of weak and final
references impose certain restrictions on the order in which the trace
can scan things.

.rank: The tracer uses a system of 'reference ranks' (or just ranks) so
that it can impose an order on its scanning work.  The ranks are
ordered.  [TODO: Explain how ordering is also required for transforms.
See impl.c.trans.rank-order.  RB 2023-06-16]

The tracer proceeds band by band.  The first band is all objects it can
reach by following references of the first rank.  The second band is all
subsequent objects it can reach by following references of the second
and first ranks.  The third band is all subsequent objects it can reach
by following references of the third, second, and first ranks.  And so
on.  The description of the tracer working like this originated in *note
[RHSK_2007-06-25]: 14d4.

A trace keeps track of which band it is tracing.  This is returned by
the ‘TraceBand()’ method.  Keeping this band information helps it
implement the semantics of finalization and weakness.  The band used to
not be explicitly stored, but this hindered the implementation of good
finalization semantics (in some circumstances finalization messages were
delayed by at least one collection cycle: see job001658(1)).

The band is used when selecting a grey segment to scan (the selection
occurs in ‘traceFindGrey()’).  The tracer attempts to first find
segments whose rank is the current band, then segments whose rank is
previous to the current band, and so on.  If there are no segments found
then the current band is exhausted and the current band is incremented
to the next rank.  When the current band is moved through all the ranks
in this fashion there is no more tracing to be done.

   ---------- Footnotes ----------

   (1) https://info.ravenbrook.com/project/mps/issue/job001658/


File: MemoryPoolSystem.info,  Node: References<26>,  Prev: Life cycle of a trace object,  Up: Tracer

5.30.7 References
-----------------

(RHSK_2007-06-25) Richard Kistruck.  Ravenbrook Limited.  2007-06-25.
“The semantics of rank-based tracing(1)”.

   ---------- Footnotes ----------

   (1) https://info.ravenbrook.com/mail/2007/06/25/11-35-57/0/


File: MemoryPoolSystem.info,  Node: Bibliography,  Next: Memory Management Glossary,  Prev: Old design,  Up: Top

6 Bibliography
**************

   * Ole Agesen, David L. Detlefs.  1997.  “Finding References in Java
     Stacks(1)”.  Sun Labs.  OOPSLA97 Workshop on Garbage Collection and
     Memory Management.

          Abstract: Exact garbage collection for the strongly-typed Java
          language may seem straightforward.  Unfortunately, a single
          pair of bytecodes in the Java Virtual Machine instruction set
          presents an obstacle that has thus far not been discussed in
          the literature.  We explain the problem, outline the space of
          possible solutions, and present a solution utilizing
          bytecode-preprocessing to enable exact garbage collection
          while maintaining compatibility with existing compiled Java
          class files.

   * Ole Agesen, David L. Detlefs, J. Eliot B. Moss.  1998.  “Garbage
     Collection and Local Variable Type-precision and Liveness in Java
     Virtual Machines(2)”.  ACM. Proceedings of the ACM SIGPLAN ‘98
     conference on Programming language design and implementation, pp.
     269–279.

          Abstract: Full precision in garbage collection implies
          retaining only those heap allocated objects that will actually
          be used in the future.  Since full precision is not computable
          in general, garbage collectors use safe (i.e., conservative)
          approximations such as reachability from a set of root
          references.  Ambiguous roots collectors (commonly called
          “conservative”) can be overly conservative because they
          overestimate the root set, and thereby retain unexpectedly
          large amounts of garbage.  We consider two more precise
          collection schemes for Java virtual machines (JVMs).  One uses
          a type analysis to obtain a type-precise root set (only those
          variables that contain references); the other adds a live
          variable analysis to reduce the root set to only the live
          reference variables.  Even with the Java programming
          language’s strong typing, it turns out that the JVM
          specification has a feature that makes type-precise root sets
          difficult to compute.  We explain the problem and ways in
          which it can be solved.

          Our experimental results include measurements of the costs of
          the type and liveness analyses at load time, of the
          incremental benefits at run time of the liveness analysis over
          the type-analysis alone, and of various map sixes and counts.
          We find that the liveness analysis often produces little or no
          improvement in heap size, sometimes modest improvements, and
          occasionally the improvement is dramatic.  While further study
          is in order, we conclude that the main benefit of the liveness
          analysis is preventing bad surprises.

   * Andrew Appel, John R. Ellis, Kai Li.  1988.  “Real-time Concurrent
     Collection on Stock Multiprocessors(3)”.  ACM, SIGPLAN. ACM PLDI
     88, SIGPLAN Notices 23, 7 (July 88), pp.  11–20.

          Abstract: We’ve designed and implemented a copying
          garbage-collection algorithm that is efficient, real-time,
          concurrent, runs on commercial uniprocessors and shared-memory
          multiprocessors, and requires no change to compilers.  The
          algorithm uses standard virtual-memory hardware to detect
          references to “from space” objects and to synchronize the
          collector and mutator threads.  We’ve implemented and measured
          a prototype running on SRC’s 5-processor Firefly.  It will be
          straightforward to merge our techniques with generational
          collection.  An incremental, non-concurrent version could be
          implemented easily on many versions of Unix.

   * Apple Computer, Inc.  1994.  'Inside Macintosh: Memory'.
     Addison-Wesley.  ISBN 0-201-63240-3.

          Abstract: Inside Macintosh: Memory describes the parts of the
          Macintosh® Operating System that allow you to directly
          allocate, release, or otherwise manipulate memory.  Everyone
          who programs Macintosh computers should read this book.

          Inside Macintosh: Memory shows in detail how your application
          can manage the memory partition it is allocated and perform
          other memory-related operations.  It also provides a complete
          technical reference for the Memory Manager, the Virtual Memory
          Manager, and other memory-related utilities provided by the
          system software.

   * Giuseppe Attardi & Tito Flagella.  1994.  “A Customisable Memory
     Management Framework(4)”.  TR-94-010.

          Abstract: Memory management is a critical issue for many large
          object-oriented applications, but in C++ only explicit memory
          reclamation through the delete operator is generally
          available.  We analyse different possibilities for memory
          management in C++ and present a dynamic memory management
          framework which can be customised to the need of specific
          applications.  The framework allows full integration and
          coexistence of different memory management techniques.  The
          Customisable Memory Management (CMM) is based on a primary
          collector which exploits an evolution of Bartlett’s mostly
          copying garbage collector.  Specialised collectors can be
          built for separate memory heaps.  A Heap class encapsulates
          the allocation strategy for each heap.  We show how to emulate
          different garbage collection styles or user-specific memory
          management techniques.  The CMM is implemented in C++ without
          any special support in the language or the compiler.  The
          techniques used in the CMM are general enough to be applicable
          also to other languages.

   * Giuseppe Attardi, Tito Flagella, Pietro Iglio.  1998.  “A
     customisable memory management framework for C++(5)”.  Software –
     Practice and Experience.  28(11), 1143–1183.

          Abstract: Automatic garbage collection relieves programmers
          from the burden of managing memory themselves and several
          techniques have been developed that make garbage collection
          feasible in many situations, including real time applications
          or within traditional programming languages.  However optimal
          performance cannot always be achieved by a uniform general
          purpose solution.  Sometimes an algorithm exhibits a
          predictable pattern of memory usage that could be better
          handled specifically, delaying as much as possible the
          intervention of the general purpose collector.  This leads to
          the requirement for algorithm specific customisation of the
          collector strategies.  We present a dynamic memory management
          framework which can be customised to the needs of an
          algorithm, while preserving the convenience of automatic
          collection in the normal case.  The Customisable Memory
          Manager (CMM) organises memory in multiple heaps.  Each heap
          is an instance of a C++ class which abstracts and encapsulates
          a particular storage discipline.  The default heap for
          collectable objects uses the technique of mostly copying
          garbage collection, providing good performance and memory
          compaction.  Customisation of the collector is achieved
          exploiting object orientation by defining specialised versions
          of the collector methods for each heap class.  The object
          oriented interface to the collector enables coexistence and
          coordination among the various collectors as well as
          integration with traditional code unaware of garbage
          collection.  The CMM is implemented in C++ without any special
          support in the language or the compiler.  The techniques used
          in the CMM are general enough to be applicable also to other
          languages.  The performance of the CMM is analysed and
          compared to other conservative collectors for C/C++ in various
          configurations.

   * Alain Azagury, Elliot K. Kolodner, Erez Petrank, Zvi Yehudai.
     1998.  “Combining Card Marking with Remembered Sets: How to Save
     Scanning Time(6)”.  ACM. ISMM’98 pp.  10–19.

          Abstract: We consider the combination of card marking with
          remembered sets for generational garbage collection as
          suggested by Hosking and Moss.  When more than two generations
          are used, a naive implementation may cause excessive and
          wasteful scanning of the cards and thus increase the
          collection time.  We offer a simple data structure and a
          corresponding algorithm to keep track of which cards need be
          scanned for which generation.  We then extend these ideas for
          the Train Algorithm of Hudson and Moss.  Here, the solution is
          more involved, and allows tracking of which card should be
          scanned for which car-collection in the train.

   * Henry G. Baker, Carl Hewitt.  1977.  “The Incremental Garbage
     Collection of Processes(7)”.  ACM. SIGPLAN Notices 12, 8 (August
     1977), pp.  55–59.

          Abstract: This paper investigates some problems associated
          with an argument evaluation order that we call “future” order,
          which is different from both call-by-name and call-by-value.
          In call-by-future, each formal parameter of a function is
          bound to a separate process (called a “future”) dedicated to
          the evaluation of the corresponding argument.  This mechanism
          allows the fully parallel evaluation of arguments to a
          function, and has been shown to augment the expressive power
          of a language.

          We discuss an approach to a problem that arises in this
          context: futures which were thought to be relevant when they
          were created become irrelevant through being ignored in the
          body of the expression where they were bound.  The problem of
          irrelevant processes also appears in multiprocessing
          problem-solving systems which start several processors working
          on the same problem but with different methods, and return
          with the solution which finishes first.  This “parallel method
          strategy” has the drawback that the processes which are
          investigating the losing methods must be identified, stopped,
          and reassigned to more useful tasks.

          The solution we propose is that of garbage collection.  We
          propose that the goal structure of the solution plan be
          explicitly represented in memory as part of the graph memory
          (like Lisp’s heap) so that a garbage collection algorithm can
          discover which processes are performing useful work, and which
          can be recycled for a new task.  An incremental algorithm for
          the unified garbage collection of storage and processes is
          described.

   * Henry G. Baker.  1978.  “List Processing in Real Time on a Serial
     Computer(8)”.  ACM. Communications of the ACM 21, 4 (April 1978),
     pp.  280–294.

          Abstract: A real-time list processing system is one in which
          the time required by the elementary list operations (e.g.
          CONS, CAR, CDR, RPLACA, RPLACD, EQ, and ATOM in LISP) is
          bounded by a (small) constant.  Classical implementations of
          list processing systems lack this property because allocating
          a list cell from the heap may cause a garbage collection,
          which process requires time proportional to the heap size to
          finish.  A real-time list processing system is presented which
          continuously reclaims garbage, including directed cycles,
          while linearizing and compacting the accessible cells into
          contiguous locations to avoid fragmenting the free storage
          pool.  The program is small and requires no time-sharing
          interrupts, making it suitable for microcode.  Finally, the
          system requires the same average time, and not more than twice
          the space, of a classical implementation, and those space
          requirements can be reduced to approximately classical
          proportions by compact list representation.  Arrays of
          different sizes, a program stack, and hash linking are simple
          extensions to our system, and reference counting is found to
          be inferior for many applications.

   * Henry G. Baker.  1979.  “Optimizing Allocation and Garbage
     Collection of Spaces(9)”.  In Winston and Brown, eds.  'Artificial
     Intelligence: An MIT Perspective.'  MIT Press.

          Abstract: MACLISP, unlike some other implementations of LISP,
          allocates storage for different types of objects in
          noncontiguous areas called “spaces”.  These spaces partition
          the active storage into disjoint areas, each of which holds a
          different type of object.  For example, “list cells” are
          stored in one space, “full-word integers” reside in another
          space, “full-word floating point numbers” in another, and so
          on.

          Allocating space in this manner has several advantages.  An
          object’s type can easily be computed from a pointer to it,
          without any memory references to the object itself.  Thus, the
          LISP primitive ATOM(x) can easily compute its result without
          even paging in x.  Another advantage is that the type of an
          object does not require any storage within the object, so that
          arithmetic with hardware data types such as full-word integers
          can use hardware instructions directly.

          There are problems associated with this method of storage and
          type management, however.  When all data types are allocated
          from the same heap, there is no problem with varying demand
          for the different data types; all data types require storage
          from the same pool, so that only the total amount of storage
          is important.  Once different data types must be allocated
          from different spaces, however, the relative sizes of the
          spaces becomes important.

   * Henry G. Baker.  1991.  “Cache-Conscious Copying Collectors(10)”.
     OOPSLA’91/GC’91 Workshop on Garbage Collection.

          Abstract: Garbage collectors must minimize the scarce
          resources of cache space and off-chip communications bandwidth
          to optimize performance on modern single-chip computer
          architectures.  Strategies for achieving these goals in the
          context of copying garbage collection are discussed.  A
          multi-processor mutator/collector system is analyzed.
          Finally, the Intel 80860XP architecture is studied.

   * Henry G. Baker.  1992.  “Lively Linear Lisp - 'Look Ma, No
     Garbage!'(11)”.  ACM. SIGPLAN Notices 27, 8 (August 1992), pp.
     89–98.

          Abstract: Linear logic has been proposed as one solution to
          the problem of garbage collection and providing efficient
          “update-in-place” capabilities within a more functional
          language.  Linear logic conserves accessibility, and hence
          provides a “mechanical metaphor” which is more appropriate for
          a distributed-memory parallel processor in which copying is
          explicit.  However, linear logic’s lack of sharing may
          introduce significant inefficiencies of its own.

          We show an efficient implementation of linear logic called
          “Linear Lisp” that runs within a constant factor of non-linear
          logic.  This Linear Lisp allows RPLACX operations, and manages
          storage as safely as a non-linear Lisp, but does not need a
          garbage collector.  Since it offers assignments but no
          sharing, it occupies a twilight zone between functional
          languages and imperative languages.  Our Linear Lisp Machine
          offers many of the same capabilities as combinator/graph
          reduction machines, but without their copying and garbage
          collection problems.

   * Henry G. Baker.  1992.  “The Treadmill: Real-Time Garbage
     Collection Without Motion Sickness(12)”.  ACM. SIGPLAN Notices 27,
     3 (March 1992), pp.  66–70.

          Abstract: A simple real-time garbage collection algorithm is
          presented which does not copy, thereby avoiding some of the
          problems caused by the asynchronous motion of objects.  This
          in-place “treadmill” garbage collection scheme has
          approximately the same complexity as other non-moving garbage
          collectors, thus making it usable in a high-level language
          implementation where some pointers cannot be traced.  The
          treadmill is currently being used in a Lisp system built in
          Ada.

   * Henry G. Baker.  1992.  “CONS Should not CONS its Arguments, or, a
     Lazy Alloc is a Smart Alloc(13)”.  ACM. SIGPLAN Notices 27, 3
     (March 1992), 24–34.

          Abstract: “Lazy allocation” is a model for allocating objects
          on the execution stack of a high-level language which does not
          create dangling references.  Our model provides safe
          transportation into the heap for objects that may survive the
          deallocation of the surrounding stack frame.  Space for
          objects that do not survive the deallocation of the
          surrounding stack frame is reclaimed without additional effort
          when the stack is popped.  Lazy allocation thus performs a
          first-level garbage collection, and if the language supports
          garbage collection of the heap, then our model can reduce the
          amortized cost of allocation in such a heap by filtering out
          the short-lived objects that can be more efficiently managed
          in LIFO order.  A run-time mechanism called “result
          expectation” further filters out unneeded results from
          functions called only for their effects.  In a shared-memory
          multi-processor environment, this filtering reduces contention
          for the allocation and management of global memory.

          Our model performs simple local operations, and is therefore
          suitable for an interpreter or a hardware implementation.  Its
          overheads for functional data are associated only with
          'assignments', making lazy allocation attractive for “mostly
          functional” programming styles.  Many existing stack
          allocation optimizations can be seen as instances of this
          generic model, in which some portion of these local operations
          have been optimized away through static analysis techniques.

          Important applications of our model include the efficient
          allocation of temporary data structures that are passed as
          arguments to anonymous procedures which may or may not use
          these data structures in a stack-like fashion.  The most
          important of these objects are functional arguments (funargs),
          which require some run-time allocation to preserve the local
          environment.  Since a funarg is sometimes returned as a
          first-class value, its lifetime can survive the stack frame in
          which it was created.  Arguments which are evaluated in a lazy
          fashion (Scheme “delays” or “suspensions”) are similarly
          handled.  Variable-length argument “lists” themselves can be
          allocated in this fashion, allowing these objects to become
          “first-class”.  Finally, lazy allocation correctly handles the
          allocation of a Scheme control stack, allowing Scheme
          continuations to become first-class values.

   * Henry G. Baker.  1992.  “NREVERSAL of Fortune - The Thermodynamics
     of Garbage Collection(14)”.  Springer-Verlag.  LNCS Vol.  637.

          Abstract: The need to 'reverse' a computation arises in many
          contexts – debugging, editor undoing, optimistic concurrency
          undoing, speculative computation undoing, trace scheduling,
          exception handling undoing, database recovery, optimistic
          discrete event simulations, subjunctive computing, etc.  The
          need to 'analyze' a reversed computation arises in the context
          of static analysis – liveness analysis, strictness analysis,
          type inference, etc.  Traditional means for restoring a
          computation to a previous state involve checkpoints;
          checkpoints require time to copy, as well as space to store,
          the copied material.  Traditional reverse abstract
          interpretation produces relatively poor information due to its
          inability to guess the previous values of assigned-to
          variables.

          We propose an abstract computer model and a programming
          language – Psi-Lisp – whose primitive operations are injective
          and hence reversible, thus allowing arbitrary undoing without
          the overheads of checkpointing.  Such a computer can be built
          from reversible conservative logic circuits, with the
          serendipitous advantage of dissipating far less heat than
          traditional Boolean AND/OR/NOT circuits.  Unlike functional
          languages, which have one “state” for all times, Psi-Lisp has
          at all times one “state”, with unique predecessor and
          successor states.

          Compiling into a reversible pseudocode can have benefits even
          when targeting a traditional computer.  Certain optimizations,
          e.g., update-in-place, and compile-time garbage collection may
          be more easily performed, because the information may be
          elicited without the difficult and time-consuming iterative
          abstract interpretation required for most non-reversible
          models.

          In a reversible machine, garbage collection for recycling
          storage can always be performed by a reversed
          (sub)computation.  While this “collection is reversed
          mutation” insight does not reduce space requirements when used
          for the computation as a whole, it does save space when used
          to recycle at finer scales.  This insight also provides an
          explanation for the fundamental importance of the push-down
          stack both for recognizing palindromes and for managing
          storage.

          Reversible computers are related to 'Prolog', 'linear logic'
          and 'chemical abstract machines'.

   * Henry G. Baker.  1993.  “'Infant Mortality' and Generational
     Garbage Collection(15)”.  ACM. SIGPLAN Notices 28, 4 (April 1993),
     pp.  55–57.

          Abstract: Generation-based garbage collection has been
          advocated by appealing to the intuitive but vague notion that
          “young objects are more likely to die than old objects”.  The
          intuition is, that if a generation-based garbage collection
          scheme focuses its effort on scanning recently created
          objects, then its scanning efforts will pay off more in the
          form of more recovered garbage, than if it scanned older
          objects.  In this note, we show a counterexample of a system
          in which “infant mortality” is as high as you please, but for
          which generational garbage collection is ineffective for
          improving the average mark/cons ratio.  Other benefits, such
          as better locality and a smaller number of large delays, may
          still make generational garbage collection attractive for such
          a system, however.

   * Henry G. Baker.  1993.  “Equal Rights for Functional Objects or,
     The More Things Change, The More They Are the Same(16)”.  ACM. OOPS
     Messenger 4, 4 (October 1993), pp.  2–27.

          Abstract: We argue that intensional object identity in
          object-oriented programming languages and databases is best
          defined operationally by side-effect semantics.  A corollary
          is that “functional” objects have extensional semantics.  This
          model of object identity, which is analogous to the normal
          forms of relational algebra, provides cleaner semantics for
          the value-transmission operations and built-in primitive
          equality predicate of a programming language, and eliminates
          the confusion surrounding “call-by-value” and
          “call-by-reference” as well as the confusion of multiple
          equality predicates.

          Implementation issues are discussed, and this model is shown
          to have significant performance advantages in persistent,
          parallel, distributed and multilingual processing
          environments.  This model also provides insight into the “type
          equivalence” problem of Algol-68, Pascal and Ada.

   * Henry G. Baker.  1994.  “Minimizing Reference Count Updating with
     Deferred and Anchored Pointers for Functional Data Structures(17)”.
     ACM. SIGPLAN Notices 29, 9 (September 1994), pp.  38–43.

          Abstract: “Reference counting” can be an attractive form of
          dynamic storage management.  It recovers storage promptly and
          (with a garbage stack instead of a free list) it can be made
          “real-time” – i.e., all accesses can be performed in constant
          time.  Its major drawbacks are its inability to reclaim
          cycles, its count storage, and its count update overhead.
          Update overhead is especially irritating for functional
          (read-only) data where updates may dirty pristine cache lines
          and pages.

          We show how reference count updating can be largely eliminated
          for functional data structures by using the “linear style” of
          programming that is inspired by Girard’s linear logic, and by
          distinguishing normal pointers from “anchored pointers”, which
          indicate not only the object itself, but also the depth of the
          stack frame that anchors the object.  An “anchor” for a
          pointer is essentially an enclosing data structure that is
          temporarily locked from being collected for the duration of
          the anchored pointer’s existence by a deferred reference
          count.  An “anchored pointer” thus implies a reference count
          increment that has been deferred until it is either cancelled
          or performed.

          Anchored pointers are generalizations of “borrowed” pointers
          and “phantom” pointers.  Anchored pointers can provide a
          solution to the “derived pointer problem” in garbage
          collection.

   * Henry G. Baker.  1994.  “Thermodynamics and Garbage
     Collection(18)”.  ACM. SIGPLAN Notices 29, 4 (April 1994), pp.
     58–63.

          Abstract: We discuss the principles of statistical
          thermodynamics and their application to storage management
          problems.  We point out problems which result from imprecise
          usage of the terms “information”, “state”, “reversible”,
          “conservative”, etc.

   * Henry G. Baker.  1995.  “'Use-Once' Variables and Linear Objects -
     Storage Management, Reflection and Multi-Threading(19)”.  ACM.
     SIGPLAN Notices 30, 1 (January 1995), pp.  45–52.

          Abstract: Programming languages should have ‘use-once’
          variables in addition to the usual ‘multiple-use’ variables.
          ‘Use-once’ variables are bound to linear (unshared, unaliased,
          or singly-referenced) objects.  Linear objects are cheap to
          access and manage, because they require no synchronization or
          tracing garbage collection.  Linear objects can elegantly and
          efficiently solve otherwise difficult problems of
          functional/mostly-functional systems – e.g., in-place updating
          and the efficient initialization of functional objects.
          Use-once variables are ideal for directly manipulating
          resources which are inherently linear such as freelists and
          ‘engine ticks’ in reflective languages.

          A ‘use-once’ variable must be dynamically referenced exactly
          once within its scope.  Unreferenced use-once variables must
          be explicitly killed, and multiply-referenced use-once
          variables must be explicitly copied; this duplication and
          deletion is subject to the constraint that some linear
          datatypes do not support duplication and deletion methods.
          Use-once variables are bound only to linear objects, which may
          reference other linear or non-linear objects.  Non-linear
          objects can reference other non-linear objects, but can
          reference a linear object only in a way that ensures mutual
          exclusion.

          Although implementations have long had implicit use-once
          variables and linear objects, most languages do not provide
          the programmer any help for their utilization.  For example,
          use-once variables allow for the safe/controlled use of
          reified language implementation objects like single-use
          continuations.

          Linear objects and use-once variables map elegantly into
          dataflow models of concurrent computation, and the graphical
          representations of dataflow models make an appealing visual
          linear programming language.

   * Henry G. Baker.  1995.  'Memory Management: International Workshop
     IWMM’95'.  Springer-Verlag.  ISBN 3-540-60368-9.

          From the Preface: The International Workshop on Memory
          Management 1995 (IWMM’95) is a continuation of the excellent
          series started by Yves Bekkers and Jacques Cohen with IWMM’92.
          The present volume assembles the refereed and invited
          technical papers which were presented during this year’s
          workshop.

   * Nick Barnes, Richard Brooksby, David Jones, Gavin Matthews, Pekka
     P.  Pirinen, Nick Dalton, P. Tucker Withington.  1997.  “A Proposal
     for a Standard Memory Management Interface(20)”.  OOPSLA97 Workshop
     on Garbage Collection and Memory Management.

          From the notes: There is no well-defined memory-management
          library API which would allow programmers to easily choose the
          best memory management implementation for their application.

          Some languages allow replacement of their memory management
          functions, but usually only the program API is specified,
          hence replacement of the entire program interface is required.

          Few languages support multiple memory management policies
          within a single program.  Those that do use proprietary memory
          management policies.

          We believe that the design of an abstract program API is a
          prerequisite to the design of a “server” API and eventually an
          API that would permit multiple cooperating memory “servers”.
          If the interface is simple yet powerful enough to encompass
          most memory management systems, it stands a good chance of
          being widely adopted.

   * David A. Barrett, Benjamin Zorn.  1993.  “Using Lifetime Predictors
     to Improve Memory Allocation Performance(21)”.  ACM. SIGPLAN’93
     Conference on Programming Language Design and Implementation, pp.
     187–196.

          Abstract: Dynamic storage allocation is used heavily in many
          application areas including interpreters, simulators,
          optimizers, and translators.  We describe research that can
          improve all aspects of the performance of dynamic storage
          allocation by predicting the lifetimes of short-lived objects
          when they are allocated.  Using five significant,
          allocation-intensive C programs, we show that a great fraction
          of all bytes allocated are short-lived (> 90% in all cases).
          Furthermore, we describe an algorithm for lifetime prediction
          that accurately predicts the lifetimes of 42–99% of all
          objects allocated.  We describe and simulate a storage
          allocator that takes advantage of lifetime prediction of
          short-lived objects and show that it can significantly improve
          a program’s memory overhead and reference locality, and even,
          at times, improve CPU performance as well.

   * David A. Barrett, Benjamin Zorn.  1995.  “Garbage Collection using
     a Dynamic Threatening Boundary(22)”.  ACM. SIGPLAN’95 Conference on
     Programming Language Design and Implementation, pp.  301–314.

          Abstract: Generational techniques have been very successful in
          reducing the impact of garbage collection algorithms upon the
          performance of programs.  However, it is impossible for
          designers of collection algorithms to anticipate the memory
          allocation behavior of all applications in advance.  Existing
          generational collectors rely upon the applications programmer
          to tune the behavior of the collector to achieve maximum
          performance for each application.  Unfortunately, because the
          many tuning parameters require detailed knowledge of both the
          collection algorithm and the program allocation behavior in
          order to be used effectively, such tuning is difficult and
          error prone.  We propose a new garbage collection algorithm
          that uses just two easily understood tuning parameters that
          directly reflect the maximum memory and pause time constraints
          familiar to application programmers and users.

          Like generational collectors, ours divides memory into two
          spaces, one for short-lived, and another for long-lived
          objects.  Unlike previous work, our collector dynamically
          adjusts the boundary between these two spaces in order to
          directly meet the resource constraints specified by the user.
          We describe two methods for adjusting this boundary, compare
          them with several existing algorithms, and show how
          effectively ours meets the specified constraints.  Our pause
          time collector saved memory by holding median pause times
          closer to the constraint than the other pause time constrained
          algorithm and, when not over-constrained, our memory
          constrained collector exhibited the lowest CPU overhead of the
          algorithms we measured yet was capable of maintaining a
          maximum memory constraint.

   * Joel F. Bartlett.  1988.  “Compacting Garbage Collection with
     Ambiguous Roots(23)”.  Digital Equipment Corporation.

          Abstract: This paper introduces a copying garbage collection
          algorithm which is able to compact most of the accessible
          storage in the heap without having an explicitly defined set
          of pointers that contain all the roots of all accessible
          storage.  Using “hints” found in the processor’s registers and
          stack, the algorithm is able to divide heap allocated objects
          into two groups: those that might be referenced by a pointer
          in the stack or registers, and those that are not.  The
          objects which might be referenced are left in place, and the
          other objects are copied into a more compact representation.

          A Lisp compiler and runtime system which uses such a collector
          need not have complete control of the processor in order to
          force a certain discipline on the stack and registers.  A
          Scheme implementation has been done for the Digital WRL Titan
          processor which uses a garbage collector based on this “mostly
          copying” algorithm.  Like other languages for the Titan, it
          uses the Mahler intermediate language as its target.  This
          simplifies the compiler and allows it to take advantage of the
          significant machine dependent optimizations provided by
          Mahler.  The common intermediate language also simplifies
          call-outs from Scheme programs to functions written in other
          languages and call-backs from functions in other languages.

          Measurements of the Scheme implementation show that the
          algorithm is efficient, as little unneeded storage is retained
          and only a very small fraction of the heap is left in place.

          Simple pointer manipulation protocols also mean that compiler
          support is not needed in order to correctly handle pointers.
          Thus it is reasonable to provide garbage collected storage in
          languages such as C. A collector written in C which uses this
          algorithm is included in the Appendix.

   * Joel F. Bartlett.  1989.  “Mostly-Copying Garbage Collection Picks
     Up Generations and C++(24)”.  Digital Equipment Corporation.

          Abstract: The “mostly-copying” garbage collection algorithm
          provides a way to perform compacting garbage collection in
          spite of the presence of ambiguous pointers in the root set.
          As originally defined, each collection required almost all
          accessible objects to be moved.  While adequate for many
          applications, programs that retained a large amount of storage
          spent a significant amount of time garbage collecting.  To
          improve performance of these applications, a generational
          version of the algorithm has been designed.  This note reports
          on this extension of the algorithm, and its application in
          collectors for Scheme and C++.

   * Yves Bekkers & Jacques Cohen.  1992.  “Memory Management,
     International Workshop IWMM 92(25)”.  Springer-Verlag.  LNCS Vol.
     637, ISBN 3-540-55940-X.

   * Emery D. Berger, Robert D. Blumofe.  1999.  “Hoard: A Fast,
     Scalable, and Memory-Efficient Allocator for Shared-Memory
     Multiprocessors(26)”.  University of Texas at Austin.  UTCS
     TR99-22.

          Abstract: In this paper, we present Hoard, a memory allocator
          for shared-memory multiprocessors.  We prove that its
          worst-case memory fragmentation is asymptotically equivalent
          to that of an optimal uniprocessor allocator.  We present
          experiments that demonstrate its speed and scalability.

   * Emery D. Berger, Benjamin G. Zorn, Kathryn S. McKinley.  2001.
     “Composing high-performance memory allocators(27)” ACM SIGPLAN
     Conference on Programming Language Design and Implementation 2001,
     pp.  114–124.

          Abstract: Current general-purpose memory allocators do not
          provide sufficient speed or flexibility for modern
          high-performance applications.  Highly-tuned general purpose
          allocators have per-operation costs around one hundred cycles,
          while the cost of an operation in a custom memory allocator
          can be just a handful of cycles.  To achieve high performance,
          programmers often write custom memory allocators from scratch
          – a difficult and error-prone process.

          In this paper, we present a flexible and efficient
          infrastructure for building memory allocators that is based on
          C++ templates and inheritance.  This novel approach allows
          programmers to build custom and general-purpose allocators as
          “heap layers” that can be composed without incurring any
          additional runtime overhead or additional programming cost.
          We show that this infrastructure simplifies allocator
          construction and results in allocators that either match or
          improve the performance of heavily-tuned allocators written in
          C, including the Kingsley allocator and the GNU obstack
          library.  We further show this infrastructure can be used to
          rapidly build a general-purpose allocator that has performance
          comparable to the Lea allocator, one of the best uniprocessor
          allocators available.  We thus demonstrate a clean,
          easy-to-use allocator interface that seamlessly combines the
          power and efficiency of any number of general and custom
          allocators within a single application.

   * Hans-J. Boehm, Mark Weiser.  1988.  “Garbage collection in an
     uncooperative environment(28)”.  Software – Practice and
     Experience.  18(9):807–820.

          Abstract: We describe a technique for storage allocation and
          garbage collection in the absence of significant co-operation
          from the code using the allocator.  This limits garbage
          collection overhead to the time actually required for garbage
          collection.  In particular, application programs that rarely
          or never make use of the collector no longer encounter a
          substantial performance penalty.  This approach greatly
          simplifies the implementation of languages supporting garbage
          collection.  It further allows conventional compilers to be
          used with a garbage collector, either as the primary means of
          storage reclamation, or as a debugging tool.

   * Hans-J. Boehm, Alan J. Demers, Scott Shenker.  1991.  “Mostly
     Parallel Garbage Collection(29)”.  Xerox PARC. ACM PLDI 91, SIGPLAN
     Notices 26, 6 (June 1991), pp.  157–164.

          Abstract: We present a method for adapting garbage collectors
          designed to run sequentially with the client, so that they may
          run concurrently with it.  We rely on virtual memory hardware
          to provide information about pages that have been updated or
          “dirtied” during a given period of time.  This method has been
          used to construct a mostly parallel trace-and-sweep collector
          that exhibits very short pause times.  Performance
          measurements are given.

   * Hans-J. Boehm, David Chase.  1992.  “A Proposal for
     Garbage-Collector-Safe C Compilation(30)”.  'Journal of C Language
     Translation.'  vol.  4, 2 (December 1992), pp.  126–141.

          Abstract: Conservative garbage collectors are commonly used in
          combination with conventional C programs.  Empirically, this
          usually works well.  However, there are no guarantees that
          this is safe in the presence of “improved” compiler
          optimization.  We propose that C compilers provide a facility
          to suppress optimizations that are unsafe in the presence of
          conservative garbage collection.  Such a facility can be added
          to an existing compiler at very minimal cost, provided the
          additional analysis is done in a machine-independent
          source-to-source prepass.  Such a prepass may also check the
          source code for garbage-collector-safety.

   * Hans-J. Boehm.  1993.  “Space Efficient Conservative Garbage
     Collection(31)”.  ACM, SIGPLAN. Proceedings of the ACM SIGPLAN ‘91
     Conference on Programming Language Design and Implementation,
     SIGPLAN Notices 28, 6, pp 197–206.

          Abstract: We call a garbage collector conservative if it has
          only partial information about the location of pointers, and
          is thus forced to treat arbitrary bit patterns as though they
          might be pointers, in at least some cases.  We show that some
          very inexpensive, but previously unused techniques can have
          dramatic impact on the effectiveness of conservative garbage
          collectors in reclaiming memory.  Our most significant
          observation is that static data that appears to point to the
          heap should not result in misidentified reference to the heap.
          The garbage collector has enough information to allocate
          around such references.  We also observe that programming
          style has a significantly impact on the amount of spuriously
          retained storage, typically even if the collector is not
          terribly conservative.  Some fairly common C and C++
          programming styles significantly decrease the effectiveness of
          any garbage collector.  These observations suffice to explain
          some of the different assessments of conservative collection
          that have appeared in the literature.

   * Hans-J. Boehm.  2000.  “Reducing Garbage Collector Cache
     Misses(32)”.  ACM. ISMM’00 pp.  59–64.

          Abstract: Cache misses are currently a major factor in the
          cost of garbage collection, and we expect them to dominate in
          the future.  Traditional garbage collection algorithms exhibit
          relatively litle temporal locality; each live object in the
          heap is likely to be touched exactly once during each garbage
          collection.  We measure two techniques for dealing with this
          issue: prefetch-on-grey, and lazy sweeping.  The first of
          these is new in this context.  Lazy sweeping has been in
          common use for a decade.  It was introduced as a mechanism for
          reducing paging and pause times; we argue that it is also
          crucial for eliminating cache misses during the sweep phase.

          Our measurements are obtained in the context of a non-moving
          garbage collector.  Fully copying garbage collection
          inherently requires more traffic through the cache, and thus
          probably also stands to benefit substantially from something
          like the prefetch-on-grey technique.  Generational garbage
          collection may reduce the benefit of these techniques for some
          applications, but experiments with a non-moving generational
          collector suggest that they remain quite useful.

   * Hans-J. Boehm.  2001.  “Bounding Space Usage of Conservative
     Garbage Collectors
     ‘http://www.hpl.hp.com/techreports/2001/HPL-2001-251.html’”.  HP
     Labs technical report HPL-2001-251.

          Abstract: Conservative garbage collectors can automatically
          reclaim unused memory in the absence of precise pointer
          location information.  If a location can possibly contain a
          pointer, it is treated by the collector as though it contained
          a pointer.  Although it is commonly assumed that this can lead
          to unbounded space use due to misidentified pointers, such
          extreme space use is rarely observed in practice, and then
          generally only if the number of misidentified pointers is
          itself unbounded.  We show that if the program manipulates
          only data structures satisfying a simple GC-robustness
          criterion, then a bounded number of misidentified pointers can
          result at most in increasing space usage by a constant factor.
          We argue that nearly all common data structures are already
          GC- robust, and it is typically easy to identify and replace
          those that are not.  Thus it becomes feasible to prove space
          bounds on programs collected by mildly conservative garbage
          collectors, such as the one in Barabash et al.  (2001).  The
          worst-case space overhead introduced by such mild conservatism
          is comparable to the worst-case fragmentation overhead for
          inherent in any non-moving storage allocator.  The same
          GC-robustness criterion also ensures the absence of temporary
          space leaks of the kind discussed in Rojemo (1995) for
          generational garbage collectors.

   * Hans-J. Boehm.  2002.  “Destructors, Finalizers, and
     Synchronization(33)”.  HP Labs technical report HPL-2002-335.

          Abstract: We compare two different facilities for running
          cleanup actions for objects that are about to reach the end of
          their life.  Destructors, such as we find in C++, are invoked
          synchronously when an object goes out of scope.  They make it
          easier to implement cleanup actions for objects of well-known
          lifetime, especially in the presence of exceptions.  Languages
          like Java, Modula-3, and C# provide a different kind of
          “finalization” facility: Cleanup methods may be run when the
          garbage collector discovers a heap object to be otherwise
          inaccessible.  Unlike C++ destructors, such methods run in a
          separate thread at some much less well-defined time.  We argue
          that these are fundamentally different, and potentially
          complementary, language facilities.  We also try to resolve
          some common misunderstandings about finalization in the
          process.  In particular: 1. The asynchronous nature of
          finalizers is not just an accident of implementation or a
          shortcoming of tracing collectors; it is necessary for
          correctness of client code, fundamentally affects how
          finalizers must be written, and how finalization facilities
          should be presented to the user.  2. An object may
          legitimately be finalized while one of its methods are still
          running.  This should and can be addressed by the language
          specification and client code.

   * Robert S. Boyer and J. Strother Moore.  1977.  “A Fast String
     Searching Algorithm(34)”.  'Communications of the ACM'
     20(10):762–772.

          Abstract: An algorithm is presented that searches for the
          location, “'i',” of the first occurrence of a character
          string, “'pat',” in another string, “'string'.” During the
          search operation, the characters of 'pat' are matched starting
          with the last character of 'pat'.  The information gained by
          starting the match at the end of the pattern often allows the
          algorithm to proceed in large jumps through the text being
          searched.  Thus the algorithm has the unusual property that,
          in most cases, not all of the first 'i' characters of 'string'
          are inspected.  The number of characters actually inspected
          (on the average) decreases as a function of the length of
          'pat'.  For a random English pattern of length 5, the
          algorithm will typically inspect 'i'/4 characters of string
          before finding a match at 'i'.  Furthermore, the algorithm has
          been implemented so that (on the average) fewer than 'i' +
          'patlen' machine instructions are executed.  These conclusions
          are supported with empirical evidence and a theoretical
          analysis of the average behavior of the algorithm.  The worst
          case behavior of the algorithm is linear in 'i' + 'patlen',
          assuming the availability of array space for tables linear in
          'patlen' plus the size of the alphabet.

   * P. Branquart, J. Lewi.  1972.  “A scheme of storage allocation and
     garbage collection for ALGOL 68”.  Elsevier/North-Holland.  ALGOL
     68 Implementation – Proceedings of the IFIP Working Conference on
     ALGOL 68 Implementation, July 1970.

   * Richard Brooksby.  2002.  “The Memory Pool System: Thirty
     person-years of memory management development goes Open
     Source(35)”.  ISMM’02.

          Abstract: The Memory Pool System (MPS) is a very general,
          adaptable, flexible, reliable, and efficient memory management
          system.  It permits the flexible combination of memory
          management techniques, supporting manual and automatic memory
          management, in-line allocation, finalization, weakness, and
          multiple simultaneous co-operating incremental generational
          garbage collections.  It also includes a library of memory
          pool classes implementing specialized memory management
          policies.

          Between 1994 and 2001, Harlequin (now part of Global Graphics)
          invested about thirty person-years of effort developing the
          MPS. The system contained many innovative techniques and
          abstractions which were kept secret.  In 1997 Richard
          Brooksby, the manager and chief architect of the project, and
          Nicholas Barnes, a senior developer, left Harlequin to form
          their own consultancy company, Ravenbrook, and in 2001,
          Ravenbrook acquired the MPS technology from Global Graphics.
          We are happy to announce that we are publishing the source
          code and documentation under an open source licence.  This
          paper gives an overview of the system.

   * International Standard ISO/IEC 9899:1990.  “Programming languages —
     C”.

   * International Standard ISO/IEC 9899:1999.  “Programming languages —
     C(36)”.

   * Brad Calder, Dirk Grunwald, Benjamin Zorn.  1994.  “Quantifying
     Behavioral Differences Between C and C++ Programs(37)”.  'Journal
     of Programming Languages.'  2(4):313–351.

          Abstract: Improving the performance of C programs has been a
          topic of great interest for many years.  Both hardware
          technology and compiler optimization research has been applied
          in an effort to make C programs execute faster.  In many
          application domains, the C++ language is replacing C as the
          programming language of choice.  In this paper, we measure the
          empirical behavior of a group of significant C and C++
          programs and attempt to identify and quantify behavioral
          differences between them.  Our goal is to determine whether
          optimization technology that has been successful for C
          programs will also be successful in C++ programs.  We
          furthermore identify behavioral characteristics of C++
          programs that suggest optimizations that should be applied in
          those programs.  Our results show that C++ programs exhibit
          behavior that is significantly different than C programs.
          These results should be of interest to compiler writers and
          architecture designers who are designing systems to execute
          object-oriented programs.

   * Dante J. Cannarozzi, Michael P. Plezbert, Ron K. Cytron.  2000.
     “Contaminated garbage collection(38)”.  ACM. Proceedings of the ACM
     SIGPLAN ‘00 conference on on Programming language design and
     implementation, pp.  264–273.

          Abstract: We describe a new method for determining when an
          object can be garbage collected.  The method does not require
          marking live objects.  Instead, each object 'X' is
          'dynamically' associated with a stack frame 'M', such that 'X'
          is collectable when 'M' pops.  Because 'X' could have been
          dead earlier, our method is conservative.  Our results
          demonstrate that the methos nonetheless idenitifies a large
          percentage of collectable objects.  The method has been
          implemented in Sun’s Java™ Virtual Machine interpreter, and
          results are presented based on this implementation.

   * Patrick J. Caudill, Allen Wirfs-Brock.  1986.  “A Third-Generation
     Smalltalk-80 Implementation”.  ACM. SIGPLAN Notices.  21(11),
     OOPSLA’86 ACM Conference on Object-Oriented Systems, Languages and
     Applications.

          Abstract: A new, high performance Smalltalk-80™ implementation
          is described which builds directly upon two previous
          implementation efforts.  This implementation supports a large
          object space while retaining compatibility with previous
          Smalltalk-80™ images.  The implementation utilizes a
          interpreter which incorporates a generation based garbage
          collector and which does not have an object table.  This paper
          describes the design decisions which lead to this
          implementation and reports preliminary performance results.

   * C. J. Cheney.  1970.  “A non-recursive list compacting
     algorithm(39)”.  CACM. 13-11 pp.  677–678.

          Abstract: A simple nonrecursive list structure compacting
          scheme or garbage collector suitable for both compact and
          LISP-like list structures is presented.  The algorithm avoids
          the need for recursion by using the partial structure as it is
          built up to keep track of those lists that have been copied.

   * Perry Cheng, Robert Harper, Peter Lee.  1998.  “Generational stack
     collection and profile-driven pretenuring(40)”.  ACM. Proceedings
     of SIGPLAN’98 Conference on Programming Language Design and
     Implementation, pp.  162–173.

          Abstract: This paper presents two techniques for improving
          garbage collection performance: generational stack collection
          and profile-driven pretenuring.  The first is applicable to
          stack-based implementations of functional languages while the
          second is useful for any generational collector.  We have
          implemented both techniques in a generational collector used
          by the TIL compiler, and have observed decreases in garbage
          collection times of as much as 70% and 30%, respectively.

          Functional languages encourage the use of recursion which can
          lead to a long chain of activation records.  When a collection
          occurs, these activation records must be scanned for roots.
          We show that scanning many activation records can take so long
          as to become the dominant cost of garbage collection.
          However, most deep stacks unwind very infrequently, so most of
          the root information obtained from the stack remains unchanged
          across successive garbage collections.  'Generational stack
          collection' greatly reduces the stack scan cost by reusing
          information from previous scans.

          Generational techniques have been successful in reducing the
          cost of garbage collection.  Various complex heap arrangements
          and tenuring policies have been proposed to increase the
          effectiveness of generational techniques by reducing the cost
          and frequency of scanning and copying.  In contrast, we show
          that by using profile information to make lifetime
          predictions, 'pretenuring' can avoid copying data altogether.
          In essence, this technique uses a refinement of the
          generational hypothesis (most data die young) with a locality
          principle concerning the age of data: most allocations sites
          produce data that immediately dies, while a few allocation
          sites consistently produce data that survives many
          collections.

   * Trishul M. Chilimbi, James R. Larus.  1998.  “Using Generational
     Garbage Collection To Implement Cache-Conscious Data
     Placement(41)”.  ACM. ISMM’98 pp.  37–48.

          Abstract: Processor and memory technology trends show a
          continual increase in the cost of accessing main memory.
          Machine designers have tried to mitigate the effect of this
          trend through a variety of techniques that attempt to reduce
          or tolerate memory latency.  These techniques, unfortunately,
          have only been partially successful for pointer-manipulating
          programs.  Recent research has demonstrated that these
          programs can benefit greatly from the complementary approach
          of reorganizing pointer data structures to improve cache
          locality.  This paper describes how a generational garbage
          collector can be used to achieve a cache-conscious data
          layout, in which objects with high temporal affinity are
          placed next to each other, so they are likely to reside in the
          same cache block.  The paper demonstrates the feasibility of
          collecting low overhead, real-time profiling information about
          data access patterns for object-oriented languages, and
          describes a new copying algorithm that utilizes this
          information to produce a cache-conscious object layout.
          Preliminary results indicate that this technique reduces cache
          miss rates by 21-42%, and improves program performance by
          14-37%.

   * William D Clinger & Lars T Hansen.  1997.  “Generational Garbage
     Collection and the Radioactive Decay Model(42)”.  ACM. Proceedings
     of PLDI 1997.

          Abstract: If a fixed exponentially decreasing probability
          distribution function is used to model every object’s
          lifetime, then the age of an object gives no information about
          its future life expectancy.  This 'radioactive decay model'
          implies that there can be no rational basis for deciding which
          live objects should be promoted to another generation.  Yet
          there remains a rational basis for deciding how many objects
          to promote, when to collect garbage, and which generations to
          collect.

          Analysis of the model leads to a new kind of generational
          garbage collector whose effectiveness does not depend upon
          heuristics that predict which objects will live longer than
          others.

          This result provides insight into the computational advantages
          of generational garbage collection, with implications for the
          management of objects whose life expectancies are difficult to
          predict.

   * Jacques Cohen.  1981.  “Garbage collection of linked data
     structures”.  Computing Surveys.  Vol.  13, no.  3.

          Abstract: A concise and unified view of the numerous existing
          algorithms for performing garbage collection of linked data
          structures is presented.  The emphasis is on garbage
          collection proper, rather than on storage allocation.

          First, the classical garbage collection algorithms and their
          marking and collecting phases, with and without compacting,
          are discussed.

          Algorithms describing these phases are classified according to
          the type of cells to be collected: those for collecting
          single-sized cells are simpler than those for varisized cells.
          Recently proposed algorithms are presented and compared with
          the classical ones.  Special topics in garbage collection are
          also covered.  A bibliography with topical annotations is
          included.

   * Dominique Colnet, Philippe Coucaud, Olivier Zendra.  1998.
     “Compiler Support to Customize the Mark and Sweep Algorithm(43)”.
     ACM. ISMM’98 pp.  154–165.

          Abstract: Mark and sweep garbage collectors (GC) are classical
          but still very efficient automatic memory management systems.
          Although challenged by other kinds of systems, such as copying
          collectors, mark and sweep collectors remain among the best in
          terms of performance.

          This paper describes our implementation of an efficient mark
          and sweep garbage collector tailored to each program.
          Compiler support provides the type information required to
          statically and automatically generate this customized garbage
          collector.  The segregation of object by type allows the
          production of a more efficient GC code.  This technique,
          implemented in SmallEiffel, our compiler for the
          object-oriented language Eiffel, is applicable to other
          languages and other garbage collection algorithms, be they
          distributed or not.

          We present the results obtained on programs featuring a
          variety of programming styles and compare our results to a
          well-known and high-quality garbage collector.

   * Jonathan E. Cook, Alexander L. Wolf, Benjamin Zorn.  1994.
     “Partition Selection Policies in Object Database Garbage
     Collection(44)”.  ACM. SIGMOD. International Conference on the
     Management of Data (SIGMOD’94), pp.  371–382.

          Abstract: The automatic reclamation of storage for
          unreferenced objects is very important in object databases.
          Existing language system algorithms for automatic storage
          reclamation have been shown to be inappropriate.  In this
          paper, we investigate methods to improve the performance of
          algorithms for automatic storage reclamation of object
          databases.  These algorithms are based on a technique called
          partitioned garbage collection, in which a subset of the
          entire database is collected independently of the rest.
          Specifically, we investigate the policy that is used to select
          what partition in the database should be collected.  The new
          partition selection policies that we propose and investigate
          are based on the intuition that the values of overwritten
          pointers provide good hints about where to find garbage.
          Using trace-driven simulation, we show that one of our
          policies requires less I/O to collect more garbage than any
          existing implementable policy and performs close to an
          impractical-to-implement but near-optimal policy over a wide
          range of database sizes and connectivities.

   * Jonathan E. Cook, Artur Klauser, Alexander L. Wolf, Benjamin Zorn.
     1996.  “Semi-automatic, Self-adaptive Control of Garbage Collection
     Rates in Object Databases(45)”.  ACM, SIGMOD. International
     Conference on the Management of Data (SIGMOD’96), pp.  377–388.

          Abstract: A fundamental problem in automating object database
          storage reclamation is determining how often to perform
          garbage collection.  We show that the choice of collection
          rate can have a significant impact on application performance
          and that the “best” rate depends on the dynamic behavior of
          the application, tempered by the particular performance goals
          of the user.  We describe two semi-automatic, self-adaptive
          policies for controlling collection rate that we have
          developed to address the problem.  Using trace-driven
          simulations, we evaluate the performance of the policies on a
          test database application that demonstrates two distinct
          reclustering behaviors.  Our results show that the policies
          are effective at achieving user-specified levels of I/O
          operations and database garbage percentage.  We also
          investigate the sensitivity of the policies over a range of
          object connectivities.  The evaluation demonstrates that
          semi-automatic, self-adaptive policies are a practical means
          for flexibly controlling garbage collection rate.

   * Eric Cooper, Scott Nettles, Indira Subramanian.  1992.  “Improving
     the Performance of SML Garbage Collection using
     Application-Specific Virtual Memory Management”.  ACM Conference on
     LISP and Functional Programming, pp.  43–52.

          Abstract: We improved the performance of garbage collection in
          the Standard ML of New Jersey system by using the virtual
          memory facilities provided by the Mach kernel.  We took
          advantage of Mach’s support for large sparse address spaces
          and user-defined paging servers.  We decreased the elapsed
          time for realistic applications by as much as a factor of 4.

   * Michael C. Daconta.  1993.  'C Pointers and Dynamic Memory
     Management.'  Wiley.  ISBN 0-471-56152-5.

   * Michael C. Daconta.  1995.  'C++ Pointers and Dynamic Memory
     Management.'  Wiley.  ISBN 0-471-04998-0.

          From the back cover: Using techniques developed in the
          classroom at America Online’s Programmer’s University, Michael
          Daconta deftly pilots programmers through the intricacies of
          the two most difficult aspects of C++ programming: pointers
          and dynamic memory management.  Written by a programmer for
          programmers, this no-nonsense, nuts-and-bolts guide shows you
          how to fully exploit advanced C++ programming features, such
          as creating class-specific allocators, understanding
          references versus pointers, manipulating multidimensional
          arrays with pointers, and how pointers and dynamic memory are
          the core of object-oriented constructs like inheritance,
          name-mangling, and virtual functions.

   * O.-J. Dahl.  1963.  “The SIMULA Storage Allocation Scheme”.  Norsk
     Regnesentral.  NCC Document no.  162.

   * P. J. Denning.  1968.  “Thrashing: Its Causes and Prevention(46)”.
     Proceedings AFIPS,1968 Fall Joint Computer Conference, vol.  33,
     pp.  915–922.

          From the introduction: A particularly troublesome phenomenon,
          thrashing, may seriously interfere with the performance of
          paged memory systems, reducing computing giants (Multics, IBM
          System 360, and others not necessarily excepted) to computing
          dwarfs.  The term thrashing denotes excessive overhead and
          severe performance degradation or collapse caused by too much
          paging.  Thrashing inevitably turns a shortage of memory space
          into a surplus of processor time.

   * P. J. Denning.  1970.  “Virtual Memory(47)”.  ACM. ACM Computing
     Surveys, vol.  2, no.  3, pp.  153–190, Sept.  1970.

          Abstract: The need for automatic storage allocation arises
          from desires for program modularity, machine independence, and
          resource sharing.  Virtual memory is an elegant way of
          achieving these objectives.  In a virtual memory, the
          addresses a program may use to identify information are
          distinguished from the addresses the memory system uses to
          identify physical storage sites, and program-generated
          addresses are translated automatically to the corresponding
          machine addresses.  Two principal methods for implementing
          virtual memory, segmentation and paging, are compared and
          contrasted.  Many contemporary implementations have
          experienced one or more of these problems: poor utilization of
          storage, thrashing, and high costs associated with loading
          information into memory.  These and subsidiary problems are
          studied from a theoretic view, and are shown to be
          controllable by a proper combination of hardware and memory
          management policies.

   * P. J. Denning, S. C. Schwartz.  1972.  “Properties of the
     Working-set Model(48)”.  CACM. vol.  15, no.  3, pp.  191–198.

          Abstract: A program’s working set 'W'('t', 'T') at time 't' is
          the set of distinct pages among the 'T' most recently
          referenced pages.  Relations between the average working-set
          size, the missing-page rate, and the interreference-interval
          distribution may be derived both from time-average definitions
          and from ensemble-average (statistical) definitions.  An
          efficient algorithm for estimating these quantities is given.
          The relation to LRU (least recently used) paging is
          characterized.  The independent-reference model, in which page
          references are statistically independent, is used to assess
          the effects of interpage dependencies on working-set size
          observations.  Under general assumptions, working-set size is
          shown to be normally distributed.

   * David L. Detlefs.  1992.  “Garbage collection and runtime typing as
     a C++ library(49)”.  USENIX C++ Conference.

          From the introduction: Automatic storage management, or
          'garbage collection', is a feature that can ease program
          development and enhance program reliability.  Many high-level
          languages other than C++ provide garbage collection.  This
          paper proposes the use of “smart pointer” template classes as
          an interface for the use of garbage collection in C++.
          Template classes and operator overloading are techniques
          allowing language extension at the level of user code; I claim
          that using these techniques to create smart pointer classes
          provdes a syntax for manipulating garbage-collected storage
          safely and conveniently.  Further, the use of a smart-pointer
          template class offers the possibility of implementing the
          collector at the user-level, without requiring support from
          the compiler.  If such a compiler-independent implementation
          is possible with adequate performance, then programmers can
          start to write code using garbage collection without waiting
          for language and compiler modifications.  If the use of such a
          garbage collection interface becomes widespread, then C++
          compilation systems can be built to specially support tht
          garbage collection interface, thereby allowing the use of
          collection algorithms with enhanced performance.

   * David L. Detlefs, Al Dosser, Benjamin Zorn.  1994.  “Memory
     Allocation Costs in Large C and C++ Programs(50)”.  Software –
     Practice and Experience.  24(6):527–542.

          Abstract: Dynamic storage allocation is an important part of a
          large class of computer programs written in C and C++.
          High-performance algorithms for dynamic storage allocation
          have been, and will continue to be, of considerable interest.
          This paper presents detailed measurements of the cost of
          dynamic storage allocation in 11 diverse C and C++ programs
          using five very different dynamic storage allocation
          implementations, including a conservative garbage collection
          algorithm.  Four of the allocator implementations measured are
          publicly-available on the Internet.  A number of the programs
          used in these measurements are also available on the Internet
          to facilitate further research in dynamic storage allocation.
          Finally, the data presented in this paper is an abbreviated
          version of more extensive statistics that are also
          publicly-available on the Internet.

   * L. Peter Deutsch, Daniel G. Bobrow.  1976.  “An Efficient,
     Incremental, Automatic Garbage Collector(51)”.  CACM. vol.  19, no.
     9, pp.  522–526.

          Abstract: This paper describes a new way of solving the
          storage reclamation problem for a system such as Lisp that
          allocates storage automatically from a heap, and does not
          require the programmer to give any indication that particular
          items are no longer useful or accessible.  A reference count
          scheme for reclaiming non-self-referential structures, and a
          linearizing, compacting, copying scheme to reorganize all
          storage at the users discretion are proposed.  The algorithms
          are designed to work well in systems which use multiple levels
          of storage, and large virtual address space.  They depend on
          the fact that most cells are referenced exactly once, and that
          reference counts need only be accurate when storage is about
          to be reclaimed.  A transaction file stores changes to
          reference counts, and a multiple reference table stores the
          count for items which are referenced more than once.

   * E. W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, E. F.
     M. Steffens.  1976.  “On-the-fly Garbage Collection: An Exercise in
     Cooperation(52)”.  Springer-Verlag.  Lecture Notes in Computer
     Science, Vol.  46.

          Abstract: As an example of cooperation between sequential
          processes with very little mutual interference despite
          frequent manipulations of a large shared data space, a
          technique is developed which allows nearly all of the activity
          needed for garbage detection and collection to be performed by
          an additional processor operating con- currently with the
          processor devoted to the computation proper.  Exclusion and
          synchronization constraints have been kept as weak as could be
          achieved; the severe complexities engendered by doing so are
          illustrated.

   * Amer Diwan, Richard L. Hudson, J. Eliot B. Moss.  1992.  “Compiler
     Support for Garbage Collection in a Statically Typed Language(53)”.
     ACM. Proceedings of the 5th ACM SIGPLAN conference on Programming
     language design and implementation, pp.  273–282.

          Abstract: We consider the problem of supporting compacting
          garbage collection in the presence of modern compiler
          optimizations.  Since our collector may move any heap object,
          it must accurately locate, follow, and update all pointers and
          values derived from pointers.  To assist the collector, we
          extend the compiler to emit tables describing live pointers,
          and values derived from pointers, at each program location
          where collection may occur.  Significant results include
          identification of a number of problems posed by optimizations,
          solutions to those problems, a working compiler, and
          experimental data concerning table sizes, table compression,
          and time overhead of decoding tables during collection.  While
          gc support can affect the code produced, our sample programs
          show no significant changes, the table sizes are a modest
          fraction of the size of the optimized code, and stack tracing
          is a small fraction of total gc time.  Since the compiler
          enhancements are also modest, we conclude that the approach is
          practical.

   * Amer Diwan, David Tarditi, J. Eliot B. Moss.  1993.  “Memory
     Subsystem Performance of Programs with Intensive Heap
     Allocation(54)”.  Carnegie Mellon University.  CMU-CS-93-227.

          Abstract: Heap allocation with copying garbage collection is a
          general storage management technique for modern programming
          languages.  It is believed to have poor memory subsystem
          performance.  To investigate this, we conducted an in-depth
          study of the memory subsystem performance of heap allocation
          for memory subsystems found on many machines.  We studied the
          performance of mostly-functional Standard ML programs which
          made heavy use of heap allocation.  We found that most
          machines support heap allocation poorly.  However, with the
          appropriate memory subsystem organization, heap allocation can
          have good performance.  The memory subsystem property crucial
          for achieving good performance was the ability to allocate and
          initialize a new object into the cache without a penalty.
          This can be achieved by having subblock placement with a
          subblock size of one word with a write allocate policy, along
          with fast page-mode writes or a write buffer.  For caches with
          subblock placement, the data cache overhead was under 9% for a
          64k or larger data cache; without subblock placement the
          overhead was often higher than 50%.

   * Amer Diwan, David Tarditi, J. Eliot B. Moss.  1994.  “Memory
     Subsystem Performance of Programs Using Copying Garbage
     Collection(55)”.  ACM. CMU-CS-93-210, also in POPL ‘94.

          Abstract: Heap allocation with copying garbage collection is
          believed to have poor memory subsystem performance.  We
          conducted a study of the memory subsystem performance of heap
          allocation for memory subsystems found on many machines.  We
          found that many machines support heap allocation poorly.
          However, with the appropriate memory subsystem organization,
          heap allocation can have good memory subsystem performance.

   * Damien Doligez & Xavier Leroy.  1993.  “A concurrent, generational
     garbage collector for a multithreaded implementation of ML(56)”.
     ACM. POPL ‘93, 113–123.

          Abstract: This paper presents the design and implementation of
          a “quasi real-time” garbage collector for Concurrent Caml
          Light, an implementation of ML with threads.  This
          two-generation system combines a fast, asynchronous copying
          collector on the young generation with a non-disruptive
          concurrent marking collector on the old generation.  This
          design crucially relies on the ML compile-time distinction
          between mutable and immutable objects.

   * Damien Doligez & Georges Gonthier.  1994.  “Portable, unobtrusive
     garbage collection for multiprocessor systems(57)”.  ACM. POPL ‘94,
     70–83.

          Abstract: We describe and prove the correctness of a new
          concurrent mark-and-sweep garbage collection algorithm.  This
          algorithm derives from the classical on-the-fly algorithm from
          Dijkstra et al.  A distinguishing feature of our algorithm is
          that it supports multiprocessor environments where the
          registers of running processes are not readily accessible,
          without imposing any overhead on the elementary operations of
          loading a register or reading or initializing a field.
          Furthermore our collector never blocks running mutator
          processes except possibly on requests for free memory; in
          particular, updating a field or creating or marking or
          sweeping a heap object does not involve system-dependent
          synchronization primitives such as locks.  We also provide
          support for process creation and deletion, and for managing an
          extensible heap of variable-sized objects.

   * R. Kent Dybvig, Carl Bruggeman, David Eby.  1993.  “Guardians in a
     Generation-Based Garbage Collector(58)”.  SIGPLAN. Proceedings of
     the ACM SIGPLAN ‘93 Conference on Programming Language Design and
     Implementation, June 1993.

          Abstract: This paper describes a new language feature that
          allows dynamically allocated objects to be saved from
          deallocation by an automatic storage management system so that
          clean-up or other actions can be performed using the data
          stored within the objects.  The program has full control over
          the timing of clean-up actions, which eliminates several
          potential problems and often eliminates the need for critical
          sections in code that interacts with clean-up actions.  Our
          implementation is “generation-friendly” in the sense that the
          additional overhead within the mutator is proportional to the
          number of clean-up actions actually performed.

   * Daniel R. Edelson.  1992.  “Smart pointers: They're smart, but
     they're not pointers(59)”.  USENIX C++ Conference.

          From the introduction: This paper shows hhow the behaviour of
          smart pointers diverges from that of pointers in certain
          common C++ constructs.  Given this, we conclude that the C++
          programming language does not support seamless smart pointers:
          smart pointers cannot transparently replace raw pointers in
          all ways except declaration syntax.  We show that this
          conclusion also applies to 'accessors'.

   * Daniel R. Edelson.  1992.  “Comparing Two Garbage Collectors for
     C++(60)”.  University of California at Santa Cruz.  Technical
     Report UCSC-CRL-93-20.

          Abstract: Our research is concerned with compiler-
          independent, tag-free garbage collection for the C++
          programming language.  This paper presents a mark-and-sweep
          collector, and explains how it ameliorates shortcomings of a
          previous copy collector.  The new collector, like the old,
          uses C++’s facilities for creating abstract data types to
          define a 'tracked reference' type, called 'roots', at the
          level of the application program.  A programmer wishing to
          utilize the garbage collection service uses these roots in
          place of normal, raw pointers.  We present a detailed study of
          the cost of using roots, as compared to both normal pointers
          and reference counted pointers, in terms of instruction
          counts.  We examine the efficiency of a small C++ application
          using roots, reference counting, manual reclamation, and
          conservative collection.  Coding the application to use
          garbage collection, and analyzing the resulting efficiency,
          helped us identify a number of memory leaks and inefficiencies
          in the original, manually reclaimed version.  We find that for
          this program, garbage collection using roots is much more
          efficient than reference counting, though less efficient than
          manual reclamation.  It is hard to directly compare our
          collector to the conservative collector because of the
          differing efficiencies of their respective memory allocators.

   * Daniel J. Edwards.  n.d.  “Lisp II Garbage Collector(61)”.  MIT. AI
     Memo 19 (AIM-19).

          Our summary: (This short memo doesn’t have an abstract.
          Basically, it describes the plan for the LISP II Relocating
          Garbage Collector.  It has four phases: marking, collection,
          relocation and moving.  Marking is by recursive descent using
          a bit table.  The remaining phases are linear sweeps through
          the bit table.  The collection phase calculates how much
          everything needs to move, storing this information in the free
          blocks.  The relocation phase updates all relocatable
          addresses.  The moving phase moves the surviving objects into
          one contiguous block.)

   * John R. Ellis, David L. Detlefs.  1993.  “Safe, Efficient Garbage
     Collection for C++(62)”.  Xerox PARC.

          Abstract: We propose adding safe, efficient garbage collection
          to C++, eliminating the possibility of storage-management bugs
          and making the design of complex, object-oriented systems much
          easier.  This can be accomplished with almost no change to the
          language itself and only small changes to existing
          implementations, while retaining compatibility with existing
          class libraries.

   * Paulo Ferreira.  1996.  “Larchant: garbage collection in a cached
     distributed shared store with persistence by reachability(63)”.
     Université Paris VI. Thése de doctorat.

          Abstract: The model of Larchant is that of a 'Shared Address
          Space' (spanning every site in a network including secondary
          storage) with 'Persistence By Reachability'.  To provide the
          illusion of a shared address space across the network, despite
          the fact that site memories are disjoint, Larchant implements
          a 'distributed shared memory' mechanism.  Reachability is
          accessed by tracing the pointer graph, starting from the
          persistent root, and reclaiming unreachable objects.  This is
          the task of 'Garbage Collection' (GC).

          GC was until recently thought to be intractable in a
          large-scale system, due to problems of scale, incoherence,
          asynchrony, and performance.  This thesis presents the
          solutions that Larchant proposes to these problems.

          The GC algorithm in Larchant combines tracing and
          reference-listing.  It traces whenever economically feasible,
          i.e., as long as the memory subset being collected remains
          local to a site, and counts references that would cost I/O
          traffic to trace.  GC is orthogonal to coherence, i.e., makes
          progress even if only incoherent replicas are locally
          available.  The garbage collector runs concurrently and
          asynchronously to applications.  The reference-listing
          boundary changes dynamically and seamlessly, and independently
          at each site, in order to collect cycles of unreachable
          objects.

          We prove formally that our GC algorithm is correct, i.e., it
          is safe and live.  The performance results from our Larchant
          prototype show that our design goals (scalability, coherence
          orthogonality, and good performance) are fulfilled.

   * Paulo Ferreira & Marc Shapiro.  1998.  “Modelling a Distributed
     Cached Store for Garbage Collection(64)”.  Springer-Verlag.
     Proceedings of 12th European Conference on Object-Oriented
     Programming, ECOOP98, LNCS 1445.

          Abstract: Caching and persistence support efficient,
          convenient and transparent distributed data sharing.  The most
          natural model of persistence is persistence by reachability,
          managed automatically by a garbage collector (GC). We propose
          a very general model of such a system (based on distributed
          shared memory) and a scalable, asynchronous distributed GC
          algorithm.  Within this model, we show sufficient and widely
          applicable correctness conditions for the interactions between
          applications, store, memory, coherence, and GC.

          The GC runs as a set of processes (local to each participating
          machine) communicating by asynchronous messages.  Collection
          does not interfere with applications by setting locks,
          polluting caches, or causing I/O; this requirement raised some
          novel and interesting challenges which we address in this
          article.  The algorithm is safe and live; it is not complete,
          i.e.  it collects some distributed cycles of garbage but not
          necessarily all.

   * Daniel P Friedman, David S. Wise.  1976.  “Garbage collecting a
     heap which includes a scatter table(65)”.  'Information Processing
     Letters.'  5, 6 (December 1976): 161–164.

          Abstract: A new algorithm is introduced for garbage collecting
          a heap which contains shared data structures accessed from a
          scatter table.  The scheme provides for the purging of useless
          entries from the scatter table with no traversals beyond the
          two required by classic collection schemes.  For languages
          which use scatter tables to sustain unique existence of
          complex structures, like natural variables of SNOBOL, it
          indirectly allows liberal use of a single scatter table by
          ensuring efficient deletion of useless entries.  Since the
          scatter table is completely restructured during the course of
          execution, the hashing scheme itself is easily altered during
          garbage collection whenever skewed loading of the scatter
          table warrants abandonment of the old hashing.  This procedure
          is applicable to the maintenance of dynamic structures such as
          those in information retrieval schemes or in languages like
          LISP and SNOBOL.

   * Daniel P Friedman, David S. Wise.  1977.  “The One Bit Reference
     Count(66)”.  'BIT.' (17)3: 351–359.

          Abstract: Deutsch and Bobrow propose a storage reclamation
          scheme for a heap which is a hybrid of garbage collection and
          reference counting.  The point of the hybrid scheme is to keep
          track of very low reference counts between necessary
          invocation of garbage collection so that nodes which are
          allocated and rather quickly abandoned can be returned to
          available space, delaying necessity for garbage collection.
          We show how such a scheme may be implemented using the mark
          bit already required in every node by the garbage collector.
          Between garbage collections that bit is used to distinguish
          nodes with a reference count known to be one.  A significant
          feature of our scheme is a small cache of references to nodes
          whose implemented counts “ought to be higher” which prevents
          the loss of logical count information in simple manipulations
          of uniquely referenced structures.

   * Daniel P Friedman, David S. Wise.  1979.  “Reference counting can
     manage the circular environments of mutual recursion(67)”.
     'Information Processing Letters.'  8, 1 (January 1979): 41–45.

          From the introduction: In this note we advance reference
          counting as a storage management technique viable for
          implementing recursive languages like ISWIM or pure LISP with
          the ‘labels’ construct for implementing mutual recursion from
          SCHEME. ‘Labels’ is derived from ‘letrec’ and displaces the
          ‘label’ operator, a version of the paradoxical Y-combinator.
          The key observation is that the requisite circular structure
          (which ordinarily cripples reference counts) occurs only
          within the language–rather than the user–structure, and that
          the references into this structure are well-controlled.

   * Dirk Grunwald, Benjamin Zorn, R. Henderson.  1993.  “Improving the
     Cache Locality of Memory Allocation(68)”.  SIGPLAN. SIGPLAN ‘93,
     Conference on PLDI, June 1993, Albuquerque, New Mexico.

          Abstract: The allocation and disposal of memory is a
          ubiquitous operation in most programs.  Rarely do programmers
          concern themselves with details of memory allocators; most
          assume that memory allocators provided by the system perform
          well.  This paper presents a performance evaluation of the
          reference locality of dynamic storage allocation algorithms
          based on trace-driven simulation of five large
          allocation-intensive C programs.  In this paper, we show how
          the design of a memory allocator can significantly affect the
          reference locality for various applications.  Our measurements
          show that poor locality in sequential-fit algorithms reduces
          program performance, both by increasing paging and cache miss
          rates.  While increased paging can be debilitating on any
          architecture, cache misses rates are also important for modern
          computer architectures.  We show that algorithms attempting to
          be space-efficient, by coalescing adjacent free objects show
          poor reference locality, possibly negating the benefits of
          space efficiency.  At the other extreme, algorithms can expend
          considerable effort to increase reference locality yet gain
          little in total execution performance.  Our measurements
          suggest an allocator design that is both very fast and has
          good locality of reference.

   * Dirk Grunwald & Benjamin Zorn.  1993.  “CustoMalloc: Efficient
     Synthesized Memory Allocators(69)”.  Software – Practice and
     Experience.  23(8):851–869.

          Abstract: The allocation and disposal of memory is a
          ubiquitous operation in most programs.  Rarely do programmers
          concern themselves with details of memory allocators; most
          assume that memory allocators provided by the system perform
          well.  Yet, in some applications, programmers use
          domain-specific knowledge in an attempt to improve the speed
          or memory utilization of memory allocators.  In this paper, we
          describe a program (CustoMalloc) that synthesizes a memory
          allocator customized for a specific application.  Our
          experiments show that the synthesized allocators are uniformly
          faster than the common binary-buddy (BSD) allocator, and are
          more space efficient.  Constructing a custom allocator
          requires little programmer effort.  The process can usually be
          accomplished in a few minutes, and yields results superior
          even to domain-specific allocators designed by programmers.
          Our measurements show the synthesized allocators are from two
          to ten times faster than widely used allocators.

   * David Gudeman.  1993.  “Representing Type Information in
     Dynamically Typed Languages(70)”.  University of Arizona at Tucson.
     Technical Report TR 93-27.

          Abstract: This report is a discussion of various techniques
          for representing type information in dynamically typed
          languages, as implemented on general-purpose machines (and
          costs are discussed in terms of modern RISC machines).  It is
          intended to make readily available a large body of knowledge
          that currently has to be absorbed piecemeal from the
          literature or re-invented by each language implementor.  This
          discussion covers not only tagging schemes but other forms of
          representation as well, although the discussion is strictly
          limited to the representation of type information.  It should
          also be noted that this report does not purport to contain a
          survey of the relevant literature.  Instead, this report
          gathers together a body of folklore, organizes it into a
          logical structure, makes some generalizations, and then
          discusses the results in terms of modern hardware.

   * Timothy Harris.  1999.  “Early storage reclamation in a tracing
     garbage collector(71)”.  ACM. ACM SIG-PLAN Notices 34:4, pp.
     46–53.

          Abstract: This article presents a technique for allowing the
          early recovery of storage space occupied by garbage data.  The
          idea is similar to that of generational garbage collection,
          except that the heap is partitioned based on a static analysis
          of data type definitions rather than on the approximate age of
          allocated objects.  A prototype implementation is presented,
          along with initial results and ideas for future work.

   * Roger Henriksson.  1994.  “Scheduling Real Time Garbage
     Collection”.  Department of Computer Science at Lund University.
     LU-CS-TR:94-129.

          Abstract: This paper presents a new model for scheduling the
          work of an incremental garbage collector in a system with hard
          real time requirements.  The method utilizes the fact that
          just some of the processes in the system have to meet hard
          real time requirements and that these processes typically run
          periodically, a fact that we can make use of when scheduling
          the garbage collection.  The work of the collector is
          scheduled to be performed in the pauses between the critical
          processes and is suspended when the processes with hard real
          time requirements run.  It is shown that this approach is
          feasible for many real time systems and that it leaves the
          time-critical parts of the system undisturbed from garbage
          collection induced delays.

   * Roger Henriksson.  1996.  “Adaptive Scheduling of Incremental
     Copying Garbage Collection for Interactive Applications(72)”.
     NWPER96.

          Abstract: Incremental algorithms are often used to interleave
          the work of a garbage collector with the execution of an
          application program, the intention being to avoid long pauses.
          However, overestimating the worst-case storage needs of the
          program often causes all the garbage collection work to be
          performed in the beginning of the garbage collection cycles,
          slowing down the application program to an unwanted degree.
          This paper explores an approach to distributing the work more
          evenly over the garbage collection cycle.

   * Roger Henriksson.  1998.  “Scheduling Garbage Collection in
     Embedded Systems(73)”.  Department of Computer Science at Lund
     University.  Ph.D. thesis.

          Abstract: The complexity of systems for automatic control and
          other safety-critical applications grows rapidly.  Computer
          software represents an increasing part of the complexity.  As
          larger systems are developed, we need to find scalable
          techniques to manage the complexity in order to guarantee high
          product quality.  Memory management is a key quality factor
          for these systems.  Automatic memory management, or garbage
          collection, is a technique that significantly reduces the
          complex problem of correct memory management.  The risk of
          software errors decreases and development time is reduced.

          Garbage collection techniques suitable for interactive and
          soft real-time systems exist, but few approaches are suitable
          for systems with hard real-time requirements, such as control
          systems (embedded systems).  One part of the problem is solved
          by incremental garbage collection algorithms, which have been
          presented before.  We focus on the scheduling problem which
          forms the second part of the problem, i.e.  how the work of a
          garbage collector should be scheduled in order to disturb the
          application program as little as possible.  It is studied how
          a priori scheduling analysis of systems with automatic memory
          management can be made.  The field of garbage collection
          research is thus joined with the field of scheduling analysis
          in order to produce a practical synthesis of the two fields.

          A scheduling strategy is presented that employs the properties
          of control systems to ensure that no garbage collection work
          is performed during the execution of critical processes.  The
          hard real-time part of the system is thus never disturbed by
          garbage collection work.  Existing incremental garbage
          collection algorithms are adapted to the presented strategy.
          Necessary modifications of the algorithms and the real-time
          kernel are discussed.  A standard scheduling analysis
          technique, rate monotonic analysis, is extended in order to
          make a priori analysis of the schedulability of the garbage
          collector possible.

          The scheduling algorithm has been implemented in an
          industrially relevant real-time environment in order to show
          that the strategy is feasible in practice.  The experimental
          evaluation shows that predictable behaviour and
          sub-millisecond worst-case delays can be achieved on standard
          hardware even by a non-optimized prototype garbage collector.

   * Antony L. Hosking.  1991.  “Main memory management for
     persistence(74)”.  ACM. Proceedings of the ACM OOPSLA’91 Workshop
     on Garbage Collection.

          Abstract: Reachability-based persistence imposes new
          requirements for main memory management in general, and
          garbage collection in particular.  After a brief introduction
          to the characteristics and requirements of reachability-based
          persistence, we present the design of a run-time storage
          manager for Persistent Smalltalk and Persistent Modula-3,
          which allows the reclamation of storage from both temporary
          objects and buffered persistent objects.

   * Antony L. Hosking, J. Eliot B. Moss, Darko Stefanovic.  1992.  “A
     comparative performance evaluation of write barrier
     implementations(75)”.  ACM. OOPSLA’92 Conference Proceedings, ACM
     SIGPLAN Notices 27(10), pp 92–109.

          Abstract: Generational garbage collectors are able to achieve
          very small pause times by concentrating on the youngest (most
          recently allocated) objects when collecting, since objects
          have been observed to die young in many systems.  Generational
          collectors must keep track of all pointers from older to
          younger generations, by “monitoring” all stores into the heap.
          This 'write barrier' has been implemented in a number of ways,
          varying essentially in the granularity of the information
          observed and stored.  Here we examine a range of write barrier
          implementations and evaluate their relative performance within
          a generation scavenging garbage collector for Smalltalk.

   * Antony L. Hosking, Richard L. Hudson.  1993.  “Remembered sets can
     also play cards(76)”.  ACM. Proceedings of the ACM OOPSLA’93
     Workshop on Memory Management and Garbage Collection.

          Abstract: Remembered sets and dirty bits have been proposed as
          alternative implementations of the write barrier for garbage
          collection.  There are advantages to both approaches.  Dirty
          bits can be efficiently maintained with minimal, bounded
          overhead per store operation, while remembered sets concisely,
          and accurately record the necessary information.  Here we
          present evidence to show that hybrids can combine the virtues
          of both schemes and offer competitive performance.  Moreover,
          we argue that a hybrid can better avoid the devils that are
          the downfall of the separate alternatives.

   * Antony L. Hosking, J. Eliot B. Moss.  1993.  “Protection traps and
     alternatives for memory management of an object-oriented
     language(77)”.  ACM. Proceedings of the Fourteenth ACM Symposium on
     Operating Systems Principles, ACM Operating Systems Review 27(5),
     pp 106–119.

          Abstract: Many operating systems allow user programs to
          specify the protection level (inaccessible, read-only,
          read-write) of pages in their virtual memory address space,
          and to handle any protection violations that may occur.  Such
          page-protection techniques have been exploited by several
          user-level algorithms for applications including generational
          garbage collection and persistent stores.  Unfortunately,
          modern hardware has made efficient handling of page protection
          faults more difficult.  Moreover, page-sized granularity may
          not match the natural granularity of a given application.  In
          light of these problems, we reevaluate the usefulness of
          page-protection primitives in such applications, by comparing
          the performance of implementations that make use of the
          primitives with others that do not.  Our results show that for
          certain applications software solutions outperform solutions
          that rely on page-protection or other related virtual memory
          primitives.

   * Richard L. Hudson, J. Eliot B. Moss, Amer Diwan, Christopher F.
     Weight.  1991.  “A Language-Independent Garbage Collector
     Toolkit(78)”.  University of Massachusetts at Amherst.  COINS
     Technical Report 91–47.

          Abstract: We describe a memory management toolkit for language
          implementors.  It offers efficient and flexible generation
          scavenging garbage collection.  In addition to providing a
          core of language-independent algorithms and data structures,
          the toolkit includes auxiliary components that ease
          implementation of garbage collection for programming
          languages.  We have detailed designs for Smalltalk and
          Modula-3 and are confident the toolkit can be used with a wide
          variety of languages.  The toolkit approach is itself novel,
          and our design includes a number of additional innovations in
          flexibility, efficiency, accuracy, and cooperation between the
          compiler and the collector.

   * Richard L. Hudson, J. Eliot B. Moss.  1992.  “Incremental
     Collection of Mature Objects(79)”.  Springer-Verlag.  LNCS #637
     International Workshop on Memory Management, St.  Malo, France,
     Sept.  1992, pp.  388–403.

          Abstract: We present a garbage collection algorithm that
          extends generational scavenging to collect large older
          generations (mature objects) non-disruptively.  The
          algorithm’s approach is to process bounded-size pieces of
          mature object space at each collection; the subtleties lie in
          guaranteeing that it eventually collects any and all garbage.
          The algorithm does not assume any special hardware or
          operating system support, e.g., for forwarding pointers or
          protection traps.  The algorithm copies objects, so it
          naturally supports compaction and reclustering.

   * Richard L. Hudson, Ron Morrison, J. Eliot B. Moss, David S. Munro.
     1997.  “Garbage Collecting the World: One Car at a Time(80)”.  ACM.
     Proc.  OOPSLA 97, pp.  162–175.

          Abstract: A new garbage collection algorithm for distributed
          object systems, called DMOS (Distributed Mature Object Space),
          is presented.  It is derived from two previous algorithms, MOS
          (Mature Object Space), sometimes called the train algorithm,
          and PMOS (Persistent Mature Object Space).  The contribution
          of DMOS is that it provides the following unique combination
          of properties for a distributed collector: safety,
          completeness, non-disruptiveness, incrementality, and
          scalability.  Furthermore, the DMOS collector is non-blocking
          and does not use global tracing.

   * Mark S. Johnstone.  1997.  “Non-Compacting Memory Allocation and
     Real-Time Garbage Collection(81)”.  University of Texas at Austin.

          Abstract: Dynamic memory use has been widely recognized to
          have profound effects on program performance, and has been the
          topic of many research studies over the last forty years.  In
          spite of years of research, there is considerable confusion
          about the effects of dynamic memory allocation.  Worse, this
          confusion is often unrecognized, and memory allocators are
          widely thought to be fairly well understood.

          In this research, we attempt to clarify many issues for both
          manual and automatic non-moving memory management.  We show
          that the traditional approaches to studying dynamic memory
          allocation are unsound, and develop a sound methodology for
          studying this problem.  We present experimental evidence that
          fragmentation costs are much lower than previously recognized
          for most programs, and develop a framework for understanding
          these results and enabling further research in this area.  For
          a large class of programs using well-known allocation
          policies, we show that fragmentation costs are near zero.  We
          also study the locality effects of memory allocation on
          programs, a research area that has been almost completely
          ignored.  We show that these effects can be quite dramatic,
          and that the best allocation policies in terms of
          fragmentation are also among the best in terms of locality at
          both the cache and virtual memory levels of the memory
          hierarchy.

          We extend these fragmentation and locality results to
          real-time garbage collection.  We have developed a hard
          real-time, non-copying generational garbage collector which
          uses a write-barrier to coordinate collection work only with
          modifications of pointers, therefore making coordination costs
          cheaper and more predictable than previous approaches.  We
          combine this write-barrier approach with implicit non-copying
          reclamation, which has most of the advantages of copying
          collection (notably avoiding both the sweep phase required by
          mark-sweep collectors, and the referencing of garbage objects
          when reclaiming their space), without the disadvantage of
          having to actually copy the objects.  In addition, we present
          a model for non-copying implicit-reclamation garbage
          collection.  We use this model to compare and contrast our
          work with that of others, and to discuss the tradeoffs that
          must be made when developing such a garbage collector.

   * Mark S. Johnstone, Paul R. Wilson.  1998.  “The Memory
     Fragmentation Problem: Solved?(82)”.  ACM. ISMM’98 pp.  26–36.

          Abstract: We show that for 8 real and varied C and C++
          programs, several conventional dynamic storage allocators
          provide near-zero fragmentation, once overheads due to
          implementation details (headers, alignment, etc.)  are
          properly accounted for.  This substantially strengthens our
          previous results showing that the memory fragmentation problem
          has generally been misunderstood, and that good allocator
          policies can provide good memory usage for most programs.  The
          new results indicate that for most programs, excellent
          allocator policies are readily available, and efficiency of
          implementation is the major challenge.  While we believe that
          our experimental results are state-of-the-art and our
          methodology is superior to most previous work, more work
          should be done to identify and study unusual problematic
          program behaviors not represented in our sample.

   * Richard E. Jones.  1992.  “Tail recursion without space leaks(83)”.
     'Journal of Functional Programming.'  2(1):73–79.

          Abstract: The G-machine is a compiled graph reduction machine
          for lazy functional languages.  The G-machine compiler
          contains many optimisations to improve performance.  One set
          of such optimisations is designed to improve the performance
          of tail recursive functions.  Unfortunately the abstract
          machine is subject to a space leak–objects are unnecessarily
          preserved by the garbage collector.

          This paper analyses why a particular form of space leak occurs
          in the G-machine, and presents some ideas for fixing this
          problem.  This phenomena in other abstract machines is also
          examined briefly.

   * Richard E. Jones, Rafael Lins.  1992.  “Cyclic weighted reference
     counting without delay(84)”.  Computing Laboratory, The University
     of Kent at Canterbury.  Technical Report 28-92.

          Abstract: Weighted Reference Counting is a low-communication
          distributed storage reclamation scheme for loosely-coupled
          multiprocessors.  The algorithm we present herein extends
          weighted reference counting to allow the collection of cyclic
          data structures.  To do so, the algorithm identifies candidate
          objects that may be part of cycles and performs a tricolour
          mark-scan on their subgraph in a lazy manner to discover
          whether the subgraph is still in use.  The algorithm is
          concurrent in the sense that multiple useful computation
          processes and garbage collection processes can be performed
          simultaneously.

   * Richard E. Jones, Rafael Lins.  1996.  “Garbage Collection:
     Algorithms for Automatic Dynamic Memory Management(85)”.  Wiley.
     ISBN 0-471-94148-4.

          From the back cover: The memory storage requirements of
          complex programs are extremely difficult to manage correctly
          by hand.  A single error may lead to indeterminate and
          inexplicable program crashes.  Worse still, failures are often
          unrepeatable and may surface only long after the program has
          been delivered to the customer.  The eradication of memory
          errors typically consumes a substantial amount of development
          time.  And yet the answer is relatively easy – garbage
          collection; removing the clutter of memory management from
          module interfaces, which then frees the programmer to
          concentrate on the problem at hand rather than low-level
          book-keeping details.  For this reason, most modern
          object-oriented languages such as Smalltalk, Eiffel, Java and
          Dylan, are supported by garbage collection.  Garbage
          collecting libraries are even available for such uncooperative
          languages as C and C++.

          This book considers how dynamic memory can be recycled
          automatically to guarantee error-free memory management.
          There is an abundant but disparate literature on the subject,
          largely confined to research papers.  This book sets out to
          pool this experience in a single accessible and unified
          framework.

          Each of the important algorithms is explained in detail, often
          with illustrations of its characteristic features and
          animations of its use.  Techniques are described and compared
          for declarative and imperative programming styles, for
          sequential, concurrent and distributed architectures.

          For professionals developing programs from simple software
          tools to complex systems, as well as for researchers and
          students working in compiler construction, functional, logic
          and object-oriented programming design, this book will provide
          not only a clear introduction but also a convenient reference
          source for modern garbage collection techniques.

   * Richard E. Jones.  1998.  “ISMM'98 International Symposium on
     Memory Management(86)”.  ACM. ISBN 1-58113-114-3.

          From the Preface: The International Symposium on Memory
          Management is a forum for research in several related areas of
          memory management, especially garbage collectors and dynamic
          storage allocators.  […] The nineteen papers selected for
          publication in this volume cover a remarkably broad range of
          memory management topics from explicit malloc-style allocation
          to automatic memory management, from cache-conscious data
          layout to efficient management of distributed references, from
          conservative to type-accurate garbage collection, for
          applications ranging from user application to long-running
          servers, supporting languages as different as C, C++,
          Modula-3, Java, Eiffel, Erlang, Scheme, ML, Haskell and
          Prolog.

   * Richard E. Jones, Antony Hosking, and Eliot Moss.  2012.  “The
     Garbage Collection Handbook(87)”.  Chapman & Hall.

   * Ian Joyner.  1996.  “C++??: A Critique of C++(88).”.

          Abstract: The C++??  Critique is an analysis of some of the
          flaws of C++.  It is by no means exhaustive, nor does it
          attempt to document every little niggle with C++, rather
          concentrating on main themes.  The critique uses Java and
          Eiffel as comparisons to C++ to give a more concrete feel to
          the criticisms, viewing conceptual differences rather than
          syntactic ones as being more important.  Some C++ authors
          realising there are glaring deficiencies in C++ have chosen to
          defend C++ by also being critical within their own work.  Most
          notable are Bjarne Stroustup’s “Design and Evolution of C++,”
          and Scott Meyers’ “Effective” and “More Effective C++.” These
          warn of many traps and pitfalls, but reach the curious
          conclusion that since “good” C++ programmers are aware of
          these problems and know how to avoid them, C++ is alright.

          The C++ critique makes many of the same criticisms, but comes
          to the different conclusion that these pitfalls are not
          acceptable, and should not be in a language used for modern
          large scale software engineering.  Clean design is more
          important than after the fact warnings, and it is
          inconceivable that purchasers of end user software would
          tolerate this tactic on the part of vendors.  The critique
          also takes a look at C, and concludes that many of the
          features of C should be left out of modern languages, and that
          C is a flawed base for a language.

   * Bob Kanefsky.  1989.  “Recursive Memory Allocation(89)”.  Bob
     Kanefsky.  Songworm 3, p.?.

   * Jin-Soo Kim, Xiaohan Qin, Yarsun Hsu.  1998.  “Memory
     Characterization of a Parallel Data Mining Workload(90)”.  IEEE.
     Proc.  Workload Characterization: Methodology and Case Studies, pp.
     .

          Abstract: This paper studies a representative of an important
          class of emerging applications, a parallel data mining
          workload.  The application, extracted from the IBM Intelligent
          Miner, identifies groups of records that are mathematically
          similar based on a neural network model called self-organizing
          map.  We examine and compare in details two implementations of
          the application: (1) temporal locality or working set sizes;
          (2) spatial locality and memory block utilization;
          (3) communication characteristics and scalability; and (4) TLB
          performance.

          First, we find that the working set hierarchy of the
          application is governed by two parameters, namely the size of
          an input record and the size of prototype array; it is
          independent of the number of input records.  Second, the
          application shows good spatial locality, with the
          implementation optimized for sparse data sets having slightly
          worse spatial locality.  Third, due to the batch update
          scheme, the application bears very low communication.
          Finally, a 2-way set associative TLB may result in severely
          skewed TLB performance in a multiprocessor environment caused
          by the large discrepancy in the amount of conflict misses.
          Increasing the set associativity is more effective in
          mitigating the problem than increasing the TLB size.

   * Jin-Soo Kim & Yarsun Hsu.  2000.  “Memory system behavior of Java
     programs: methodology and analysis”.  ACM. Proc.  International
     conference on measurements and modeling of computer systems, pp.
     264–274.

          Abstract: This paper studies the memory system behavior of
          Java programs by analyzing memory reference traces of several
          SPECjvm98 applications running with a Just-In-Time (JIT)
          compiler.  Trace information is collected by an
          exception-based tracing tool called JTRACE, without any
          instrumentation to the Java programs or the JIT
          compiler.First, we find that the overall cache miss ratio is
          increased due to garbage collection, which suffers from higher
          cache misses compared to the application.  We also note that
          going beyond 2-way cache associativity improves the cache miss
          ratio marginally.  Second, we observe that Java programs
          generate a substantial amount of short-lived objects.
          However, the size of frequently-referenced long-lived objects
          is more important to the cache performance, because it tends
          to determine the application’s working set size.  Finally, we
          note that the default heap configuration which starts from a
          small initial heap size is very inefficient since it invokes a
          garbage collector frequently.  Although the direct costs of
          garbage collection decrease as we increase the available heap
          size, there exists an optimal heap size which minimizes the
          total execution time due to the interaction with the virtual
          memory performance.

   * Elliot K. Kolodner.  1992.  “Atomic Incremental Garbage Collection
     and Recovery for a Large Stable Heap”.  Laboratory for Computer
     Science at MIT. MIT-LCS-TR-534.

          Abstract: A stable heap is a storage that is managed
          automatically using garbage collection, manipulated using
          atomic transactions, and accessed using a uniform storage
          model.  These features enhance reliability and simplify
          programming by preventing errors due to explicit deallocation,
          by masking failures and concurrency using transactions, and by
          eliminating the distinction between accessing temporary
          storage and permanent storage.  Stable heap management is
          useful for programming language for reliable distributed
          computing, programming languages with persistent storage, and
          object-oriented database systems.  Many applications that
          could benefit from a stable heap (e.g., computer-aided design,
          computer-aided software engineering, and office information
          systems) require large amounts of storage, timely responses
          for transactions, and high availability.  We present garbage
          collection and recovery algorithms for a stable heap
          implementation that meet these goals and are appropriate for
          stock hardware.  The collector is incremental: it does not
          attempt to collect the whole heap at once.  The collector is
          also atomic: it is coordinated with the recovery system to
          prevent problems when it moves and modifies objects .  The
          time for recovery is independent of heap size, and can be
          shortened using checkpoints.

   * Per-Åke Larson & Murali Krishnan.  1998.  “Memory Allocation for
     Long-Running Server Applications(91)”.  ACM. ISMM’98 pp.  176–185.

          Abstract: Prior work on dynamic memory allocation has largely
          neglected long-running server applications, for example, web
          servers and mail servers.  Their requirements differ from
          those of one-shot applications like compilers or text editors.
          We investigated how to build an allocator that is not only
          fast and memory efficient but also scales well on SMP
          machines.  We found that it is not sufficient to focus on
          reducing lock contention.  Only limited improvement can be
          achieved this way; higher speedups require a reduction in
          cache misses and cache invalidation traffic.  We then designed
          and prototyped a new allocator, called Lkmalloc, targeted for
          both traditional applications and server applications.
          LKmalloc uses several subheaps, each one with a separate set
          of free lists and memory arena.  A thread always allocates
          from the same subheap but can free a block belonging to any
          subheap.  A thread is assigned to a subheap by hashing on its
          thread ID. We compared its performance with several other
          allocators on a server-like, simulated workload and found that
          it indeed scales well and is quite fast but could use memory
          more efficiently.

   * Henry Lieberman & Carl Hewitt.  1983.  “A real-time garbage
     collector based on the lifetimes of objects(92)”.  ACM.
     26(6):419–429.

          Abstract: In previous heap storage systems, the cost of
          creating objects and garbage collection is independent of the
          lifetime of the object.  Since objects with short lifetimes
          account for a large portion of storage use, it is worth
          optimizing a garbage collector to reclaim storage for these
          objects more quickly.  The garbage collector should spend
          proportionately less effort reclaiming objects with longer
          lifetimes.  We present a garbage collection algorithm that
          (1) makes storage for short-lived objects cheaper than storage
          for long-lived objects, (2) that operates in real-time–object
          creation and access times are bounded, (3) increases locality
          of reference, for better virtual memory performance, (4) works
          well with multiple processors and a large address space.

   * J. McCarthy, M. L. Minsky.  1959.  “Artificial Intelligence,
     Quarterly Progress Report no.  53(93)”.  Research Laboratory of
     Electronics at MIT.

   * J. McCarthy.  1960.  “Recursive Functions of Symbolic Expressions
     and Their Computation by Machine(94)”.  CACM.

          Abstract: A programming system called LISP (for LISt
          Processor) has been developed for the IBM 704 computer by the
          Artificial Intelligence group at M.I.T. The system was
          designed to facilitate experiments with a proposed system
          called the Advice Taker, whereby a machine could be instructed
          to handle declarative as well as imperative sentences and
          could exhibit “common sense” in carrying out its instructions.
          The original proposal for the Advice Taker was made in
          November 1958.  The main requirement was a programming system
          for manipulating expressions representing formalized
          declarative and imperative sentences so that the Advice Taker
          could make deductions.

          In the course of its development the LISP system went through
          several stages of simplification and eventually came to be
          based on a scheme for representing the partial recursive
          functions of a certain class of symbolic expressions.  This
          representation is independent of the IBM 704 computer, or of
          any other electronic computer, and it now seems expedient to
          expound the system by starting with the class of expressions
          called S-expressions and the functions called S-functions.

   * John McCarthy.  1979.  “History of Lisp(95)”.  In 'History of
     programming languages I', pp.  173–185.  ACM.

   * Veljko Milutinovic, Jelica Protic, Milo Tomasevic.  1997.
     “Distributed shared memory: concepts and systems(96)”.  IEEE
     Computer Society Press.  ISBN 0-8186-7737-6.

          From the publisher’s catalog: Presents a survey of both
          distributed shared memory (DSM) efforts and commercial DSM
          systems.  The book discusses relevant issues that make the
          concept of DSM one of the most attractive approaches for
          building large-scale, high-performance multiprocessor systems.
          Its text provides a general introduction to the DSM field as
          well as a broad survey of the basic DSM concepts, mechanisms,
          design issues, and systems.

          Distributed Shared Memory concentrates on basic DSM
          algorithms, their enhancements, and their performance
          evaluation.  In addition, it details implementations that
          employ DSM solutions at the software and the hardware level.
          The book is a research and development reference that provides
          state-of-the art information that will be useful to
          architects, designers, and programmers of DSM systems.

   * M. L. Minsky.  1963.  “A LISP Garbage Collector Algorithm Using
     Serial Secondary Storage(97)”.  MIT. Memorandum MAC-M-129,
     Artificial Intelligence Project, Memo 58 (revised).

          Abstract: This paper presents an algorithm for reclaiming
          unused free storage memory cells is LISP. It depends on
          availability of a fast secondary storage device, or a large
          block of available temporary storage.  For this price, we get
          1. Packing of free-storage into a solidly packed block.
          2. Smooth packing of arbitrary linear blocks and arrays.
          3. The collector will handle arbitrarily complex re-entrant
          list structure with no introduction of spurious copies.
          4. The algorithm is quite efficient; the marking pass visits
          words at most twice and usually once, and the loading pass is
          linear.  5. The system is easily modified to allow for
          increase in size of already fixed consecutive blocks, provide
          one can afford to initiate a collection pass or use a modified
          array while waiting for such a pass to occur.

   * David Moon.  1984.  “Garbage Collection in a Large Lisp
     System(98)”.  ACM. Symposium on Lisp and Functional Programming,
     August 1984.

          Abstract: This paper discusses garbage collection techniques
          used in a high-performance Lisp implementation with a large
          virtual memory, the Symbolics 3600.  Particular attention is
          paid to practical issues and experience.  In a large system
          problems of scale appear and the most straightforward
          garbage-collection techniques do not work well.  Many of these
          problems involve the interaction of the garbage collector with
          demand-paged virtual memory.  Some of the solutions adopted in
          the 3600 are presented, including incremental copying garbage
          collection, approximately depth-first copying, ephemeral
          objects, tagged architecture, and hardware assists.  We
          discuss techniques for improving the efficiency of garbage
          collection by recognizing that objects in the Lisp world have
          a variety of lifetimes.  The importance of designing the
          architecture and the hardware to facilitate garbage collection
          is stressed.

   * David Moon.  1985.  “Architecture of the Symbolics 3600”.  IEEE.
     12th International Symposium on Computer Architecture, pp.  76–83.

   * David Moon.  1990.  “Symbolics Architecture”.  Wiley.  Chapter 3 of
     'Computers for Artificial Intelligence Processing', ISBN
     0-471-84811-5.

   * David Moon.  1991.  “Genera Retrospective”.  IEEE. 1991
     International Workshop on Object Orientation in Operating Systems,
     order #2265.

   * Ben-Ari Mordechai.  1984.  “Algorithms for On-the-fly Garbage
     Collection”.  'TOPLAS' 6(3): 333–344 (1984).

   * Luc Moreau.  1998.  “Hierarchical Distributed Reference
     Counting(99)”.  ACM. ISMM’98 pp.  57–67.

          Abstract: Massively distributed computing is a challenging
          problem for garbage collection algorithm designers as it
          raises the issue of scalability.  The high number of hosts
          involved in a computation can require large tables for
          reference listing, whereas the lack of information sharing
          between hosts in a same locality can entail redundant GC
          traffic.  In this paper, we argue that a conceptual
          hierarchical organisation of massive distributed computations
          can solve this problem.  By conceptual hierarchical
          organisation, we mean that processors are still able to
          communicate in a peer to peer manner using their usual
          communication mechanism, but GC messages will be routed as if
          processors were organised in hierarchy.  We present an
          extension of a distributed reference counting algorithm that
          uses such a hierarchical organisation.  It allows us to bound
          table sizes by the number of hosts in a domain, and it allows
          us to share GC information between hosts in a same locality in
          order to reduce cross-network GC traffic.

   * Greg Morrisett, Matthias Felleisen, Robert Harper.  1995.
     “Abstract Models of Memory Management(100)”.  Carnegie Mellon
     University.  CMU-CS-FOX-95-01.

          Abstract: Most specifications of garbage collectors
          concentrate on the low-level algorithmic details of how to
          find and preserve accessible objects.  Often, they focus on
          bit-level manipulations such as “scanning stack frames,”
          “marking objects,” “tagging data,” etc.  While these details
          are important in some contexts, they often obscure the more
          fundamental aspects of memory management: what objects are
          garbage and why?

          We develop a series of calculi that are just low-level enough
          that we can express allocation and garbage collection, yet are
          sufficiently abstract that we may formally prove the
          correctness of various memory management strategies.  By
          making the heap of a program syntactically apparent, we can
          specify memory actions as rewriting rules that allocate values
          on the heap and automatically dereference pointers to such
          objects when needed.  This formulation permits the
          specification of garbage collection as a relation that removes
          portions of the heap without affecting the outcome of
          evaluation.

          Our high-level approach allows us to specify in a compact
          manner a wide variety of memory management techniques,
          including standard trace-based garbage collection (i.e., the
          family of copying and mark/sweep collection algorithms),
          generational collection, and type-based, tag-free collection.
          Furthermore, since the definition of garbage is based on the
          semantics of the underlying language instead of the
          conservative approximation of inaccessibility, we are able to
          specify and prove the idea that type inference can be used to
          collect some objects that are accessible but never used.

   * David S. Munro, Alfred Brown, Ron Morrison, J. Eliot B. Moss.
     1999.  “Incremental Garbage Collection of a Persistent Object Store
     using PMOS(101)”.  Morgan Kaufmann.  in Advances in Persistent
     Object Systems, pp.  78–91.

          Abstract: PMOS is an incremental garbage collector designed
          specifically to reclaim space in a persistent object store.
          It is complete in that it will, after a finite number of
          invocations, reclaim all unreachable storage.  PMOS imposes
          minimum constraints on the order of collection and offers
          techniques to reduce the I/O traffic induced by the collector.
          Here we present the first implementation of the PMOS collector
          called PMOS#1.  The collector has been incorporated into the
          stable heap layer of the generic persistent object store used
          to support a number of languages including Napier88.  Our main
          design goals are to maintain the independence of the language
          from the store and to retain the existing store interface.
          The implementation has been completed and tested using a
          Napier88 system.  The main results of this work show that the
          PMOS collector is implementable in a persistent store and that
          it can be built without requiring changes to the language
          interpreter.  Initial performance measurements are reported.
          These results suggest however, that effective use of PMOS
          requires greater co-operation between language and store.

   * Scott Nettles, James O’Toole, David Pierce, Nickolas Haines.  1992.
     “Replication-Based Incremental Copying Collection(102)”.  IWMM’92.

          Abstract: We introduce a new replication-based copying garbage
          collection technique.  We have implemented one simple
          variation of this method to provide incremental garbage
          collection on stock hardware with no special operating system
          or virtual memory support.  The performance of the prototype
          implementation is excellent: major garbage collection pauses
          are completely eliminated with only a slight increase in minor
          collection pause times.

          Unlike the standard copying algorithm, the replication-based
          method does not destroy the original replica when a copy is
          created.  Instead, multiple copies may exist, and various
          standard strategies for maintaining consistency may be
          applied.  In our implementation for Standard ML of New Jersey,
          the mutator continues to use the from-space replicas until the
          collector has achieved a consistent replica of all live data
          in to-space.

          We present a design for a concurrent garbage collector using
          the replication-based technique.  We also expect
          replication-based GC methods to be useful in providing
          services for persistence and distribution, and briefly discuss
          these possibilities.

   * Scott Nettles.  1992.  “A Larch Specification of Copying Garbage
     Collection(103)”.  Carnegie Mellon University.  CMU-CS-92-219.

          Abstract: Garbage collection (GC) is an important part of many
          language implementations.  One of the most important garbage
          collection techniques is copying GC. This paper consists of an
          informal but abstract description of copying collection, a
          formal specification of copying collection written in the
          Larch Shared Language and the Larch/C Interface Language, a
          simple implementation of a copying collector written in C, an
          informal proof that the implementation satisfies the
          specification, and a discussion of how the specification
          applies to other types of copying GC such as generational
          copying collectors.  Limited familiarity with copying GC or
          Larch is needed to read the specification.

   * Scott Nettles & James O’Toole.  1993.  “Implementing Orthogonal
     Persistence: A Simple Optimization Using Replicating Collection”.
     USENIX. IWOOOS’93.

          Abstract: Orthogonal persistence provides a safe and
          convenient model of object persistence.  We have implemented a
          transaction system which supports orthogonal persistence in a
          garbage-collected heap.  In our system, replicating collection
          provides efficient concurrent garbage collection of the heap.
          In this paper, we show how replicating garbage collection can
          also be used to reduce commit operation latencies in our
          implementation.

          We describe how our system implements transaction commit.  We
          explain why the presence of non-persistent objects can add to
          the cost of this operation.  We show how to eliminate these
          additional costs by using replicating garbage collection.  The
          resulting implementation of orthogonal persistence should
          provide transaction performance that is independent of the
          quantity of non-persistent data in use.  We expect efficient
          support for orthogonal persistence to be valuable in operating
          systems applications which use persistent data.

   * Scott Nettles & James O’Toole.  1993.  “Real-Time Replication
     Garbage Collection(104)”.  ACM. PLDI’93.

          Abstract: We have implemented the first copying garbage
          collector that permits continuous unimpeded mutator access to
          the original objects during copying.  The garbage collector
          incrementally replicates all accessible objects and uses a
          mutation log to bring the replicas up-to-date with changes
          made by the mutator.  An experimental implementation
          demonstrates that the costs of using our algorithm are small
          and that bounded pause times of 50 milliseconds can be readily
          achieved.

   * Norman R. Nielsen.  1977.  “Dynamic Memory Allocation in Computer
     Simulation”.  ACM. CACM 20:11.

          Abstract: This paper investigates the performance of 35
          dynamic memory allocation algorithms when used to service
          simulation programs as represented by 18 test cases.
          Algorithm performance was measured in terms of processing
          time, memory usage, and external memory fragmentation.
          Algorithms maintaining separate free space lists for each size
          of memory block used tended to perform quite well compared
          with other algorithms.  Simple algorithms operating on memory
          ordered lists (without any free list) performed surprisingly
          well.  Algorithms employing power-of-two block sizes had
          favorable processing requirements but generally unfavorable
          memory usage.  Algorithms employing LIFO, FIFO, or memory
          ordered free lists generally performed poorly compared with
          others.

   * James O’Toole.  1990.  “Garbage Collecting Locally”.

          Abstract: Generational garbage collection is a simple
          technique for automatic partial memory reclamation.  In this
          paper, I present the basic mechanics of generational
          collection and discuss its characteristics.  I compare several
          published algorithms and argue that fundamental considerations
          of locality, as reflected in the changing relative speeds of
          processors, memories, and disks, strongly favor a focus on
          explicit optimization of I/O requirements during garbage
          collection.  I show that this focus on I/O costs due to memory
          hierarchy debunks a well-known claim about the relative costs
          of garbage collection and stack allocation.  I suggest two
          directions for future research in this area and discuss some
          simple architectural changes in virtual memory interfaces
          which may enable efficient garbage collector utilization of
          standard virtual memory hardware.

   * James O’Toole & Scott Nettles.  1994.  “Concurrent Replicating
     Garbage Collection(105)”.  ACM. LFP’94.

          Abstract: We have implemented a concurrent copying garbage
          collector that uses replicating garbage collection.  In our
          design, the client can continuously access the heap during
          garbage collection.  No low-level synchronization between the
          client and the garbage collector is required on individual
          object operations.  The garbage collector replicates live heap
          objects and periodically synchronizes with the client to
          obtain the client’s current root set and mutation log.  An
          experimental implementation using the Standard ML of New
          Jersey system on a shared-memory multiprocessor demonstrates
          excellent pause time performance and moderate execution time
          speedups.

   * Simon Peyton Jones, Norman Ramsey, Fermin Reig.  1999.  “C-: a
     portable assembly language that supports garbage collection(106)”.
     Springer-Verlag.  International Conference on Principles and
     Practice of Declarative Programming 1999, LNCS 1702, pp.  1–28.

          Abstract: For a compiler writer, generating good machine code
          for a variety of platforms is hard work.  One might try to
          reuse a retargetable code generator, but code generators are
          complex and difficult to use, and they limit one’s choice of
          implementation language.  One might try to use C as a portable
          assembly language, but C limits the compiler writer’s
          flexibility and the performance of the resulting code.  The
          wide use of C, despite these drawbacks, argues for a portable
          assembly language.  C– is a new language designed expressly
          for this purpose.  The use of a portable assembly language
          introduces new problems in the support of such high-level
          run-time services as garbage collection, exception handling,
          concurrency, profiling, and debugging.  We address these
          problems by combining the C– language with a C– run-time
          interface.  The combination is designed to allow the compiler
          writer a choice of source-language semantics and
          implementation techniques, while still providing good
          performance.

   * John S. Pieper.  1993.  “Compiler Techniques for Managing Data
     Motion”.  Carnegie Mellon University.  Technical report number
     CMU-CS-93-217.

          Abstract: Software caching, automatic algorithm blocking, and
          data overlays are different names for the same problem:
          compiler management of data movement throughout the memory
          hierarchy.  Modern high-performance architectures often omit
          hardware support for moving data between levels of the memory
          hierarchy: iWarp does not include a data cache, and Cray
          supercomputers do not have virtual memory.  These systems have
          effectively traded a more complicated programming model for
          performance by replacing a hardware-controlled memory
          hierarchy with a simple fast memory.  The simpler memories
          have less logic in the critical path, so the cycle time of the
          memories is improved.

          For programs which fit in the resulting memory, the extra
          performance is great.  Unfortunately, the driving force behind
          supercomputing today is a class of very large scientific
          problems, both in terms of computation time and in terms of
          the amount of data used.  Many of these programs do not fit in
          the memory of the machines available.  When architects trade
          hardware support for data migration to gain performance,
          control of the memory hierarchy is left to the programmer.
          Either the program size must be cut down to fit into the
          machine, or every loop which accesses more data than will fit
          into memory must be restructured by hand.  This thesis
          describes how a compiler can relieve the programmer of this
          burden, and automate data motion throughout the memory
          hierarchy without direct hardware support.

          This works develops a model of how data is accessed within a
          nested loop by typical scientific programs.  It describes
          techniques which can be used by compilers faced with the task
          of managing data motion.  The concentration is on nested loops
          which process large data arrays using linear array subscripts.
          Because the array subscripts are linear functions of the loop
          indices and the loop indices form an integer lattice, linear
          algebra can be applied to solve many compilation problems.

          The approach it to tile the iteration space of the loop nest.
          Tiling allows the compiler to improve locality of reference.
          The tiling basis matrix is chosen from a set of candidate
          vectors which neatly divide the data set.  The execution order
          of the tiles is selected to maximize locality between tiles.
          Finally, the tile sizes are chosen to minimize execution time.

          The approach has been applied to several common scientific
          loop nests: matrix-matrix multiplication, QR-decomposition,
          and LU-decomposition.  In addition, an illustrative example
          from the Livermore Loop benchmark set is examined.  Although
          more compiler time can be required in some cases, this
          technique produces better code at no cost for most programs.

   * Pekka P. Pirinen.  1998.  “Barrier techniques for incremental
     tracing”.  ACM. ISMM’98 pp.  20–25.

          Abstract: This paper presents a classification of barrier
          techniques for interleaving tracing with mutator operation
          during an incremental garbage collection.  The two useful
          tricolour invariants are derived from more elementary
          considerations of graph traversal.  Barrier techniques for
          maintaining these invariants are classified according to the
          action taken at the barrier (such as scanning an object or
          changing its colour), and it is shown that the algorithms
          described in the literature cover all the possibilities except
          one.  Unfortunately, the new technique is impractical.  Ways
          of combining barrier techniques are also discussed.

   * Tony Printezis.  1996.  “Disk Garbage Collection Strategies for
     Persistent Java”.  Proceedings of the First International Workshop
     on Persistence and Java.

          Abstract: This paper presents work currently in progress on
          Disk Garbage Collection issues for PJava, an orthogonally
          persistent version of Java.  In particular, it concentrates on
          the initial Prototype of the Disk Garbage Collector of PJava0
          which has already been implemented.  This Prototype was
          designed to be very simple and modular in order to be easily
          changed, evolved, improved, and allow experimentation.
          Several experiments were performed in order to test possible
          optimisations; these experiments concentrated on the following
          four areas: a) efficient access to the store; b)
          page-replacement algorithms; c) efficient discovery of live
          objects during compaction; and d) dealing with forward
          references.  The paper presents a description of the
          Prototype’s architecture, the results of these experiments and
          related discussion, and some future directions based on the
          experience gained from this work.

   * Tony Printezis & Quentin Cutts.  1996.  “Measuring the Allocation
     Rate of Napier88”.  Department of Computing Science at University
     of Glasgow.  TR ?.

   * M. B. Reinhold.  1993.  “Cache Performance of Garbage Collected
     Programming Languages(107)”.  Laboratory for Computer Science at
     MIT. MIT/LCS/TR-581.

          Abstract: As processor speeds continue to improve relative to
          main-memory access times, cache performance is becoming an
          increasingly important component of program performance.
          Prior work on the cache performance of garbage-collected
          programming languages has either assumed or argued that
          conventional garbage-collection methods will yield poor
          performance, and has therefore concentrated on new collection
          algorithms designed specifically to improve cache-level
          reference locality.  This dissertation argues to the contrary:
          Many programs written in garbage-collected languages are
          naturally well-suited to the direct-mapped caches typically
          found in modern computer systems.

          Using a trace-driven cache simulator and other analysis tools,
          five nontrivial, long-running Scheme programs are studied.  A
          control experiment shows that the programs have excellent
          cache performance without any garbage collection at all.  A
          second experiment indicates that the programs will perform
          well with a simple and infrequently-run generational
          compacting collector.

          An analysis of the test programs’ memory usage patterns
          reveals that the mostly-functional programming style typically
          used in Scheme programs, in combination with simple linear
          storage allocation, causes most data objects to be dispersed
          in time and space so that references to them cause little
          cache interference.  From this it follows that other Scheme
          programs, and programs written in similar styles in different
          languages, should perform well with a simple generational
          compacting collector; sophisticated collectors intended to
          improve cache performance are unlikely to be effective.  The
          analysis also suggests that, as locality becomes ever more
          important to program performance, programs written in
          garbage-collected languages may turn out to have significant
          performance advantage over programs written in more
          conventional languages.

   * J. M. Robson.  1977.  “Worst case fragmentation of first fit and
     best fit storage allocation strategies”.  ACM. ACM Computer
     Journal, 20(3):242–244.

   * Gustavo Rodriguez-Rivera & Vince Russo.  1997.  “Non-intrusive
     Cloning Garbage Collection with Stock Operating System Support”.
     Software – Practice and Experience.  27:8.

          Abstract: It is well accepted that automatic garbage
          collection simplifies programming, promotes modularity, and
          reduces development effort.  However it is commonly believed
          that these advantages do not counteract the perceived price:
          excessive overheads, possible long pause times while garbage
          collections occur, and the need to modify existing code.  Even
          though there are publically available garbage collector
          implementations that can be used in existing programs, they do
          not guarantee short pauses, and some modification of the
          application using them is still required.  In this paper we
          describe a snapshot-at-beginning concurrent garbage collector
          algorithm and its implementation.  This algorithm guarantees
          short pauses, and can be easily implemented on stock UNIX-like
          operating systems.  Our results show that our collector
          performs comparable to other garbage collection
          implementations on uniprocessor machines and outperforms
          similar collectors on multiprocessor machines.  We also show
          our collector to be competitive in performance with explicit
          deallocation.  Our collector has the added advantage of being
          non-intrusive.  Using a dynamic linking technique and
          effective root set inferencing, we have been able to
          successfully run our collector even in commercial programs
          where only the binary executable and no source code is
          available.  In this paper we describe our algorithm, its
          implementation, and provide both an algorithmic and a
          performance comparison between our collector and other similar
          garbage collectors.

   * Niklas Röjemo.  1995.  “Highlights from nhc – a space-efficient
     Haskell compiler”.  Chalmers University of Technology.

          Abstract: Self-compiling implementations of Haskell, i.e.,
          those written in Haskell, have been and, except one, are still
          space consuming monsters.  Object code size for the compilers
          themselves are 3-8Mb, and they need 12-20Mb to recompile
          themselves.  One reason for the huge demands for memory is
          that the main goal for these compilers is to produce fast
          code.  However, the compiler described in this paper, called
          “nhc” for “Nearly a Haskell Compiler”, is the one above
          mentioned exception.  This compiler concentrates on keeping
          memory usage down, even at a cost in time.  The code produced
          is not fast, but nhc is usable, and the resulting programs can
          be run on computers with small memory.

          This paper describes some of the implementation choices done,
          in the Haskell part of the source code, to reduce memory
          consumption in nhc.  It is possible to use these also in other
          Haskell compilers with no, or very small, changes to their
          run-time systems.

          Time is neither the main focus of nhc nor of this paper, but
          there is nevertheless a small section about the speed of nhc.
          The most notable observation concerning speed is that nhc
          spends approximately half the time processing interface files,
          which is much more than needed in the type checker.
          Processing interface files is also the most space consuming
          part of nhc in most cases.  It is only when compiling source
          files with large sets of mutually recursive functions that
          more memory is needed to type check than to process interface
          files.

   * Niklas Röjemo.  1995.  “Generational garbage collection for lazy
     functional languages without temporary space leaks”.  Chalmers
     University of Technology.

          Abstract: Generational garbage collection is an established
          method for creating efficient garbage collectors.  Even a
          simple implementation where all nodes that survive one garbage
          collection are 'tenured', i.e., moved to an old generation,
          works well in strict languages.  In lazy languages, however,
          such an implementation can create severe 'temporary space
          leaks'.  The temporary space leaks appear in programs that
          traverse large lazily built data structures, e.g., a lazy list
          representing a large file, where only a small part is needed
          at any time.  A simple generational garbage collector cannot
          reclaim the memory, used by the lazily built list, at minor
          collections.  The reason is that at least one of the nodes in
          the list belongs to the old generation, after the first minor
          collection, and will hold on to the rest of the nodes in the
          list until the next major collection.

   * Niklas Röjemo & Colin Runciman.  1996.  “Lag, drag, void and use –
     heap profiling and space-efficient compilation revisited”.  ACM,
     SIGPLAN. ICFP’96, ACM SIGPLAN Notices 31:6, ISBN 0-89791-770-7, pp.
     34–41.

          Abstract: The context for this paper is functional computation
          by graph reduction.  Our overall aim is more efficient use of
          memory.  The specific topic is the detection of dormant cells
          in the live graph – those retained in heap memory though not
          actually playing a useful role in computation.  We describe a
          profiler that can identify heap consumption by such ‘useless’
          cells.  Unlike heap profilers based on traversals of the live
          heap, this profiler works by examining cells post-mortem.  The
          new profiler has revealed a surprisingly large proportion of
          ‘useless’ cells, even in some programs that previously seemed
          space-efficient such as the bootstrapping Haskell compiler
          “nhc”.

   * David J. Roth, David S. Wise.  1999.  “One-bit counts between
     unique and sticky(108)”.  ACM. ISMM’98, pp.  49–56.

          Abstract: Stoye’s one-bit reference tagging scheme can be
          extended to local counts of two or more via two strategies.
          The first, suited to pure register transactions, is a cache of
          referents to two shared references.  The analog of Deutch’s
          and Bobrow’s multiple-reference table, this cache is
          sufficient to manage small counts across successive assignment
          statements.  Thus, accurate reference counts above one can be
          tracked for short intervals, like that bridging one function’s
          environment to its successor’s.

          The second, motivated by runtime stacks that duplicate
          references, avoids counting any references from the stack.  It
          requires a local pointer-inversion protocol in the mutator,
          but one still local to the referent and the stack frame.
          Thus, an accurate reference count of one can be maintained
          regardless of references from the recursion stack.

   * Paul Rovner.  1985.  “On Adding Garbage Collection and Runtime
     Types to a Strongly-Typed, Statically-Checked, Concurrent
     Language(109)”.  Xerox PARC. TR CSL-84-7.

          Abstract: Enough is known now about garbage collection,
          runtime types, strong-typing, static-checking and concurrency
          that it is possible to explore what happens when they are
          combined in a real programming system.

          Storage management is one of a few central issues through
          which one can get a good view of the design of an entire
          system.  Tensions between ease of integration and the need for
          protection; between generality, simplicity, flexibility,
          extensibility and efficiency are all manifest when assumptions
          and attitudes about managing storage are studied.  And deep
          understanding follows best from the analysis of systems that
          people use to get real work done.

          This paper is not for those who seek arguments pro or con
          about the need for these features in programming systems; such
          issues are for other papers.  This one assumes these features
          to be good and describes how they combine and interact in
          Cedar, a programming language and environment designed to help
          programmers build moderate-sized experimental systems for
          moderate numbers of people to test and use.

   * Colin Runciman & David Wakeling.  1992.  “Heap Profiling of Lazy
     Functional Programs(110)”.  University of York.

          Abstract: We describe the design, implementation, and use of a
          new kind of profiling tool that yields valuable information
          about the memory use of lazy functional programs.  The tool
          has two parts: a modified functional language implementation
          which generated profiling implementation during the execution
          of programs, and a separate program which converts this
          information to graphical form.  With the aid of profile
          graphs, one can make alterations to a functional program which
          dramatically reduce its space consumption.  We demonstrate
          that this is the case of a genuine example – the first to
          which the tool has been applied – for which the results are
          strikingly successful.

   * Colin Runciman & Niklas Röjemo.  1994.  “New dimensions in heap
     profiling(111)”.  University of York.

          Abstract: First-generation heap profilers for lazy functional
          languages have proved to be effective tools for locating some
          kinds of space faults, but in other cases they cannot provide
          sufficient information to solve the problem.  This paper
          describes the design, implementation and use of a new profiler
          that goes beyond the two-dimensional “who produces what” view
          of heap cells to provide information about their more dynamic
          and structural attributes.  Specifically, the new profiler can
          distinguish between cells according to their 'eventual
          lifetime', or on the basis of the 'closure retainers' by
          virtue of which they remain part of the live heap.  A
          bootstrapping Haskell compiler (nhc) hosts the implementation:
          among examples of the profiler’s use we include
          self-application to nhc.  Another example is the original
          heap-profiling case study “clausify”, which now consumes even
          less memory and is much faster.

   * Colin Runciman & Niklas Röjemo.  1996.  “Two-pass heap profiling: a
     matter of life and death”.  Department of Computer Science,
     University of York.

          Abstract: A heap profile is a chart showing the contents of
          heap memory throughout a computation.  Contents are depicted
          abstractly by showing how much space is occupied by memory
          cells in each of several classes.  A good heap profiler can
          use a variety of attributes of memory cells to de-fine a
          classification.  Effective profiling usually involves a
          combination of attributes.  The ideal profiler gives full
          support for combination in two ways.  First, a section of the
          heap of interest to the programmer can be specified by
          constraining the values of any combination of cell attributes.
          Secondly, no matter what attributes are used to specify such a
          section, a heap profile can be obtained for that section only,
          and any other attribute can be used to define the
          classification.

          Achieving this ideal is not simple For some combinations of
          attributes.  A heap profile is derived by interpolation of a
          series of censuses of heap contents at different stages.  The
          obvious way to obtain census data is to traverse the live heap
          at intervals throughout the computation.  This is fine for
          static attributes (e.g.  What type of value does this memory
          cell represent?), and for dynamic attributes that can be
          determined for each cell by examining the heap at any given
          moment (e.g.  From which function closures can this cell be
          reached?).  But some attributes of cells can only be
          determined retrospectively by post-mortem inspection asa cell
          is overwritten or garbage-collected (e.g.  Is this cell ever
          used again?).  Now we see the problem: if a profiler supports
          both live and pose-mortem attributes, how can we implement the
          ideal of unrestricted combinations?  That is the problem me
          solve in this paper.  We give techniques for profiling a.
          heap section specified in terms of both live and post-mortem
          attributes.  We show how to generate live-attribute profiles
          of a section of the heal, specified using post-mortem
          attributes, and vice versa.

   * Jacob Seligmann & Steffen Grarup.  1995.  “Incremental Mature
     Garbage Collection Using the Train Algorithm(112)”.
     Springer-Verlag.  ECOOP’95, Lecture Notes in Computer Science, Vol.
     952, pp.  235–252, ISBN 3-540-60160-0.

          Abstract: We present an implementation of the Train Algorithm,
          an incremental collection scheme for reclamation of mature
          garbage in generation-based memory management systems.  To the
          best of our knowledge, this is the first Train Algorithm
          implementation ever.  Using the algorithm, the traditional
          mark-sweep garbage collector employed by the Mj&oslash;lner
          run-time system for the object-oriented BETA programming
          language was replaced by a non-disruptive one, with only
          negligible time and storage overheads.

   * Manuel Serrano, Hans-J. Boehm.  2000.  “Understanding memory
     allocation of Scheme programs(113)”.  ACM. Proceedings of
     International Conference on Functional Programming 2000.

          Abstract: Memory is the performance bottleneck of modern
          architectures.  Keeping memory consumption as low as possible
          enables fast and unobtrusive applications.  But it is not easy
          to estimate the memory use of programs implemented in
          functional languages, due to both the complex translations of
          some high level constructs, and the use of automatic memory
          managers.  To help understand memory allocation behavior of
          Scheme programs, we have designed two complementary tools.
          The first one reports on frequency of allocation, heap
          configurations and on memory reclamation.  The second tracks
          down memory leaks.  We have applied these tools to our Scheme
          compiler, the largest Scheme program we have been developing.
          This has allowed us to drastically reduce the amount of memory
          consumed during its bootstrap process, without requiring much
          development time.  Development tools will be neglected unless
          they are both conveniently accessible and easy to use.  In
          order to avoid this pitfall, we have carefully designed the
          user interface of these two tools.  Their integration into a
          real programming environment for Scheme is detailed in the
          paper.

   * Marc Shapiro & Paulo Ferreira.  1994.  “Larchant-RDOSS: a
     distributed shared persistent memory and its garbage
     collector(114)”.  INRIA. INRIA Rapport de Recherche no.  2399;
     Cornell Computer Science TR94-1466.

          Abstract: Larchant-RDOSS is a distributed shared memory that
          persists on reliable storage across process lifetimes.  Memory
          management is automatic: including consistent caching of data
          and of locks, collecting objects unreachable from the
          persistent root, writing reachable objects to disk, and
          reducing store fragmentation.  Memory management is based on a
          novel garbage collection algorithm, that approximates a global
          trace by a series of local traces, with no induced I/O or
          locking traffic, and no synchronization between the collector
          and the application processes.  This results in a simple
          programming model, and expected minimal added application
          latency.  The algorithm is designed for the most unfavorable
          environment (uncontrolled programming language, reference by
          pointers, distributed system, non-coherent shared memory) and
          should work well also in more favorable settings.

   * Robert A. Shaw.  1987.  “Improving Garbage Collector Performance in
     Virtual Memory”.  Stanford University.  CSL-TR-87-323.

   * Robert A. Shaw.  1988.  “Empirical Analysis of a LISP System”.
     Stanford University.  CSL-TR-88-351.

   * Vivek Singhal, Sheetal V. Kakkad, Paul R. Wilson.  1992.  “Texas:
     An Efficient, Portable Persistent Store(115)”.  University of Texas
     at Austin.

          Abstract: Texas is a persistent storage system for C++,
          providing high performance while emphasizing simplicity,
          modularity and portability.  A key component of the design is
          the use of pointer swizzling at page fault time, which
          exploits existing virtual memory features to implement large
          address spaces efficiently on stock hardware, with little or
          no change to existing compilers.  Long pointers are used to
          implement an enormous address space, but are transparently
          converted to the hardware-supported pointer format when pages
          are loaded into virtual memory.

          Runtime type descriptors and slightly modified heap allocation
          routines support pagewise pointer swizzling by allowing
          objects and their pointer fields to be identified within
          pages.  If compiler support for runtime type identification is
          not available, a simple preprocessor can be used to generate
          type descriptors.

          This address translation is largely independent of issues of
          data caching, sharing, and checkpointing; it employs operating
          systems’ existing virtual memories for caching, and a simple
          and flexible log-structured storage manager to improve
          checkpointing performance.

          Pagewise virtual memory protections are also used to detect
          writes for logging purposes, without requiring any changes to
          compiled code.  This may degrade checkpointing performance for
          small transactions with poor locality of writes, but page
          diffing and sub-page logging promise to keep performance
          competitive with finer-grained checkpointing schemes.

          Texas presents a simple programming interface; an application
          creates persistent objects by simply allocating them on the
          persistent heap.  In addition, the implementation is
          relatively small, and is easy to incorporate into existing
          applications.  The log-structured storage module easily
          supports advanced extensions such as compressed storage,
          versioning, and adaptive reorganization.

   * P. G. Sobalvarro.  1988.  “A Lifetime-based Garbage Collector for
     LISP Systems on General-Purpose Computers(116)”.  MIT. AITR-1417.

          Abstract: Garbage collector performance in LISP systems on
          custom hardware has been substantially improved by the
          adoption of lifetime-based garbage collection techniques.  To
          date, however, successful lifetime-based garbage collectors
          have required special-purpose hardware, or at least privileged
          access to data structures maintained by the virtual memory
          system.  I present here a lifetime-based garbage collector
          requiring no special-purpose hardware or virtual memory system
          support, and discuss its performance.

   * Guy L. Steele.  1975.  “Multiprocessing Compactifying Garbage
     Collection”.  CACM. 18:9 pp.  495–508.

          Abstract: Algorithms for a multiprocessing compactifying
          garbage collector are presented and discussed.  The simple
          case of two processors, one performing LISP-like list
          operations and the other performing garbage collection
          continuously, is thoroughly examined.  The necessary
          capabilities of each processor are defined, as well as
          interprocessor communication and interlocks.  Complete
          procedures for garbage collection and for standard list
          processing primitives are presented and thoroughly explained.
          Particular attention is given to the problems of marking and
          relocating list cells while another processor may be operating
          on them.  The primary aim throughout is to allow the list
          processor to run unimpeded while the other processor reclaims
          list storage The more complex case involving several list
          processors and one or more garbage collection processors are
          also briefly discussed.

   * Guy L. Steele.  1976.  “Corrigendum: Multiprocessing Compactifying
     Garbage Collection”.  CACM. 19:6 p.354.

   * Guy L. Steele.  1977.  “Data Representation in PDP-10
     MACLISP(117)”.  MIT. AI Memo 420.

          Abstract: The internal representations of the various MacLISP
          data types are presented and discussed.  Certain
          implementation tradeoffs are considered.  The ultimate
          decisions on these tradeoffs are discussed in the light of
          MacLISP’s prime objective of being an efficient high-level
          language for the implementation of large systems such as
          MACSYMA. The basic strategy of garbage collection is outlined,
          with reference to the specific representations involved.
          Certain “clever tricks” are explained and justified.  The
          “address space crunch” is explained and some alternative
          solutions explored.

   * James M. Stichnoth, Guei-Yuan Lueh, Michal Cierniak.  1999.
     “Support for Garbage Collection at Every Instruction in a Java
     Compiler(118)”.  SIGPLAN. Proceedings of the 1999 ACM SIGPLAN
     Conference on Programming Language Design and Implementation
     (PLDI).  SIGPLAN Notices 34(5).  pp.  118–127.

          Abstract: A high-performance implementation of a Java Virtual
          Machine requires a compiler to translate Java bytecodes into
          native instructions, as well as an advanced garbage collector
          (e.g., copying or generational).  When the Java heap is
          exhausted and the garbage collector executes, the compiler
          must report to the garbage collector all live object
          references contained in physical registers and stack
          locations.  Typical compilers only allow certain instructions
          (e.g., call instructions and backward branches) to be GC-safe;
          if GC happens at some other instruction, the compiler may need
          to advance execution to the next GC-safe point.  Until now, no
          one has ever attempted to make every compiler-generated
          instruction GC-safe, due to the perception that recording this
          information would require too much space.  This kind of
          support could improve the GC performance in multithreaded
          applications.  We show how to use simple compression
          techniques to reduce the size of the GC map to about 20% of
          the generated code size, a result that is competitive with the
          best previously published results.  In addition, we extend the
          work of Agesen, Detlefs, and Moss, regarding the so-called
          “JSR Problem” (the single exception to Java’s type safety
          property), in a way that eliminates the need for extra runtime
          overhead in the generated code.

   * Will R Stoye, T J W Clarke, Arthur C Norman.  1984.  “Some
     Practical Methods for Rapid Combinator Reduction”.  In LFP 1984,
     159–166.

          Abstract: The SKIM II processor is a microcoded hardware
          machine for the rapid evaluation of functional languages.
          This paper gives details of some of the more novel methods
          employed by SKIM II, and resulting performance measurements.
          The authors conclude that combinator reduction can still form
          the basis for the efficient implementation of a functional
          language.

   * David Tarditi & Amer Diwan.  1995.  “Measuring the Cost of Storage
     Management(119)”.  Carnegie Mellon University.  CMU-CS-94-201.

          Abstract: We study the cost of storage management for
          garbage-collected programs compiled with the Standard ML of
          New Jersey compiler.  We show that the cost of storage
          management is not the same as the time spent garbage
          collecting.  For many of the programs, the time spent garbage
          collecting is less than the time spent doing other
          storage-management tasks.

   * Stephen Thomas, Richard E. Jones.  1994.  “Garbage Collection for
     Shared Environment Closure Reducers”.  Computing Laboratory, The
     University of Kent at Canterbury.  Technical Report 31-94.

          Abstract: Shared environment closure reducers such as
          Fairbairn and Wray’s TIM incur a comparatively low cost when
          creating a suspension, and so provide an elegant method for
          implementing lazy functional evaluation.  However,
          comparatively little attention has been given to the problems
          involved in identifying which portions of a shared environment
          are needed (and ignoring those which are not) during a garbage
          collection.  Proper consideration of this issue has subtle
          consequences when implementing a storage manager in a TIM-like
          system.  We describe the problem and illustrate the negative
          consequences of ignoring it.

          We go on to describe a solution in which the compiler
          determines statically which portions of that code’s
          environment are required for each piece of code it generates,
          and emits information to assist the run-time storage manager
          to scavenge environments selectively.  We also describe a
          technique for expressing this information directly as
          executable code, and demonstrate that a garbage collector
          implemented in this way can perform significantly better than
          an equivalent, table-driven interpretive collector.

   * Stephen Thomas.  1995.  “Garbage Collection in Shared-Environment
     Closure Reducers: Space-Efficient Depth First Copying using a
     Tailored Approach”.  'Information Processing Letters.'  56:1, pp.
     1–7.

          Abstract: Implementations of abstract machines such as the
          OP-TIM and the PG-TIM need to use a tailored garbage collector
          which seems to require an auxiliary stack,with a potential
          maximum size that is directly proportional to the amount of
          live data in the heap.  However, it turns out that it is
          possible to build a recursive copying collector that does not
          require additional space by reusing already-scavenged space.
          This paper is a description of this technique.

   * Mads Tofte & Jean-Pierre Talpin.  1997.  “Region-Based Memory
     Management(120)”.  Information and Computation 132(2), pp.
     109–176.

          Abstract: This paper describes a memory management discipline
          for programs that perform dynamic memory allocation and
          de-allocation.  At runtime, all values are put into regions.
          The store consists of a stack of regions.  All points of
          region allocation and de-allocation are inferred
          automatically, using a type and effect based program analysis.
          The scheme does not assume the presence of a garbage
          collector.  The scheme was first presented in 1994 (M. Tofte
          and J.-P. Talpin, in 'Proceedings of the 21st ACM
          SIGPLAN-SIGACT Symposium on Principles of Programming
          Languages,' pp.  188–201); subsequently, it has been tested in
          the ML Kit with Regions, a region-based, garbage-collection
          free implementation of the Standard ML Core Language, which
          includes recursive datatypes, higher-order functions and
          updatable references (L. Birkedal, M. Tofte, and M. Vejlstrup,
          (1996), in 'Proceedings of the 23rd ACM SIGPLAN-SIGACT
          Symposium on Principles of Programming Languages,' pp.
          171–183).  This paper defines a region-based dynamic semantics
          for a skeletal programming language extracted from Standard
          ML.  We present the inference system which specifies where
          regions can be allocated and de-allocated and a detailed proof
          that the system is sound with respect to a standard semantics.
          We conclude by giving some advice on how to write programs
          that run well on a stack of regions, based on practical
          experience with the ML Kit.

   * Dave Ungar.  1984.  “Generation Scavenging: A Non-disruptive High
     Performance Storage Reclamation Algorithm(121)”.  ACM, SIGSOFT,
     SIGPLAN. Practical Programming Environments Conference.

          Abstract: Many interactive computing environments provide
          automatic storage reclamation and virtual memory to ease the
          burden of managing storage.  Unfortunately, many storage
          reclamation algorithms impede interaction with distracting
          pauses.  'Generation Scavenging' is a reclamation algorithm
          that has no noticeable pauses, eliminates page faults for
          transient objects, compacts objects without resorting to
          indirection, and reclaims circular structures, in one third
          the time of traditional approaches.

   * Dave Ungar & Frank Jackson.  1988.  “Tenuring Policies for
     Generation-Based Storage Reclamation(122)”.  SIGPLAN. OOPSLA ‘88
     Conference Proceedings, ACM SIGPLAN Notices, Vol.  23, No.  11, pp.
     1–17.

          Abstract: One of the most promising automatic storage
          reclamation techniques, generation-based storage reclamation,
          suffers poor performance if many objects live for a fairly
          long time and then die.  We have investigated the severity of
          the problem by simulating Generation Scavenging automatic
          storage reclamation from traces of actual four-hour sessions.
          There was a wide variation in the sample runs, with
          garbage-collection overhead ranging from insignificant, during
          interactive runs, to sever, during a single non-interactive
          run.  All runs demonstrated that performance could be improved
          with two techniques: segregating large bitmaps and strings,
          and mediating tenuring with demographic feedback.  These two
          improvements deserve consideration for any generation-based
          storage reclamation strategy.

   * Kiem-Phong Vo.  1996.  “Vmalloc: A General and Efficient Memory
     Allocator”.  Software – Practice and Experience.  26(3): 357–374
     (1996).

          Abstract: On C/Unix systems, the malloc interface is standard
          for dynamic memory allocation.  Despite its popularity,
          malloc’s shortcomings frequently cause programmers to code
          around it.  The new library Vmalloc generalizes malloc to give
          programmers more control over memory allocation.  Vmalloc
          introduces the idea of organizing memory into separate
          regions, each with a discipline to get raw memory and a method
          to manage allocation.  Applications can write their own
          disciplines to manipulate arbitrary type of memory or just to
          better organize memory in a region by creating new regions out
          of its memory.  The provided set of allocation methods include
          general purpose allocations, fast special cases and aids for
          memory debugging or profiling.  A compatible malloc interface
          enables current applications to select allocation methods
          using environment variables so they can tune for performance
          or perform other tasks such as profiling memory usage,
          generating traces of allocation calls or debugging memory
          errors.  A performance study comparing Vmalloc and currently
          popular malloc implementations shows that Vmalloc is
          competitive to the best of these allocators.  Applications can
          gain further performance improvement by using the right
          mixture of regions with different Vmalloc methods.

   * Daniel C. Watson, David S. Wise.  1976.  “Tuning Garwick’s
     algorithm for repacking sequential storage”.  'BIT.' 16, 4
     (December 1976): 442–450.

          Abstract: Garwick’s algorithm, for repacking LIFO lists stored
          in a contiguous block of memory, bases the allocation of
          remaining space upon both sharing and previous stack growth.
          A system whereby the weight applied to each method can be
          adjusted according to the current behaviour of the stacks is
          discussed.

          We also investigate the problem of determining during memory
          repacking that the memory is used to saturation and the
          driving program should therefore be aborted.  The tuning
          parameters studied here seem to offer no new grasp on this
          problem.

   * Paul R. Wilson, Michael S. Lam, Thomas G. Moher.  1992.  “Caching
     Considerations for Generational Garbage Collection”.  ACM. L&FP 92.

          Abstract: GC systems allocate and reuse memory cyclically;
          this imposes a cyclic pattern on memory accesses that has its
          own distinctive locality characteristics.  The cyclic reuse of
          memory tends to defeat caching strategies if the reuse cycle
          is too large to fit in fast memory.  Generational GCs allow a
          smaller amount of memory to be reused more often.  This
          improves VM performance, because the frequently-reused area
          stays in main memory.  The same principle can be applied at
          the level of high-speed cache memories, if the cache is larger
          than the youngest generation.  Because of the repeated cycling
          through a fixed amount of memory, however, generational GC
          interacts with cache design in unusual ways, and modestly
          set-associative caches can significantly outperform
          direct-mapped caches.

          While our measurements do not show very high miss rates for
          GCed systems, they indicate that performance problems are
          likely in faster next-generation systems, where second-level
          cache misses may cost scores of cycles.  Software techniques
          can improve cache performance of garbage-collected systems, by
          decreasing the cache “footprint” of the youngest generation;
          compiler techniques that reduce the amount of heap allocation
          also improve locality.  Still, garbage-collected systems with
          a high rate of heap allocation require somewhat more cache
          capacity and/or main memory bandwidth than conventional
          systems.

   * Paul R. Wilson, Sheetal V. Kakkad.  1992.  “Pointer Swizzling at
     Page Fault Time(123)”.  University of Texas at Austin.

          Abstract: Pointer swizzling at page fault time is a novel
          address translation mechanism that exploits conventional
          address translation hardware.  It can support huge address
          spaces efficiently without long hardware addresses; such large
          address spaces are attractive for persistent object stores,
          distributed shared memories, and shared address space
          operating systems.  This swizzling scheme can be used to
          provide data compatibility across machines with different word
          sizes, and even to provide binary code compatibility across
          machines with different hardware address sizes.

          Pointers are translated (“swizzled”) from a long format to a
          shorter hardware-supported format at page fault time.  No
          extra hardware is required, and no continual software overhead
          is incurred by presence checks of indirection of pointers.
          This pagewise technique exploits temporal and spatial locality
          in much the same way as normal virtual memory; this gives it
          many desirable performance characteristics, especially given
          the trend toward larger main memories.  It is easy to
          implement using common compilers and operating systems.

   * Paul R. Wilson.  1994.  “Uniprocessor Garbage Collection
     Techniques(124)”.  University of Texas.

          Abstract: We survey basic garbage collection algorithms, and
          variations such as incremental and generational collection; we
          then discuss low-level implementation considerations and the
          relationships between storage management systems, languages,
          and compilers.  Throughout, we attempt to present a unified
          view based on abstract traversal strategies, addressing issues
          of conservatism, opportunism, and immediacy of reclamation; we
          also point out a variety of implementation details that are
          likely to have a significant impact on performance.

   * Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles.
     1995.  “Dynamic Storage Allocation: A Survey and Critical
     Review(125)”.  University of Texas at Austin.

          Abstract: Dynamic memory allocation has been a fundamental
          part of most computer systems since roughly 1960, and memory
          allocation is widely considered to be either a solved problem
          or an insoluble one.  In this survey, we describe a variety of
          memory allocator designs and point out issues relevant to
          their design and evaluation.  We then chronologically survey
          most of the literature on allocators between 1961 and 1995.
          (Scores of papers are discussed, in varying detail, and over
          150 references are given.)

          We argue that allocator designs have been unduly restricted by
          an emphasis on mechanism, rather than policy, while the latter
          is more important; higher-level strategic issues are still
          more important, but have not been given much attention.

          Most theoretical analyses and empirical allocator evaluations
          to date have relied on very strong assumptions of randomness
          and independence, but real program behavior exhibits important
          regularities that must be exploited if allocators are to
          perform well in practice.

   * David S. Wise.  1978.  “The double buddy system(126)”.  Department
     of Computer Science at Indiana University.  Technical Report 79.

          Abstract: A new buddy system is described in which the region
          of storage being managed is partitioned into two sub-regions,
          each managed by a fairly standard “binary” buddy system.  Like
          the weighted buddy systems of Shen and Peterson, the block
          sizes are of sizes 2^n+1 or 3·2^n, but unlike theirs there is
          no extra overhead for typing information or for buddy
          calculation, and an allocation which requires splitting an
          extant available block only rarely creates a block smaller
          than the one being allocated.  Such smaller blocks are carved
          out only when the boundary between the two subregions floats;
          the most interesting property of this system is that the
          procedures for allocation and deallocation are designed to
          keep blocks immediately adjacent to the subregion boundary
          free, so that the boundary may be moved within a range of
          unused space without disturbing blocks in use.  This option is
          attained with a minimum of extra computation beyond that of a
          binary buddy system, and provides this scheme with a new
          approach to the problem of external fragmentation.

   * David S. Wise.  1979.  “Morris's garbage compaction algorithm
     restores reference counts(127)”.  TOPLAS. 1, 1 (July 1979):
     115–120.

          Abstract: The two-pass compaction algorithm of F.L. Morris,
          which follows upon the mark phase in a garbage collector, may
          be modified to recover reference counts for a hybrid storage
          management system.  By counting the executions of two loops in
          that algorithm where upward and downward references,
          respectively, are forwarded to the relocation address of one
          node, we can initialize a count of active references and then
          update it but once.  The reference count may share space with
          the mark bit in each node, but it may not share the additional
          space required in each pointer by Morris’s algorithm, space
          which remains unused outside the garbage collector.

   * David S. Wise.  1985.  “Design for a multiprocessing heap with
     on-board reference counting(128)”.  Springer-Verlag.  In J.-P.
     Jouannaud (ed.), Functional Programming Languages and Computer
     Architecture, Lecture Notes in Computer Science 201: 289–304.

          Abstract: A project to design a pair of memory chips with a
          modicum of intelligence is described.  Together, the two allow
          simple fabrication of a small memory bank, a heap of binary
          (LISP-like) nodes that offers the following features: 64-bit
          nodes; two pointer fields per node up to 29 bits each;
          reference counts implicitly maintained on writes; 2 bits per
          node for marking (uncounted) circular references; 4 bits per
          node for conditional-store testing at the memory; provision
          for processor-driven, recounting garbage collection.

   * David S. Wise.  1993.  “Stop-and-copy and one-bit reference
     counting(129)”.  'Information Processing Letters.'  46, 5 (July
     1993): 243–249.

          Abstract: A stop-and-copy garbage collector updates one-bit
          reference counting with essentially no extra space and minimal
          memory cycles beyond the conventional collection algorithm.
          Any object that is uniquely referenced during a collection
          becomes a candidate for cheap recovery before the next one, or
          faster recopying then if it remains uniquely referenced.
          Since most objects stay uniquely referenced, subsequent
          collections run faster even if none are recycled between
          garbage collections.  This algorithm extends to generation
          scavenging, it admits uncounted references from roots, and it
          corrects conservatively stuck counters, that result from
          earlier uncertainty whether references were unique.

   * David S. Wise, Joshua Walgenbach.  1996.  “Static and Dynamic
     Partitioning of Pointers as Links and Threads(130)”.  SIGPLAN.
     Proc.  1996 ACM SIGPLAN Intl.  Conf.  on Functional Programming,
     SIGPLAN Not.  31, 6 (June 1996), pp.  42–49.

          Abstract: Identifying some pointers as invisible threads, for
          the purposes of storage management, is a generalization from
          several widely used programming conventions, like threaded
          trees.  The necessary invariant is that nodes that are
          accessible (without threads) emit threads only to other
          accessible nodes.  Dynamic tagging or static typing of threads
          ameliorates storage recycling both in functional and
          imperative languages.

          We have seen the distinction between threads and links sharpen
          both hardware- and software-supported storage management in
          SCHEME, and also in C. Certainly, therefore, implementations
          of languages that already have abstract management and
          concrete typing, should detect and use this as a new static
          type.

   * David S. Wise, Brian Heck, Caleb Hess, Willie Hunt, Eric Ost.
     1997.  “Uniprocessor Performance of a Reference-Counting Hardware
     Heap(131)”.  'LISP and Symbolic Computation.'  10, 2 (July 1997),
     pp.  159–181.

          Abstract: A hardware self-managing heap memory (RCM) for
          languages like LISP, SMALLTALK, and JAVA has been designed,
          built, tested and benchmarked.  On every pointer write from
          the processor, reference-counting transactions are performed
          in real time within this memory, and garbage cells are reused
          without processor cycles.  A processor allocates new nodes
          simply by reading from a distinguished location in its address
          space.  The memory hardware also incorporates support for
          off-line, multiprocessing, mark-sweep garbage collection.

          Performance statistics are presented from a partial
          implementation of SCHEME over five different memory models and
          two garbage collection strategies, from main memory (no access
          to RCM) to a fully operational RCM installed on an external
          bus.  The performance of the RCM memory is more than
          competitive with main memory.

   * P. Tucker Withington.  1991.  “How Real is 'Real-Time' Garbage
     Collection?(132)”.  ACM. OOPSLA/ECOOP ‘91 Workshop on Garbage
     Collection in Object-Oriented Systems.

          Abstract: A group at Symbolics is developing a Lisp runtime
          kernel, derived from its Genera operating system, to support
          real-time control applications.  The first candidate
          application has strict response-time requirements (so strict
          that it does not permit the use of paged virtual memory).
          Traditionally, Lisp’s automatic storage-management mechanism
          has made it unsuitable to real-time systems of this nature.  A
          number of garbage collector designs and implementations exist
          (including the Genera garbage collector) that purport to be
          “real-time”, but which actually have only mitigated the impact
          of garbage collection sufficiently that it usually goes
          unnoticed by humans.  Unfortunately, electro-mechanical
          systems are not so forgiving.  This paper examines the
          limitations of existing real-time garbage collectors and
          describes the avenues that we are exploring in our work to
          develop a CLOS-based garbage collector that can meet the
          real-time requirements of real real-time systems.

   * G. May Yip.  1991.  “Incremental, Generational Mostly-Copying
     Garbage Collection in Uncooperative Environments(133)”.  Digital
     Equipment Corporation.

          Abstract: The thesis of this project is that incremental
          collection can be done feasibly and efficiently in an
          architecture and compiler independent manner.  The design and
          implementation of an incremental, generational mostly-copying
          garbage collector for C++ is presented.  The collector
          achieves, simultaneously, real-time performance (from
          incremental collection), low total garbage collection delay
          (from generational collection), and the ability to function
          without hardware and compiler support (from mostly-copying
          collection).

          The incremental collector runs on commercially-available
          uniprocessors, such as the DECStation 3100, without any
          special hardware support.  It uses UNIX’s user controllable
          page protection facility (mprotect) to synchronize between the
          scanner (of the collector) and the mutator (of the application
          program).  Its implementation does not require any
          modification to the C++ compiler.  The maximum garbage
          collection pause is well within the 100-millisecond limit
          imposed by real-time applications executing on interactive
          workstations.  Compared to its non-incremental version, the
          total execution time of the incremental collector is not
          adversely affected.

   * Taiichi Yuasa.  1990.  “Real-Time Garbage Collection on
     General-Purpose Machines”.  Journal of Software and Systems.  11:3
     pp.  181–198.

          Abstract: An algorithm for real-time garbage collection is
          presented, proved correct, and evaluated.  This algorithm is
          intended for list-processing systems on general-purpose
          machines, i.e., Von Neumann style serial computers with a
          single processor.  On these machines, real-time garbage
          collection inevitably causes some overhead on the overall
          execution of the list-processing system, because some of the
          primitive list-processing operations must check the status of
          garbage collection.  By removing such overhead from frequently
          used primitives such as pointer references (e.g., Lisp car and
          cdr) and stack manipulations, the presented algorithm reduces
          the execution overhead to a great extent.  Although the
          algorithm does not support compaction of the whole data space,
          it efficiently supports partial compaction such as array
          relocation.

   * Benjamin Zorn & Paul Hilfinger.  1988.  “A Memory Allocation
     Profiler for C and Lisp Programs(134)”.  USENIX. Proceedings for
     the Summer 1988 USENIX Conference, pp.  223–237.

          Abstract: This paper describes inprof, a tool used to study
          the memory allocation behavior of programs.  mprof records the
          amount of memory each function allocates, breaks down
          allocation information by type and size, and displays a
          program’s dynamic cal graph so that functions indirectly
          responsible for memory allocation are easy to identify.  mprof
          is a two-phase tool.  The monitor phase is linked into
          executing programs and records information each time memory is
          allocated.  The display phase reduces the data generated by
          the monitor and displays the information to the user in
          several tables.  mprof has been implemented for C and Kyoto
          Common Lisp.  Measurements of these implementations are
          presented.

   * Benjamin Zorn.  1989.  “Comparative Performance Evaluation of
     Garbage Collection Algorithms(135)”.  Computer Science Division
     (EECS) of University of California at Berkeley.  Technical Report
     UCB/CSD 89/544 and PhD thesis.

          Abstract: This thesis shows that object-level, trace-driven
          simulation can facilitate evaluation of language runtime
          systems and reaches new conclusions about the relative
          performance of important garbage collection algorithms.  In
          particular, I reach the unexpected conclusion that
          mark-and-sweep garbage collection, when augmented with
          generations, shows comparable CPU performance and much better
          reference locality than the more widely used copying
          algorithms.  In the past, evaluation of garbage collection
          algorithms has been limited by the high cost of implementing
          the algorithms.  Substantially different algorithms have
          rarely been compared in a systematic way.

          With the availability of high-performance, low-cost
          workstations, trace-driven performance evaluation of these
          algorithms is now economical.  This thesis describes MARS, a
          runtime system simulator that is driven by operations on
          program objects, and not memory addresses.  MARS has been
          attached to a commercial Common Lisp system and eight large
          Lisp applications are used in the thesis as test programs.  To
          illustrate the advantages of the object-level tracing
          technique used by MARS, this thesis compares the relative
          performance of stop-and-copy, incremental, and mark-and-sweep
          collection algorithms, all organized with multiple
          generations.  The comparative evaluation is based on several
          metrics: CPU overhead, reference locality, and interactive
          availability.

          Mark-and-sweep collection shows slightly higher CPU overhead
          than stop-and-copy ability (5 percent), but requires
          significantly less physical memory to achieve the same page
          fault rate (30-40 percent).  Incremental collection has very
          good interactive availability, but implementing the read
          barrier on stock hardware incurs a substantial CPU overhead
          (30-60 percent).  In the future, I will use MARS to
          investigate other performance aspects of sophisticated runtime
          systems.

   * Benjamin Zorn.  1990.  “Comparing Mark-and-sweep and Stop-and-copy
     Garbage Collection”.  ACM. Conference on Lisp and Functional
     Programming, pp.  87–98.

          Abstract: Stop-and-copy garbage collection has been preferred
          to mark-and-sweep collection in the last decade because its
          collection time is proportional to the size of reachable data
          and not to the memory size.  This paper compares the CPU
          overhead and the memory requirements of the two collection
          algorithms extended with generations, and finds that
          mark-and-sweep collection requires at most a small amount of
          additional CPU overhead (3-6%) but requires an average of 20%
          (and up to 40%) less memory to achieve the same page fault
          rate.  The comparison is based on results obtained using
          trace-driven simulation with large Common Lisp programs.

   * Benjamin Zorn.  1990.  “Barrier Methods for Garbage
     Collection(136)”.  University of Colorado at Boulder.  Technical
     Report CU-CS-494-90.

          Abstract: Garbage collection algorithms have been enhanced in
          recent years with two methods: generation-based collection and
          Baker incremental copying collection.  Generation-based
          collection requires special actions during certain store
          operations to implement the “write barrier”.  Incremental
          collection requires special actions on certain load operations
          to implement the “read barrier”.  This paper evaluates the
          performance of different implementations of the read and write
          barriers and reaches several important conclusions.  First,
          the inlining of barrier checks results in surprisingly low
          overheads, both for the write barrier (2%-6%) and the read
          barrier (&lt; 20%).  Contrary to previous belief, these
          results suggest that a Baker-style read barrier can be
          implemented efficiently without hardware support.  Second, the
          use of operating system traps to implement garbage collection
          methods results in extremely high overheads because the cost
          of trap handling is so high.  Since this large overhead is
          completely unnecessary, operating system memory protection
          traps should be reimplemented to be as fast as possible.
          Finally, the performance of these approaches on several
          machine architectures is compared to show that the results are
          generally applicable.

   * Benjamin Zorn.  1991.  “The Effect of Garbage Collection on Cache
     Performance(137)”.  University of Colorado at Boulder.  Technical
     Report CU-CS-528-91.

          Abstract: Cache performance is an important part of total
          performance in modern computer systems.  This paper describes
          the use of trace-driven simulation to estimate the effect of
          garbage collection algorithms on cache performance.  Traces
          from four large Common Lisp programs have been collected and
          analyzed with an all-associativity cache simulator.  While
          previous work has focused on the effect of garbage collection
          on page reference locality, this evaluation unambiguously
          shows that garbage collection algorithms can have a profound
          effect on cache performance as well.  On processors with a
          direct-mapped cache, a generation stop-and-copy algorithm
          exhibits a miss rate up to four times higher than a comparable
          generation mark-and-sweep algorithm.  Furthermore, two-way
          set-associative caches are shown to reduce the miss rate in
          stop-and-copy algorithms often by a factor of two and
          sometimes by a factor of almost five over direct-mapped
          caches.  As processor speeds increase, cache performance will
          play an increasing role in total performance.  These results
          suggest that garbage collection algorithms will play an
          important part in improving that performance.

   * Benjamin Zorn & Dirk Grunwald.  1992.  “Empirical Measurements of
     Six Allocation-intensive C Programs(138)”.  ACM, SIGPLAN. SIGPLAN
     notices, 27(12):71–80.

          Abstract: Dynamic memory management is an important part of a
          large class of computer programs and high-performance
          algorithms for dynamic memory management have been, and will
          continue to be, of considerable interest.  This paper presents
          empirical data from a collection of six allocation-intensive C
          programs.  Extensive statistics about the allocation behavior
          of the programs measured, including the distributions of
          object sizes, lifetimes, and interarrival times, are
          presented.  This data is valuable for the following reasons:
          first, the data from these programs can be used to design
          high-performance algorithms for dynamic memory management.
          Second, these programs can be used as a benchmark test suite
          for evaluating and comparing the performance of different
          dynamic memory management algorithms.  Finally, the data
          presented gives readers greater insight into the storage
          allocation patterns of a broad range of programs.  The data
          presented in this paper is an abbreviated version of more
          extensive statistics that are publicly available on the
          internet.

   * Benjamin Zorn.  1993.  “The Measured Cost of Conservative Garbage
     Collection(139)”.  Software – Practice and Experience.
     23(7):733–756.

          Abstract: Because dynamic memory management is an important
          part of a large class of computer programs, high-performance
          algorithms for dynamic memory management have been, and will
          continue to be, of considerable interest.  Experience
          indicates that for many programs, dynamic storage allocation
          is so important that programmers feel compelled to write and
          use their own domain-specific allocators to avoid the overhead
          of system libraries.  Conservative garbage collection has been
          suggested as an important algorithm for dynamic storage
          management in C programs.  In this paper, I evaluate the costs
          of different dynamic storage management algorithms, including
          domain-specific allocators; widely-used general-purpose
          allocators; and a publicly available conservative garbage
          collection algorithm.  Surprisingly, I find that programmer
          enhancements often have little effect on program performance.
          I also find that the true cost of conservative garbage
          collection is not the CPU overhead, but the memory system
          overhead of the algorithm.  I conclude that conservative
          garbage collection is a promising alternative to explicit
          storage management and that the performance of conservative
          collection is likely to be improved in the future.  C
          programmers should now seriously consider using conservative
          garbage collection instead of malloc/free in programs they
          write.

   * Benjamin Zorn & Dirk Grunwald.  1994.  “Evaluating Models of Memory
     Allocation(140)”.  ACM. Transactions on Modeling and Computer
     Simulation 4(1):107–131.

          Abstract: Because dynamic memory management is an important
          part of a large class of computer programs, high-performance
          algorithms for dynamic memory management have been, and will
          continue to be, of considerable interest.  We evaluate and
          compare models of the memory allocation behavior in actual
          programs and investigate how these models can be used to
          explore the performance of memory management algorithms.
          These models, if accurate enough, provide an attractive
          alternative to algorithm evaluation based on trace-driven
          simulation using actual traces.  We explore a range of models
          of increasing complexity including models that have been used
          by other researchers.  Based on our analysis, we draw three
          important conclusions.  First, a very simple model, which
          generates a uniform distribution around the mean of observed
          values, is often quite accurate.  Second, two new models we
          propose show greater accuracy than those previously described
          in the literature.  Finally, none of the models investigated
          appear adequate for generating an operating system workload.

   ---------- Footnotes ----------

   (1) 
http://www-plan.cs.colorado.edu/diwan/class-papers/finding-references-in-java.pdf

   (2) 
http://pdf.aminer.org/000/542/332/garbage_collection_and_local_variable_type_precision_and_liveness_in.pdf

   (3) 
http://apotheca.hpl.hp.com/ftp/pub/compaq/SRC/research-reports/SRC-025.pdf

   (4) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.257&rep=rep1&type=pdf

   (5) ftp://ftp.di.unipi.it/pub/Papers/attardi/SPE.ps.gz

   (6) 
http://pdf.aminer.org/000/465/100/combining_card_marking_with_remembered_sets_how_to_save_scanning.pdf

   (7) http://home.pipeline.com/~hbaker1/Futures.html

   (8) http://home.pipeline.com/~hbaker1/RealTimeGC.html

   (9) http://home.pipeline.com/~hbaker1/OptAlloc.html

   (10) http://home.pipeline.com/~hbaker1/CacheCGC.html

   (11) http://home.pipeline.com/~hbaker1/LinearLisp.html

   (12) http://home.pipeline.com/~hbaker1/NoMotionGC.html

   (13) http://home.pipeline.com/~hbaker1/LazyAlloc.html

   (14) http://home.pipeline.com/~hbaker1/ReverseGC.html

   (15) http://home.pipeline.com/~hbaker1/YoungGen.html

   (16) http://home.pipeline.com/~hbaker1/ObjectIdentity.html

   (17) http://home.pipeline.com/~hbaker1/LRefCounts.html

   (18) http://home.pipeline.com/~hbaker1/ThermoGC.html

   (19) http://home.pipeline.com/~hbaker1/Use1Var.html

   (20) http://www.cs.utexas.edu/ftp/garbage/GC97/withingt.ps

   (21) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.6712&rep=rep1&type=pdf

   (22) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.1835&rep=rep1&type=pdf

   (23) 
http://computer-refuge.org/classiccmp/ftp.digital.com-jun2004/pub/Compaq/WRL/research-reports/WRL-TR-88.2.pdf

   (24) http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-12.pdf

   (25) 
http://www.informatik.uni-trier.de/%7Eley/db/conf/iwmm/iwmm92.html

   (26) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.5049&rep=rep1&type=pdf

   (27) 
http://www.cs.utexas.edu/users/speedway/DaCapo/papers/pldi2001.pdf

   (28) http://hboehm.info/spe_gc_paper/preprint.pdf

   (29) http://hboehm.info/gc/papers/pldi91.ps.Z

   (30) http://hboehm.info/gc/papers/boecha.ps.gz

   (31) http://hboehm.info/gc/papers/pldi93.ps.Z

   (32) http://www.hpl.hp.com/techreports/2000/HPL-2000-99.html

   (33) http://www.hpl.hp.com/techreports/2002/HPL-2002-335.html

   (34) http://www.cs.utexas.edu/~moore/publications/fstrpos.pdf

   (35) 
https://www.ravenbrook.com/project/mps/doc/2002-01-30/ismm2002-paper/

   (36) http://www.open-std.org/jtc1/sc22/WG14/www/docs/n1256.pdf

   (37) http://cseclassic.ucsd.edu/users/calder/papers/JplVersion.pdf

   (38) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9649&rep=rep1&type=pdf

   (39) 
http://people.cs.umass.edu/~emery/classes/cmpsci691s-fall2004/papers/p677-cheney.pdf

   (40) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.9229&rep=rep1&type=pdf

   (41) http://ftp2.cs.wisc.edu/wwt/ismm98_cache_gc.pdf

   (42) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.370&rep=rep1&type=pdf

   (43) 
http://pdf.aminer.org/000/465/134/compiler_support_to_customize_the_mark_and_sweep_algorithm.pdf

   (44) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.3656&rep=rep1&type=pdf

   (45) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.8140&rep=rep1&type=pdf

   (46) 
https://cs.uwaterloo.ca/~Brecht/courses/702/Possible-Readings/vm-and-gc/thrashing-denning-afips-1968.pdf

   (47) http://denninginstitute.com/pjd/PUBS/VirtMem_1970.pdf

   (48) http://denninginstitute.com/pjd/PUBS/WSProp_1972.pdf

   (49) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.2755&rep=rep1&type=pdf

   (50) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.3073&rep=rep1&type=pdf

   (51) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4603&rep=rep1&type=pdf

   (52) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4752&rep=rep1&type=pdf

   (53) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.71&rep=rep1&type=pdf

   (54) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9220&rep=rep1&type=pdf

   (55) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9220&rep=rep1&type=pdf

   (56) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9494&rep=rep1&type=pdf

   (57) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.4710&rep=rep1&type=pdf

   (58) http://www.cs.indiana.edu/~dyb/pubs/guardians-pldi93.pdf

   (59) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.530&rep=rep1&type=pdf

   (60) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.6011&rep=rep1&type=pdf

   (61) ftp://publications.ai.mit.edu/ai-publications/0-499/AIM-019.ps

   (62) http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-102.pdf

   (63) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.8434&rep=rep1&type=pdf

   (64) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.6176&rep=rep1&type=pdf

   (65) http://www.cs.indiana.edu/pub/techreports/TR34.pdf

   (66) http://www.cs.indiana.edu/pub/techreports/TR57.pdf

   (67) http://www.cs.indiana.edu/pub/techreports/TR73.pdf

   (68) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.6621&rep=rep1&type=pdf

   (69) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5260&rep=rep1&type=pdf

   (70) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.4394&rep=rep1&type=pdf

   (71) http://www.timharris.co.uk/papers/1999-sigplan.pdf

   (72) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.1554&rep=rep1&type=pdf

   (73) 
http://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=18921&fileOId=630830

   (74) ftp://ftp.cs.purdue.edu/pub/hosking/papers/oopsla91gc-alh.pdf

   (75) ftp://ftp.cs.purdue.edu/pub/hosking/papers/oopsla92.pdf

   (76) ftp://ftp.cs.purdue.edu/pub/hosking/papers/gc-workshop93c.pdf

   (77) ftp://ftp.cs.purdue.edu/pub/hosking/papers/sosp93.pdf

   (78) 
http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1210&context=cs_faculty_pubs

   (79) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.3883&rep=rep1&type=pdf

   (80) http://www.cs.umass.edu/~moss/papers/oopsla-1997-gc-world.pdf

   (81) ftp://ftp.cs.utexas.edu/pub/garbage/johnstone-dissertation.ps.gz

   (82) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.3382&rep=rep1&type=pdf

   (83) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.5083&rep=rep1&type=pdf

   (84) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.8499&rep=rep1&type=pdf

   (85) http://www.cs.ukc.ac.uk/people/staff/rej/gcbook/gcbook.html

   (86) http://www.acm.org/pubs/contents/proceedings/plan/286860/

   (87) http://gchandbook.org/

   (88) 
http://www.emu.edu.tr/aelci/Courses/D-318/D-318-Files/cppcrit/index.htm

   (89) 
http://www.songworm.com/db/songworm-parody/RecursiveMemoryAllocation.html

   (90) http://csl.skku.edu/papers/wwc98.pdf

   (91) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.1947&rep=rep1&type=pdf

   (92) 
http://web.media.mit.edu/~lieber/Lieberary/GC/Realtime/Realtime.html

   (93) 
http://dspace.mit.edu/bitstream/handle/1721.1/52263/RLE_QPR_053_XIII.pdf

   (94) http://www-formal.stanford.edu/jmc/recursive.html

   (95) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.4634&rep=rep1&type=pdf

   (96) 
http://www.cs.umass.edu/~mcorner/courses/691J/papers/VM/protic_dsm/protic_dsm.pdf

   (97) http://dspace.mit.edu/bitstream/handle/1721.1/6080/AIM-058.pdf

   (98) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.2438&rep=rep1&type=pdf

   (99) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.4593&rep=rep1&type=pdf

   (100) http://www.eecs.harvard.edu/~greg/papers/fpca_gc.ps

   (101) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.3687&rep=rep1&type=pdf

   (102) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.4233&rep=rep1&type=pdf

   (103) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.1498&rep=rep1&type=pdf

   (104) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.1875&rep=rep1&type=pdf

   (105) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.5001&rep=rep1&type=pdf

   (106) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.1815&rep=rep1&type=pdf

   (107) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.5454&rep=rep1&type=pdf

   (108) http://www.cs.indiana.edu/pub/techreports/TR516.pdf

   (109) 
https://archive.org/details/bitsavers_xeroxparctddingGarbageCollectionandRuntimeTypestoa_1765837

   (110) ftp://ftp.cs.york.ac.uk/reports/YCS-92-172.ps.Z

   (111) 
http://www.cs.york.ac.uk/plasma/publications/pdf/RuncimanWakelingJFP93.pdf

   (112) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7307&rep=rep1&type=pdf

   (113) http://www.hpl.hp.com/techreports/2000/HPL-2000-62.html

   (114) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.8468&rep=rep1&type=pdf

   (115) ftp://ftp.cs.utexas.edu/pub/garbage/texaspstore.ps

   (116) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.2188&rep=rep1&type=pdf

   (117) http://dspace.mit.edu/bitstream/handle/1721.1/6278/AIM-420.pdf

   (118) 
http://www.cs.tufts.edu/~nr/cs257/archive/james-stichnoth/p118-stichnoth.pdf

   (119) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.4550&rep=rep1&type=pdf

   (120) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.9105&rep=rep1&type=pdf

   (121) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.4295&rep=rep1&type=pdf

   (122) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.2810&rep=rep1&type=pdf

   (123) ftp://ftp.cs.utexas.edu/pub/garbage/swizz.ps

   (124) ftp://ftp.cs.utexas.edu/pub/garbage/bigsurv.ps

   (125) ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps

   (126) http://www.cs.indiana.edu/ftp/techreports/TR79.pdf

   (127) http://www.cs.indiana.edu/ftp/techreports/TR75.pdf

   (128) http://www.cs.indiana.edu/ftp/techreports/TR163.pdf

   (129) http://www.cs.indiana.edu/ftp/techreports/TR360.pdf

   (130) http://www.cs.indiana.edu/ftp/techreports/TR437.pdf

   (131) http://www.cs.indiana.edu/ftp/techreports/TR401.pdf

   (132) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.3169&rep=rep1&type=pdf

   (133) http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-91-8.pdf

   (134) 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.1689&rep=rep1&type=pdf

   (135) http://www.eecs.berkeley.edu/Pubs/TechRpts/1989/CSD-89-544.pdf

   (136) 
http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-494-90.pdf

   (137) 
http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-528-91.pdf

   (138) 
http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-604-92.pdf

   (139) 
http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-573-92.pdf

   (140) 
http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-603-92.pdf

